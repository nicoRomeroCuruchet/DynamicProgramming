{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b149d005",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoRomeroCuruchet/DynamicProgramming/blob/main/testing_bary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002626e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from utils.utils import test_enviroment\n",
        "from PolicyIteration import PolicyIteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b5a85a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import airplane\n",
        "\n",
        "glider = gym.make('ReducedSymmetricGliderPullout-v0')\n",
        "\n",
        "bins_space = {\n",
        "    \"flight_path_angle\": np.linspace(-np.pi, 0.5,    100,      dtype=np.float32),     # Flight Path Angle (Î³)    (0)\n",
        "    \"airspeed_norm\":     np.linspace(0.7, 4.0,       100,      dtype=np.float32),     # Air Speed         (V)    (1)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=glider, \n",
        "    bins_space=bins_space,\n",
        "    action_space=np.linspace(-0.4, 1.0, 15, dtype=np.float32),\n",
        "    gamma=0.99,\n",
        "    theta=1e-3,\n",
        ")\n",
        "\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9c6a7123",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-02-16 15:34:40.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m129\u001b[0m - \u001b[1mLower bounds: [-3.1415927   0.7        -0.34906584]\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:40.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mUpper bounds: [0.5       4.        3.4906585]\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:40.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m149\u001b[0m - \u001b[1mNumber of states: 8000\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:40.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m150\u001b[0m - \u001b[1mTotal states:200000\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:40.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mPolicy Iteration was correctly initialized.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:40.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m164\u001b[0m - \u001b[1mThe enviroment name is: TimeLimit\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:40.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m387\u001b[0m - \u001b[1mCreating Delaunay triangulation over the state space...\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:41.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m389\u001b[0m - \u001b[1mDelaunay triangulation created.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:41.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m399\u001b[0m - \u001b[1mGenerating transition and reward function table...\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:41.191\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:44.199\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:46.167\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:47.740\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:49.491\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:51.257\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:53.360\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:55.452\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:57.022\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:34:58.749\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:00.531\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:02.490\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:04.461\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:06.043\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:07.727\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:09.493\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:11.408\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:13.295\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:14.891\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:16.610\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:18.346\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:20.331\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:22.293\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:23.896\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:25.659\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m255\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:27.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mTransition and reward function table generated.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:27.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1msolving step 0\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:27.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:27.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.03999999910593033 | Avg Error: 0.012000000104308128 | 1600<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:28.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.008999999612569809 | Avg Error: 0.003000000026077032 | 1459<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:29.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0020000000949949026 | Avg Error: 0.0010000000474974513 | 4703<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:29.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:29.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:29.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:29.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mThe number of updated different actions: 200000\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:29.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:29.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1msolving step 1\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:30.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:30.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.027000000700354576 | Avg Error: 0.004000000189989805 | 2899<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:30.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.003000000026077032 | Avg Error: 0.0010000000474974513 | 5217<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:31.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0010000000474974513 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:32.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:32.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:32.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:32.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mThe number of updated different actions: 5032\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:32.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:32.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1msolving step 2\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:32.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:32.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.012000000104308128 | Avg Error: 0.0 | 7198<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:33.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0010000000474974513 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:34.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:34.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:34.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:34.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mThe number of updated different actions: 1712\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:34.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:34.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1msolving step 3\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:34.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:34.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.007000000216066837 | Avg Error: 0.0 | 7829<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:35.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:35.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:35.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:35.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mThe number of updated different actions: 354\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:35.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:35.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1msolving step 4\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:35.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:35.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0010000000474974513 | Avg Error: 0.0 | 7999<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mThe number of updated different actions: 70\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1msolving step 5\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mThe number of updated different actions: 2\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1msolving step 6\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m313\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-16 15:35:36.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m422\u001b[0m - \u001b[1mPolicy and value function saved.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import airplane\n",
        "import pickle\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from utils.utils import test_enviroment\n",
        "from PolicyIteration import PolicyIteration\n",
        "\n",
        "glider = gym.make('ReducedBankedGliderPullout-v0')\n",
        "\n",
        "bins_space = {\n",
        "    \"flight_path_angle\": np.linspace(-np.pi, 0.5,    20,      dtype=np.float32),     # Flight Path Angle (Î³)    (0)\n",
        "    \"airspeed_norm\":     np.linspace(0.7, 4.0,       20,      dtype=np.float32),     # Air Speed         (V)    (1)\n",
        "    \"bank_angle\":        np.linspace( np.deg2rad(-20), np.deg2rad(200),  20,      dtype=np.float32),     # Bank Angle        (mu)    (2)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=glider, \n",
        "    bins_space=bins_space,\n",
        "    # cross product np.linspace(-0.4, 1.0, 15, dtype=np.float32) X np.linspace(np.deg2rad(-30), np.deg2rad(30), 15, dtype=np.float32)\n",
        "    action_space= np.array(np.meshgrid(np.linspace(-0.4, 1.0, 5, dtype=np.float32), np.linspace(np.deg2rad(-30), np.deg2rad(30), 5, dtype=np.float32))).T.reshape(-1, 2),\n",
        "    #action_space=np.linspace(-0.4, 1.0, 15, dtype=np.float32),\n",
        "    gamma=0.99,\n",
        "    theta=1e-3,\n",
        ")\n",
        "\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "626c9909",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.linspace(-0.4, 1.0, 15, dtype=np.float32)\n",
        "np.linspace(np.deg2rad(-30), np.deg2rad(30), 15, dtype=np.float32)\n",
        "# cartesian product of action space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2f30c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "STALL_AIRSPEED = 27.331231856346\n",
        "from utils.utils import get_optimal_action\n",
        "\n",
        "for action in np.linspace(-0.4, 1.0, 15, dtype=np.float32):\n",
        "\n",
        "    state = np.array([0.05, 1.83])\n",
        "    glider.airplane.flight_path_angle = state[0]\n",
        "    glider.airplane.airspeed_norm = state[1]\n",
        "    \n",
        "    total_reward = 0\n",
        "    episode_length = 0\n",
        "\n",
        "    while episode_length <100:\n",
        "        #action = get_optimal_action(state, pi)\n",
        "        prev_state  = state.copy()\n",
        "        state, reward, _, _, _ = glider.step(action)\n",
        "        state = state[0]\n",
        "        total_reward += reward*STALL_AIRSPEED\n",
        "        episode_length += 1\n",
        "    \n",
        "    print(f\"Action: {action} Reward: {total_reward} Episode Length: {episode_length}\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd13daf",
      "metadata": {},
      "outputs": [],
      "source": [
        "STALL_AIRSPEED = 27.331231856346\n",
        "from utils.utils import get_optimal_action\n",
        "from tqdm import tqdm\n",
        "with open(glider.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi: PolicyIteration = pickle.load(f)\n",
        "\n",
        "prom_episode_lenght = 0\n",
        "dict_result = {}\n",
        "dict_episode_length = {}\n",
        "state_spaces = [v for v in pi.states_space if v[0] > -np.pi/2]\n",
        "for state in tqdm(state_spaces):\n",
        "    initial_state = state.copy()\n",
        "    prev_state = state.copy()\n",
        "    glider.reset()\n",
        "    glider.airplane.flight_path_angle = state[0]\n",
        "    glider.airplane.airspeed_norm = state[1]\n",
        "    glider.airplane.bank_angle = state[2]\n",
        "    done = False\n",
        "    episode_length = 0\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = get_optimal_action(state, pi)\n",
        "        prev_state  = state.copy()\n",
        "        state, reward, done, _, _ = glider.step(action)\n",
        "        done  = bool(done)\n",
        "        if done:\n",
        "            break\n",
        "        state = state[0]\n",
        "        # check if our state is in the state space\n",
        "        index = pi.triangulation.find_simplex(state)\n",
        "        if not done:\n",
        "            total_reward -= reward#*STALL_AIRSPEED\n",
        "        if (index ==-1) or episode_length > 70: \n",
        "            done = True\n",
        "        episode_length += 1\n",
        "        \n",
        "    dict_result[tuple(initial_state)] = total_reward\n",
        "    dict_episode_length[tuple(initial_state)] = episode_length\n",
        "    prom_episode_lenght += episode_length / len(pi.states_space)\n",
        "    #print(f\"Initial state: {initial_state} - Total reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab055601",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "epsilon = 1*1e-1\n",
        "#dict_result = dict_episode_length\n",
        "# Your dictionary with keys as coordinates (x, y) and values as the color intensity (z)\n",
        "# Extracting the x, y and z values\n",
        "x = [np.degrees(coord[0]) for coord in dict_result.keys() if 0.05 >= coord[0] >= -np.pi/2 - epsilon and coord[1] > 0.7]\n",
        "#convert x to grad\n",
        "y = [coord[1] for coord in dict_result.keys() if 0.05 >= coord[0] >= -np.pi/2 - epsilon and coord[1] > 0.7]\n",
        "#convert y to Vs\n",
        "keys_list = list(dict_result.keys())\n",
        "values_list = list(dict_result.values())\n",
        "z = [v for e,v in zip(keys_list, values_list) if 0.05 >= e[0] >= -np.pi/2 - epsilon and e[1] > 0.7]\n",
        "\n",
        "# Creating a 2D scatter plot\n",
        "cmap = plt.get_cmap('viridis', 2048)\n",
        "plt.tricontourf(y, x, z, cmap=cmap, levels=18)   # Change 'viridis' to any other colormap you like\n",
        "plt.colorbar(label='', shrink=0.8,  )  # Add color bar for the z values\n",
        "\n",
        "# Add labels and a title\n",
        "plt.ylabel('Flight Path Angle (Î³) [deg]')\n",
        "plt.xlabel('V/Vs')\n",
        "\n",
        "x_min, x_max = min(x), max(x)\n",
        "y_min, y_max = min(y), max(y)\n",
        "\n",
        "# Set the minor ticks for the grid, keeping the dense grid with smaller squares\n",
        "ax = plt.gca()  # Get current axes\n",
        "ax.set_yticks(np.linspace(x_min, x_max, 60), minor=True)  # 20 minor ticks for grid\n",
        "ax.set_xticks(np.linspace(y_min, y_max, 60), minor=True)  # 20 minor ticks for grid\n",
        "# put line x = 1\n",
        "plt.axvline(x=1, color='r', linestyle='--', linewidth=0.5)\n",
        "# put line y = 0\n",
        "#plt.axhline(y=0, color='k', linestyle='--')\n",
        "\n",
        "# Show the plot\n",
        "vals = np.round(np.linspace(-90, 0,6))\n",
        "plt.yticks(vals)  # Only 5 labels on x-axis\n",
        "plt.xticks(np.linspace(round(y_min), round(y_max), 6))  # Only 5 labels on y-axis\n",
        "# Enable the minor grid lines\n",
        "ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
        "\n",
        "# Habilitar la cuadrÃ­cula en las marcas menores\n",
        "ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
        "\n",
        "# Guardar la imagen sin borde blanco\n",
        "plt.savefig('output.png', bbox_inches='tight')\n",
        "\n",
        "# set size of the plot\n",
        "plt.gcf().set_size_inches(9, 5)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8416867",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "# Your dictionary with keys as coordinates (x, y) and values as the color intensity (z)\n",
        "# Extracting the x, y and z values\n",
        "x = [np.degrees(coord[0]) for coord in dict_result.keys() if 5 >= coord[0] >= -np.pi/2 and coord[1] > 0.7]\n",
        "y = [coord[1] for coord in dict_result.keys() if 5 >= coord[0] >= -np.pi/2 and coord[1] > 0.7]\n",
        "keys_list = list(dict_result.keys())\n",
        "values_list = list(dict_result.values())\n",
        "z = [v for e,v in zip(keys_list, values_list) if 5 >= e[0] >= -np.pi/2 and e[1] > 0.7]\n",
        "\n",
        "# Create a grid for interpolation\n",
        "x_grid = np.linspace(min(x), max(x), 100)\n",
        "y_grid = np.linspace(min(y), max(y), 100)\n",
        "X, Y = np.meshgrid(x_grid, y_grid)\n",
        "\n",
        "# Interpolate the scattered data into the grid\n",
        "Z = griddata((y, x), z, (Y, X), method='linear')\n",
        "\n",
        "# Create the scatter plot\n",
        "cmap = plt.get_cmap('viridis', 2048)\n",
        "plt.scatter(y, x, c=z, cmap=cmap)\n",
        "\n",
        "# Add color bar\n",
        "plt.colorbar(label='', shrink=0.8)\n",
        "contour_levels = np.linspace(np.min(z), np.max(z), 10)  # 10 levels\n",
        "contour_levels = contour_levels[contour_levels != 0]    # Remove zero from levels\n",
        "\n",
        "# Add contour lines on top of the scatter plot\n",
        "contour = plt.contour(Y, X, Z, levels=contour_levels, colors='black', linewidths=0.75)\n",
        "plt.clabel(contour, inline=True, fontsize=8)  # Optional: add labels to the contours\n",
        "\n",
        "# Add labels and a title\n",
        "plt.ylabel('Flight Path Angle (Î³) [deg]')\n",
        "plt.xlabel('V/Vs')\n",
        "\n",
        "# Plot vertical line at x=1\n",
        "plt.axvline(x=1, color='k', linestyle='--')\n",
        "\n",
        "# Configure the minor ticks and grid\n",
        "x_min, x_max = min(x), max(x)\n",
        "y_min, y_max = min(y), max(y)\n",
        "ax = plt.gca()  # Get current axes\n",
        "ax.set_yticks(np.linspace(x_min, x_max, 60), minor=True)\n",
        "ax.set_xticks(np.linspace(y_min, y_max, 60), minor=True)\n",
        "plt.yticks(np.linspace(x_min, x_max, 6))  # Major ticks on y-axis\n",
        "plt.xticks(np.linspace(y_min, y_max, 4))  # Major ticks on x-axis\n",
        "\n",
        "# Set size of the plot\n",
        "plt.gcf().set_size_inches(9, 5)\n",
        "\n",
        "# Enable the minor grid lines\n",
        "ax.grid(which='minor', color='white', linestyle='-', linewidth=0.25)\n",
        "\n",
        "# Habilitar la cuadrÃ­cula en las marcas menores\n",
        "ax.grid(which='minor', color='white', linestyle='-', linewidth=0.25)\n",
        "\n",
        "# Save the image\n",
        "plt.savefig('output_with_contours.png', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204e0318",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3c8fe4a4",
      "metadata": {},
      "source": [
        "# CartPoleEnv \n",
        "\n",
        "### Observation Space\n",
        "\n",
        "The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
        "\n",
        "| Num | Observation           | Min                 | Max               |\n",
        "|-----|-----------------------|---------------------|-------------------|\n",
        "| 0   | Cart Position         | -4.8                | 4.8               |\n",
        "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24Â°) | ~ 0.418 rad (24Â°) |\n",
        "| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
        "\n",
        "### Action Space\n",
        "\n",
        "The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
        "of the fixed force the cart is pushed with.\n",
        "\n",
        "- 0: Push cart to the left\n",
        "- 1: Push cart to the right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c04b7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train cartpole environment:\n",
        "from classic_control.cartpole import CartPoleEnv\n",
        "# CartPole environment:\n",
        "env = CartPoleEnv(sutton_barto_reward=True)\n",
        "# position thresholds:\n",
        "x_lim         = 2.4\n",
        "theta_lim     = 0.418 \n",
        "# velocity thresholds:\n",
        "x_dot_lim     = 3.1\n",
        "theta_dot_lim = 3.1\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\"         : np.linspace(-x_lim, x_lim, 10,  dtype=np.float32),                     # position space          (0)\n",
        "    \"x_dot_space\"     : np.linspace(-x_dot_lim, x_dot_lim, 10,  dtype=np.float32),             # velocity space          (1)\n",
        "    \"theta_space\"     : np.linspace(-theta_lim, theta_lim, 10, dtype=np.float32),              # angle space             (2)\n",
        "    \"theta_dot_space\" : np.linspace(-theta_dot_lim, theta_dot_lim, 10, dtype=np.float32),      # angular velocity space  (3)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=np.array([0, 1], dtype=np.int32),\n",
        "    gamma=0.99,\n",
        "    theta=1e-3\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0462a904",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test cartpole environment:\n",
        "\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi = pickle.load(f)\n",
        "\n",
        "test_enviroment(CartPoleEnv(sutton_barto_reward=True, render_mode=\"human\"), pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "063b6002",
      "metadata": {},
      "source": [
        "## Observation Space\n",
        "\n",
        "The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
        "\n",
        "| Num | Observation                          | Min   | Max  | Unit         |\n",
        "|-----|--------------------------------------|-------|------|--------------|\n",
        "| 0   | position of the car along the x-axis | -1.2  | 0.6  | position (m) |\n",
        "| 1   | velocity of the car                  | -0.07 | 0.07 | velocity (v) |\n",
        "\n",
        "## Action Space\n",
        "\n",
        "There are 3 discrete deterministic actions:\n",
        "\n",
        "- 0: Accelerate to the left\n",
        "- 1: Don't accelerate\n",
        "- 2: Accelerate to the right\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d617686",
      "metadata": {},
      "outputs": [],
      "source": [
        "from classic_control.continuous_mountain_car import Continuous_MountainCarEnv\n",
        "\n",
        "env=Continuous_MountainCarEnv()\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\":     np.linspace(env.min_position, env.max_position, 100,      dtype=np.float32),    # position space    (0)\n",
        "    \"x_dot_space\": np.linspace(-abs(env.max_speed), abs(env.max_speed), 100, dtype=np.float32),    # velocity space    (1)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=np.linspace(-1.0, +1.0, 9, dtype=np.float32),\n",
        "    gamma=0.99,\n",
        "    theta=1e-3,\n",
        ")\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f556b5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test mountain car environment:\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi: PolicyIteration = pickle.load(f)\n",
        "\n",
        "test_enviroment(Continuous_MountainCarEnv(render_mode=\"human\"), pi)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DynamicProgramming",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
