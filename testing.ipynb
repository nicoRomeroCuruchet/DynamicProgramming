{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b149d005",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoRomeroCuruchet/DynamicProgramming/blob/main/testing_bary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002626e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from PolicyIteration import PolicyIteration\n",
        "from utils.utils import plot_2D_value_function,\\\n",
        "                        plot_3D_value_function,\\\n",
        "                        test_enviroment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c8fe4a4",
      "metadata": {},
      "source": [
        "# CartPoleEnv \n",
        "\n",
        "### Observation Space\n",
        "\n",
        "The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
        "\n",
        "| Num | Observation           | Min                 | Max               |\n",
        "|-----|-----------------------|---------------------|-------------------|\n",
        "| 0   | Cart Position         | -4.8                | 4.8               |\n",
        "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
        "| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
        "\n",
        "### Action Space\n",
        "\n",
        "The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
        "of the fixed force the cart is pushed with.\n",
        "\n",
        "- 0: Push cart to the left\n",
        "- 1: Push cart to the right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c04b7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train cartpole environment:\n",
        "\n",
        "from classic_control.cartpole import CartPoleEnv\n",
        "\n",
        "env = CartPoleEnv(sutton_barto_reward=True)\n",
        "# position thresholds:\n",
        "x_lim = 2.5\n",
        "theta_lim = 0.25 \n",
        "# velocity thresholds:\n",
        "x_dot_lim = 2.5\n",
        "theta_dot_lim = 2.5\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\": np.linspace(-x_lim, x_lim, 20),                         # position space         (0)\n",
        "    \"x_dot_space\": np.linspace(-x_dot_lim, x_dot_lim, 20),             # velocity space         (1)\n",
        "    \"theta_space\": np.linspace(-theta_lim, theta_lim, 20),             # angle space            (2)\n",
        "    \"theta_dot_space\": np.linspace(-theta_dot_lim, theta_dot_lim, 20), # angular velocity space (3)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=[0, 1],\n",
        "    gamma=0.99,\n",
        "    theta=1e-3\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0462a904",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test cartpole environment:\n",
        "\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi = pickle.load(f)\n",
        "\n",
        "test_enviroment(CartPoleEnv(sutton_barto_reward=True, render_mode=\"human\"), pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "063b6002",
      "metadata": {},
      "source": [
        "# Continuous_MountainCarEnv\n",
        "\n",
        "## Observation Space\n",
        "\n",
        "The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
        "\n",
        "| Num | Observation                          | Min  | Max | Unit         |\n",
        "|-----|--------------------------------------|------|-----|--------------|\n",
        "| 0   | position of the car along the x-axis | -Inf | Inf | position (m) |\n",
        "| 1   | velocity of the car                  | -Inf | Inf | position (m) |\n",
        "\n",
        "## Action Space\n",
        "\n",
        "The action is a `ndarray` with shape `(1,)`, representing the directional force applied on the car.\n",
        "The action is clipped in the range `[-1,1]` and multiplied by a power of 0.0015.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9d617686",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-08-29 18:39:07.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mPolicy Iteration was correctly initialized.\u001b[0m\n",
            "\u001b[32m2024-08-29 18:39:07.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mThe enviroment name is: Continuous_MountainCarEnv\u001b[0m\n",
            "\u001b[32m2024-08-29 18:39:07.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mThe action space is: [-1, 1]\u001b[0m\n",
            "\u001b[32m2024-08-29 18:39:07.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mNumber of states: 66049\u001b[0m\n",
            "\u001b[32m2024-08-29 18:39:07.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m409\u001b[0m - \u001b[1mGenerating transition and reward function table...\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:09<00:00, 6782.66it/s]\n",
            "\u001b[32m2024-08-29 18:39:17.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m411\u001b[0m - \u001b[1mTransition and reward function table generated.\u001b[0m\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[32m2024-08-29 18:39:17.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 0\u001b[0m\n",
            "\u001b[32m2024-08-29 18:39:17.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 18:39:18.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 99.9 | Avg Error: 5.2885 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:40:54.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 36.5666 | Avg Error: 9.6659 | 95<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:42:31.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 13.3846 | Avg Error: 4.1023 | 214<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:44:12.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 4.8992 | Avg Error: 1.6716 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:45:52.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 1.7933 | Avg Error: 0.6668 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:47:31.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.6564 | Avg Error: 0.2625 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:49:11.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.2403 | Avg Error: 0.1024 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:50:50.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0879 | Avg Error: 0.0396 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:52:29.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0322 | Avg Error: 0.0152 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:54:10.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0118 | Avg Error: 0.0058 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:55:52.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0043 | Avg Error: 0.0022 | 6815<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:57:35.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0016 | Avg Error: 0.0008 | 50297<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:59:20.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0006 | Avg Error: 0.0003 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 18:59:21.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 18:59:21.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 33215.73it/s]\n",
            "\u001b[32m2024-08-29 18:59:23.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        66049\u001b[0m\n",
            "\u001b[32m2024-08-29 18:59:23.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  1%|          | 1/100 [20:05<33:09:26, 1205.72s/it]\u001b[32m2024-08-29 18:59:23.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 1\u001b[0m\n",
            "\u001b[32m2024-08-29 18:59:23.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 18:59:24.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 1577.2881 | Avg Error: 97.5083 | 4490<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:01:33.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 15.9944 | Avg Error: 0.0506 | 64375<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:03:45.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0001 | Avg Error: 0.0001 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:03:46.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:03:46.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 40480.36it/s]\n",
            "\u001b[32m2024-08-29 19:03:48.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        9515\u001b[0m\n",
            "\u001b[32m2024-08-29 19:03:48.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  2%|▏         | 2/100 [24:30<17:45:26, 652.31s/it] \u001b[32m2024-08-29 19:03:48.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 2\u001b[0m\n",
            "\u001b[32m2024-08-29 19:03:48.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:03:49.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 4236.9616 | Avg Error: 17.0023 | 59785<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:06:04.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:06:05.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:06:05.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 38170.32it/s]\n",
            "\u001b[32m2024-08-29 19:06:07.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        4283\u001b[0m\n",
            "\u001b[32m2024-08-29 19:06:07.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  3%|▎         | 3/100 [26:49<11:15:35, 417.90s/it]\u001b[32m2024-08-29 19:06:07.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 3\u001b[0m\n",
            "\u001b[32m2024-08-29 19:06:07.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:06:08.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 4052.0012 | Avg Error: 7.8714 | 63821<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:08:08.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:08:08.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:08:08.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 36096.51it/s]\n",
            "\u001b[32m2024-08-29 19:08:10.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        3512\u001b[0m\n",
            "\u001b[32m2024-08-29 19:08:10.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  4%|▍         | 4/100 [28:53<8:02:40, 301.67s/it] \u001b[32m2024-08-29 19:08:10.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 4\u001b[0m\n",
            "\u001b[32m2024-08-29 19:08:10.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:08:12.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 3778.6676 | Avg Error: 5.7286 | 64680<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:10:32.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:10:32.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:10:32.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 37468.56it/s]\n",
            "\u001b[32m2024-08-29 19:10:34.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        3408\u001b[0m\n",
            "\u001b[32m2024-08-29 19:10:34.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  5%|▌         | 5/100 [31:16<6:27:34, 244.78s/it]\u001b[32m2024-08-29 19:10:34.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 5\u001b[0m\n",
            "\u001b[32m2024-08-29 19:10:34.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:10:36.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 3441.359 | Avg Error: 4.1182 | 64807<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:12:43.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:12:44.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:12:44.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 36371.00it/s]\n",
            "\u001b[32m2024-08-29 19:12:46.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        3352\u001b[0m\n",
            "\u001b[32m2024-08-29 19:12:46.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  6%|▌         | 6/100 [33:28<5:23:10, 206.28s/it]\u001b[32m2024-08-29 19:12:46.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 6\u001b[0m\n",
            "\u001b[32m2024-08-29 19:12:46.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:12:47.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 3236.8311 | Avg Error: 2.8998 | 64997<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:15:39.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:15:40.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:15:40.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 53206.91it/s]\n",
            "\u001b[32m2024-08-29 19:15:41.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2875\u001b[0m\n",
            "\u001b[32m2024-08-29 19:15:41.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  7%|▋         | 7/100 [36:23<5:04:06, 196.20s/it]\u001b[32m2024-08-29 19:15:41.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 7\u001b[0m\n",
            "\u001b[32m2024-08-29 19:15:41.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:15:42.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 2942.7564 | Avg Error: 1.9432 | 65239<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:17:32.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:17:32.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:17:32.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 46383.08it/s]\n",
            "\u001b[32m2024-08-29 19:17:34.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2383\u001b[0m\n",
            "\u001b[32m2024-08-29 19:17:34.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  8%|▊         | 8/100 [38:16<4:20:03, 169.60s/it]\u001b[32m2024-08-29 19:17:34.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 8\u001b[0m\n",
            "\u001b[32m2024-08-29 19:17:34.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:17:35.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 2781.2524 | Avg Error: 1.2181 | 65680<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:19:39.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:19:39.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:19:39.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 44162.55it/s]\n",
            "\u001b[32m2024-08-29 19:19:41.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2196\u001b[0m\n",
            "\u001b[32m2024-08-29 19:19:41.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  9%|▉         | 9/100 [40:23<3:57:01, 156.28s/it]\u001b[32m2024-08-29 19:19:41.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 9\u001b[0m\n",
            "\u001b[32m2024-08-29 19:19:41.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:19:42.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 2605.7542 | Avg Error: 0.8355 | 65852<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:21:36.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:21:37.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:21:37.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 51066.79it/s]\n",
            "\u001b[32m2024-08-29 19:21:38.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2024\u001b[0m\n",
            "\u001b[32m2024-08-29 19:21:38.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 10%|█         | 10/100 [42:21<3:36:26, 144.29s/it]\u001b[32m2024-08-29 19:21:38.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 10\u001b[0m\n",
            "\u001b[32m2024-08-29 19:21:38.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:21:39.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 2418.1564 | Avg Error: 0.6038 | 65922<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:23:35.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0005 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:23:35.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:23:35.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47369.39it/s]\n",
            "\u001b[32m2024-08-29 19:23:37.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2350\u001b[0m\n",
            "\u001b[32m2024-08-29 19:23:37.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 11%|█         | 11/100 [44:19<3:22:18, 136.39s/it]\u001b[32m2024-08-29 19:23:37.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 11\u001b[0m\n",
            "\u001b[32m2024-08-29 19:23:37.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:23:38.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 2066.1673 | Avg Error: 0.4205 | 65957<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:25:33.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:25:33.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:25:33.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49391.87it/s]\n",
            "\u001b[32m2024-08-29 19:25:35.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2351\u001b[0m\n",
            "\u001b[32m2024-08-29 19:25:35.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 12%|█▏        | 12/100 [46:17<3:11:49, 130.79s/it]\u001b[32m2024-08-29 19:25:35.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 12\u001b[0m\n",
            "\u001b[32m2024-08-29 19:25:35.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:25:36.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 1765.1329 | Avg Error: 0.3029 | 65957<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:27:31.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:27:32.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:27:32.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49122.30it/s]\n",
            "\u001b[32m2024-08-29 19:27:33.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2208\u001b[0m\n",
            "\u001b[32m2024-08-29 19:27:33.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 13%|█▎        | 13/100 [48:16<3:04:22, 127.16s/it]\u001b[32m2024-08-29 19:27:34.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 13\u001b[0m\n",
            "\u001b[32m2024-08-29 19:27:34.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:27:35.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 1396.5415 | Avg Error: 0.187 | 65969<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:29:32.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:29:32.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:29:32.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48381.60it/s]\n",
            "\u001b[32m2024-08-29 19:29:34.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2339\u001b[0m\n",
            "\u001b[32m2024-08-29 19:29:34.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 14%|█▍        | 14/100 [50:16<2:59:12, 125.03s/it]\u001b[32m2024-08-29 19:29:34.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 14\u001b[0m\n",
            "\u001b[32m2024-08-29 19:29:34.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:29:35.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 942.1188 | Avg Error: 0.1036 | 65986<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:31:33.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:31:33.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:31:33.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 46445.90it/s]\n",
            "\u001b[32m2024-08-29 19:31:35.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1937\u001b[0m\n",
            "\u001b[32m2024-08-29 19:31:35.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 15%|█▌        | 15/100 [52:17<2:55:26, 123.84s/it]\u001b[32m2024-08-29 19:31:35.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 15\u001b[0m\n",
            "\u001b[32m2024-08-29 19:31:35.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:31:36.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 461.7731 | Avg Error: 0.0383 | 66001<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:33:34.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:33:34.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:33:34.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 45970.06it/s]\n",
            "\u001b[32m2024-08-29 19:33:36.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2091\u001b[0m\n",
            "\u001b[32m2024-08-29 19:33:36.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 16%|█▌        | 16/100 [54:18<2:52:14, 123.03s/it]\u001b[32m2024-08-29 19:33:36.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 16\u001b[0m\n",
            "\u001b[32m2024-08-29 19:33:36.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:33:37.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 198.0386 | Avg Error: 0.0131 | 66023<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:35:33.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:35:34.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:35:34.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 50388.97it/s]\n",
            "\u001b[32m2024-08-29 19:35:36.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2347\u001b[0m\n",
            "\u001b[32m2024-08-29 19:35:36.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 17%|█▋        | 17/100 [56:18<2:48:47, 122.01s/it]\u001b[32m2024-08-29 19:35:36.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 17\u001b[0m\n",
            "\u001b[32m2024-08-29 19:35:36.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:35:37.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 64.9662 | Avg Error: 0.0026 | 66037<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:37:33.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:37:33.714\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:37:33.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49598.50it/s]\n",
            "\u001b[32m2024-08-29 19:37:35.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2320\u001b[0m\n",
            "\u001b[32m2024-08-29 19:37:35.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 18%|█▊        | 18/100 [58:17<2:45:33, 121.14s/it]\u001b[32m2024-08-29 19:37:35.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 18\u001b[0m\n",
            "\u001b[32m2024-08-29 19:37:35.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:37:36.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 20.5717 | Avg Error: 0.0005 | 66047<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:32.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:33.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:33.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 50259.43it/s]\n",
            "\u001b[32m2024-08-29 19:39:34.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        2367\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:34.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 19%|█▉        | 19/100 [1:00:17<2:42:56, 120.70s/it]\u001b[32m2024-08-29 19:39:34.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 19\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:34.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:36.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:36.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:36.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 50124.48it/s]\n",
            "\u001b[32m2024-08-29 19:39:38.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1654\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:38.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 20%|██        | 20/100 [1:00:20<1:53:55, 85.44s/it] \u001b[32m2024-08-29 19:39:38.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 20\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:38.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:39.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:39.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:39.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48752.14it/s]\n",
            "\u001b[32m2024-08-29 19:39:41.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1526\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:41.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 21%|██        | 21/100 [1:00:23<1:20:00, 60.77s/it]\u001b[32m2024-08-29 19:39:41.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 21\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:41.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:42.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:43.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:43.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 50334.55it/s]\n",
            "\u001b[32m2024-08-29 19:39:44.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1460\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:44.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 22%|██▏       | 22/100 [1:00:26<56:33, 43.51s/it]  \u001b[32m2024-08-29 19:39:44.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 22\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:44.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:45.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:46.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:46.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47968.71it/s]\n",
            "\u001b[32m2024-08-29 19:39:48.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1381\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:48.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 23%|██▎       | 23/100 [1:00:30<40:29, 31.55s/it]\u001b[32m2024-08-29 19:39:48.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 23\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:48.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:49.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:50.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:50.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47322.84it/s]\n",
            "\u001b[32m2024-08-29 19:39:51.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1249\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:51.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 24%|██▍       | 24/100 [1:00:33<29:14, 23.09s/it]\u001b[32m2024-08-29 19:39:51.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 24\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:51.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:52.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:53.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:53.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49282.97it/s]\n",
            "\u001b[32m2024-08-29 19:39:54.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1317\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:54.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 25%|██▌       | 25/100 [1:00:37<21:24, 17.13s/it]\u001b[32m2024-08-29 19:39:54.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 25\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:54.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:55.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:56.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:56.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49587.37it/s]\n",
            "\u001b[32m2024-08-29 19:39:57.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1119\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:57.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 26%|██▌       | 26/100 [1:00:40<15:58, 12.96s/it]\u001b[32m2024-08-29 19:39:58.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 26\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:58.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:39:59.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:00.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:00.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 44644.37it/s]\n",
            "\u001b[32m2024-08-29 19:40:01.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1190\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:01.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 27%|██▋       | 27/100 [1:00:43<12:20, 10.14s/it]\u001b[32m2024-08-29 19:40:01.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 27\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:01.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:02.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:03.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:03.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 50067.74it/s]\n",
            "\u001b[32m2024-08-29 19:40:04.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1312\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:04.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 28%|██▊       | 28/100 [1:00:47<09:40,  8.06s/it]\u001b[32m2024-08-29 19:40:04.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 28\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:04.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:05.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:06.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:06.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 50932.37it/s]\n",
            "\u001b[32m2024-08-29 19:40:07.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1210\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:07.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 29%|██▉       | 29/100 [1:00:50<07:47,  6.58s/it]\u001b[32m2024-08-29 19:40:07.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 29\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:07.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:09.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:09.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:09.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 50508.53it/s]\n",
            "\u001b[32m2024-08-29 19:40:11.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1160\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:11.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 30%|███       | 30/100 [1:00:53<06:27,  5.54s/it]\u001b[32m2024-08-29 19:40:11.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 30\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:11.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:12.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:12.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:12.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48509.27it/s]\n",
            "\u001b[32m2024-08-29 19:40:14.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1260\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:14.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 31%|███       | 31/100 [1:00:56<05:35,  4.86s/it]\u001b[32m2024-08-29 19:40:14.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 31\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:14.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:15.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:16.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:16.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49247.90it/s]\n",
            "\u001b[32m2024-08-29 19:40:17.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1293\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:17.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 32%|███▏      | 32/100 [1:00:59<04:56,  4.36s/it]\u001b[32m2024-08-29 19:40:17.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 32\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:17.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:18.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:19.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:19.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 50083.27it/s]\n",
            "\u001b[32m2024-08-29 19:40:20.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1255\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:20.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 33%|███▎      | 33/100 [1:01:03<04:28,  4.01s/it]\u001b[32m2024-08-29 19:40:20.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 33\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:20.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:21.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:22.543\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:22.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48497.71it/s]\n",
            "\u001b[32m2024-08-29 19:40:23.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1177\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:23.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 34%|███▍      | 34/100 [1:01:06<04:09,  3.79s/it]\u001b[32m2024-08-29 19:40:23.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 34\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:23.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:25.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:25.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:25.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49510.00it/s]\n",
            "\u001b[32m2024-08-29 19:40:27.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1150\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:27.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 35%|███▌      | 35/100 [1:01:09<03:55,  3.62s/it]\u001b[32m2024-08-29 19:40:27.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 35\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:27.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:28.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:29.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:29.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48290.34it/s]\n",
            "\u001b[32m2024-08-29 19:40:30.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1225\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:30.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 36%|███▌      | 36/100 [1:01:13<03:53,  3.65s/it]\u001b[32m2024-08-29 19:40:30.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 36\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:30.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:32.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:32.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:32.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 46900.01it/s]\n",
            "\u001b[32m2024-08-29 19:40:34.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1200\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:34.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 37%|███▋      | 37/100 [1:01:16<03:44,  3.56s/it]\u001b[32m2024-08-29 19:40:34.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 37\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:34.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:35.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:36.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:36.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49775.43it/s]\n",
            "\u001b[32m2024-08-29 19:40:37.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1135\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:37.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 38%|███▊      | 38/100 [1:01:19<03:34,  3.46s/it]\u001b[32m2024-08-29 19:40:37.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 38\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:37.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:38.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:39.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:39.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49509.46it/s]\n",
            "\u001b[32m2024-08-29 19:40:40.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1171\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:40.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 39%|███▉      | 39/100 [1:01:22<03:26,  3.38s/it]\u001b[32m2024-08-29 19:40:40.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 39\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:40.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:41.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:42.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:42.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 45963.59it/s]\n",
            "\u001b[32m2024-08-29 19:40:44.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1160\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:44.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 40%|████      | 40/100 [1:01:26<03:24,  3.40s/it]\u001b[32m2024-08-29 19:40:44.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 40\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:44.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:45.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:45.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:45.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47709.94it/s]\n",
            "\u001b[32m2024-08-29 19:40:47.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1057\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:47.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 41%|████      | 41/100 [1:01:29<03:18,  3.37s/it]\u001b[32m2024-08-29 19:40:47.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 41\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:47.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:48.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:49.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:49.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48535.49it/s]\n",
            "\u001b[32m2024-08-29 19:40:50.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1167\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:50.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 42%|████▏     | 42/100 [1:01:33<03:13,  3.34s/it]\u001b[32m2024-08-29 19:40:50.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 42\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:50.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:51.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:52.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:52.544\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48267.77it/s]\n",
            "\u001b[32m2024-08-29 19:40:53.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1219\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:53.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 43%|████▎     | 43/100 [1:01:36<03:09,  3.32s/it]\u001b[32m2024-08-29 19:40:53.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 43\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:53.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:55.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:55.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:55.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47612.13it/s]\n",
            "\u001b[32m2024-08-29 19:40:57.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1206\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:57.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 44%|████▍     | 44/100 [1:01:39<03:05,  3.31s/it]\u001b[32m2024-08-29 19:40:57.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 44\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:57.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:58.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:59.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:40:59.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 50596.47it/s]\n",
            "\u001b[32m2024-08-29 19:41:00.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1246\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:00.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 45%|████▌     | 45/100 [1:01:42<03:00,  3.27s/it]\u001b[32m2024-08-29 19:41:00.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 45\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:00.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:01.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:02.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:02.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49380.57it/s]\n",
            "\u001b[32m2024-08-29 19:41:03.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1225\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:03.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 46%|████▌     | 46/100 [1:01:45<02:55,  3.25s/it]\u001b[32m2024-08-29 19:41:03.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 46\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:03.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:04.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:05.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:05.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 51517.31it/s]\n",
            "\u001b[32m2024-08-29 19:41:06.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1232\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:06.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 47%|████▋     | 47/100 [1:01:49<02:50,  3.22s/it]\u001b[32m2024-08-29 19:41:06.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 47\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:06.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:07.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:08.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:08.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48983.56it/s]\n",
            "\u001b[32m2024-08-29 19:41:10.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1255\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:10.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 48%|████▊     | 48/100 [1:01:52<02:47,  3.23s/it]\u001b[32m2024-08-29 19:41:10.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 48\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:10.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:11.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:11.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:11.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49337.99it/s]\n",
            "\u001b[32m2024-08-29 19:41:13.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1151\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:13.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 49%|████▉     | 49/100 [1:01:55<02:44,  3.22s/it]\u001b[32m2024-08-29 19:41:13.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 49\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:13.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:14.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:15.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:15.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49966.41it/s]\n",
            "\u001b[32m2024-08-29 19:41:16.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1227\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:16.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 50%|█████     | 50/100 [1:01:58<02:41,  3.22s/it]\u001b[32m2024-08-29 19:41:16.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 50\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:16.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:17.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:18.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:18.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49996.56it/s]\n",
            "\u001b[32m2024-08-29 19:41:19.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1246\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:19.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 51%|█████     | 51/100 [1:02:01<02:37,  3.22s/it]\u001b[32m2024-08-29 19:41:19.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 51\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:19.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:20.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:21.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:21.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 51307.45it/s]\n",
            "\u001b[32m2024-08-29 19:41:22.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1209\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:22.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 52%|█████▏    | 52/100 [1:02:05<02:33,  3.20s/it]\u001b[32m2024-08-29 19:41:22.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 52\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:22.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:23.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:24.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:24.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 33836.83it/s]\n",
            "\u001b[32m2024-08-29 19:41:26.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1301\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:26.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 53%|█████▎    | 53/100 [1:02:08<02:38,  3.38s/it]\u001b[32m2024-08-29 19:41:26.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 53\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:26.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:27.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:28.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:28.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49486.84it/s]\n",
            "\u001b[32m2024-08-29 19:41:29.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1283\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:29.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 54%|█████▍    | 54/100 [1:02:12<02:34,  3.36s/it]\u001b[32m2024-08-29 19:41:29.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 54\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:29.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:31.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:31.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:31.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 45126.84it/s]\n",
            "\u001b[32m2024-08-29 19:41:33.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1236\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:33.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 55%|█████▌    | 55/100 [1:02:15<02:31,  3.36s/it]\u001b[32m2024-08-29 19:41:33.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 55\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:33.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:34.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:35.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:35.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48302.30it/s]\n",
            "\u001b[32m2024-08-29 19:41:36.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1258\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:36.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 56%|█████▌    | 56/100 [1:02:18<02:27,  3.35s/it]\u001b[32m2024-08-29 19:41:36.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 56\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:36.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:37.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:38.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:38.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49270.80it/s]\n",
            "\u001b[32m2024-08-29 19:41:39.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1261\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:39.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 57%|█████▋    | 57/100 [1:02:22<02:22,  3.32s/it]\u001b[32m2024-08-29 19:41:39.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 57\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:39.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:41.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:41.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:41.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49904.72it/s]\n",
            "\u001b[32m2024-08-29 19:41:43.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1288\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:43.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 58%|█████▊    | 58/100 [1:02:25<02:18,  3.30s/it]\u001b[32m2024-08-29 19:41:43.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 58\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:43.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:44.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:44.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:45.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48235.57it/s]\n",
            "\u001b[32m2024-08-29 19:41:46.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1255\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:46.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 59%|█████▉    | 59/100 [1:02:28<02:15,  3.29s/it]\u001b[32m2024-08-29 19:41:46.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 59\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:46.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:47.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:48.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:48.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47511.69it/s]\n",
            "\u001b[32m2024-08-29 19:41:49.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1303\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:49.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 60%|██████    | 60/100 [1:02:31<02:11,  3.28s/it]\u001b[32m2024-08-29 19:41:49.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 60\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:49.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:50.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:51.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:51.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49159.10it/s]\n",
            "\u001b[32m2024-08-29 19:41:52.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1221\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:52.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 61%|██████    | 61/100 [1:02:35<02:07,  3.27s/it]\u001b[32m2024-08-29 19:41:52.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 61\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:52.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:54.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:54.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:54.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49818.54it/s]\n",
            "\u001b[32m2024-08-29 19:41:56.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1208\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:56.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 62%|██████▏   | 62/100 [1:02:38<02:03,  3.25s/it]\u001b[32m2024-08-29 19:41:56.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 62\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:56.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:57.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:58.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:58.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 46787.80it/s]\n",
            "\u001b[32m2024-08-29 19:41:59.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1313\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:59.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 63%|██████▎   | 63/100 [1:02:41<02:01,  3.28s/it]\u001b[32m2024-08-29 19:41:59.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 63\u001b[0m\n",
            "\u001b[32m2024-08-29 19:41:59.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:00.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:01.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:01.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47141.95it/s]\n",
            "\u001b[32m2024-08-29 19:42:02.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1216\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:02.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 64%|██████▍   | 64/100 [1:02:45<01:58,  3.28s/it]\u001b[32m2024-08-29 19:42:02.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 64\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:02.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:03.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:04.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:04.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48346.75it/s]\n",
            "\u001b[32m2024-08-29 19:42:05.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1254\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:05.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 65%|██████▌   | 65/100 [1:02:48<01:54,  3.27s/it]\u001b[32m2024-08-29 19:42:06.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 65\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:06.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:07.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:07.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:07.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 44054.28it/s]\n",
            "\u001b[32m2024-08-29 19:42:09.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1265\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:09.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 66%|██████▌   | 66/100 [1:02:51<01:52,  3.31s/it]\u001b[32m2024-08-29 19:42:09.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 66\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:09.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:10.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:11.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:11.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48710.81it/s]\n",
            "\u001b[32m2024-08-29 19:42:12.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1236\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:12.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 67%|██████▋   | 67/100 [1:02:55<01:49,  3.33s/it]\u001b[32m2024-08-29 19:42:12.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 67\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:12.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:14.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:14.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:14.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 46932.33it/s]\n",
            "\u001b[32m2024-08-29 19:42:16.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1258\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:16.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 68%|██████▊   | 68/100 [1:02:58<01:46,  3.33s/it]\u001b[32m2024-08-29 19:42:16.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 68\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:16.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:17.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:17.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:17.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48731.20it/s]\n",
            "\u001b[32m2024-08-29 19:42:19.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1264\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:19.401\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 69%|██████▉   | 69/100 [1:03:01<01:42,  3.32s/it]\u001b[32m2024-08-29 19:42:19.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 69\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:19.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:20.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:21.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:21.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48865.52it/s]\n",
            "\u001b[32m2024-08-29 19:42:22.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1262\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:22.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 70%|███████   | 70/100 [1:03:05<01:39,  3.31s/it]\u001b[32m2024-08-29 19:42:22.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 70\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:22.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:23.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:24.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:24.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 46718.59it/s]\n",
            "\u001b[32m2024-08-29 19:42:26.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1233\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:26.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 71%|███████   | 71/100 [1:03:08<01:36,  3.32s/it]\u001b[32m2024-08-29 19:42:26.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 71\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:26.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:27.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:27.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:27.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 46426.44it/s]\n",
            "\u001b[32m2024-08-29 19:42:29.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1117\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:29.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 72%|███████▏  | 72/100 [1:03:11<01:33,  3.34s/it]\u001b[32m2024-08-29 19:42:29.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 72\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:29.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:30.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:31.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:31.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49458.71it/s]\n",
            "\u001b[32m2024-08-29 19:42:33.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1164\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:33.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 73%|███████▎  | 73/100 [1:03:15<01:35,  3.52s/it]\u001b[32m2024-08-29 19:42:33.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 73\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:33.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:34.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:35.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:35.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49806.14it/s]\n",
            "\u001b[32m2024-08-29 19:42:36.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1084\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:36.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 74%|███████▍  | 74/100 [1:03:19<01:30,  3.48s/it]\u001b[32m2024-08-29 19:42:36.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 74\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:36.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:37.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:38.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:38.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47824.46it/s]\n",
            "\u001b[32m2024-08-29 19:42:40.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1111\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:40.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 75%|███████▌  | 75/100 [1:03:22<01:25,  3.42s/it]\u001b[32m2024-08-29 19:42:40.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 75\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:40.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:41.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:41.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:41.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49583.50it/s]\n",
            "\u001b[32m2024-08-29 19:42:43.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1200\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:43.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 76%|███████▌  | 76/100 [1:03:25<01:21,  3.38s/it]\u001b[32m2024-08-29 19:42:43.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 76\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:43.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:44.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:45.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:45.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 45416.67it/s]\n",
            "\u001b[32m2024-08-29 19:42:46.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1111\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:46.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 77%|███████▋  | 77/100 [1:03:29<01:19,  3.44s/it]\u001b[32m2024-08-29 19:42:46.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 77\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:46.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:48.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:48.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:48.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48006.30it/s]\n",
            "\u001b[32m2024-08-29 19:42:50.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        993\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:50.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 78%|███████▊  | 78/100 [1:03:32<01:14,  3.38s/it]\u001b[32m2024-08-29 19:42:50.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 78\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:50.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:51.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:52.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:52.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48738.14it/s]\n",
            "\u001b[32m2024-08-29 19:42:53.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1027\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:53.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 79%|███████▉  | 79/100 [1:03:35<01:10,  3.35s/it]\u001b[32m2024-08-29 19:42:53.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 79\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:53.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:54.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:55.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:55.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49794.79it/s]\n",
            "\u001b[32m2024-08-29 19:42:56.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1073\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:56.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 80%|████████  | 80/100 [1:03:38<01:06,  3.32s/it]\u001b[32m2024-08-29 19:42:56.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 80\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:56.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:57.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:58.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:58.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47549.51it/s]\n",
            "\u001b[32m2024-08-29 19:42:59.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1134\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:59.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 81%|████████  | 81/100 [1:03:42<01:02,  3.31s/it]\u001b[32m2024-08-29 19:42:59.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 81\u001b[0m\n",
            "\u001b[32m2024-08-29 19:42:59.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:01.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:01.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:01.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 50245.69it/s]\n",
            "\u001b[32m2024-08-29 19:43:03.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1040\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:03.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 82%|████████▏ | 82/100 [1:03:45<00:59,  3.28s/it]\u001b[32m2024-08-29 19:43:03.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 82\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:03.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:04.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:05.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:05.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47258.02it/s]\n",
            "\u001b[32m2024-08-29 19:43:06.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1090\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:06.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 83%|████████▎ | 83/100 [1:03:48<00:55,  3.29s/it]\u001b[32m2024-08-29 19:43:06.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 83\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:06.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:07.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:08.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:08.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47711.13it/s]\n",
            "\u001b[32m2024-08-29 19:43:09.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1060\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:09.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 84%|████████▍ | 84/100 [1:03:52<00:52,  3.30s/it]\u001b[32m2024-08-29 19:43:09.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 84\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:09.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:11.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:11.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:11.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49406.16it/s]\n",
            "\u001b[32m2024-08-29 19:43:13.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        963\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:13.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 85%|████████▌ | 85/100 [1:03:55<00:49,  3.28s/it]\u001b[32m2024-08-29 19:43:13.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 85\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:13.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:14.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:14.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:14.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48360.81it/s]\n",
            "\u001b[32m2024-08-29 19:43:16.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1068\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:16.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 86%|████████▌ | 86/100 [1:03:58<00:46,  3.30s/it]\u001b[32m2024-08-29 19:43:16.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 86\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:16.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:17.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:18.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:18.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47669.51it/s]\n",
            "\u001b[32m2024-08-29 19:43:19.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1022\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:19.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 87%|████████▋ | 87/100 [1:04:02<00:43,  3.36s/it]\u001b[32m2024-08-29 19:43:19.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 87\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:19.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:21.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:21.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:21.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49535.29it/s]\n",
            "\u001b[32m2024-08-29 19:43:23.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        957\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:23.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 88%|████████▊ | 88/100 [1:04:05<00:39,  3.33s/it]\u001b[32m2024-08-29 19:43:23.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 88\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:23.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:24.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:25.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:25.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48426.01it/s]\n",
            "\u001b[32m2024-08-29 19:43:26.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1070\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:26.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 89%|████████▉ | 89/100 [1:04:08<00:36,  3.32s/it]\u001b[32m2024-08-29 19:43:26.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 89\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:26.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:27.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:28.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:28.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47552.87it/s]\n",
            "\u001b[32m2024-08-29 19:43:29.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1146\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:29.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 90%|█████████ | 90/100 [1:04:12<00:33,  3.30s/it]\u001b[32m2024-08-29 19:43:29.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 90\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:29.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:30.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:31.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:31.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47674.27it/s]\n",
            "\u001b[32m2024-08-29 19:43:32.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        972\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:32.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 91%|█████████ | 91/100 [1:04:15<00:29,  3.29s/it]\u001b[32m2024-08-29 19:43:32.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 91\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:32.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:34.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:34.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:34.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47819.24it/s]\n",
            "\u001b[32m2024-08-29 19:43:36.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1111\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:36.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 92%|█████████▏| 92/100 [1:04:18<00:26,  3.31s/it]\u001b[32m2024-08-29 19:43:36.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 92\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:36.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:37.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:38.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:38.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48278.09it/s]\n",
            "\u001b[32m2024-08-29 19:43:39.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1181\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:39.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 93%|█████████▎| 93/100 [1:04:21<00:23,  3.31s/it]\u001b[32m2024-08-29 19:43:39.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 93\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:39.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:40.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:41.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:41.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47433.70it/s]\n",
            "\u001b[32m2024-08-29 19:43:42.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1114\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:42.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 94%|█████████▍| 94/100 [1:04:25<00:19,  3.29s/it]\u001b[32m2024-08-29 19:43:42.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 94\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:42.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:44.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:44.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:44.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 46958.06it/s]\n",
            "\u001b[32m2024-08-29 19:43:46.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1150\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:46.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 95%|█████████▌| 95/100 [1:04:28<00:16,  3.29s/it]\u001b[32m2024-08-29 19:43:46.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 95\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:46.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:47.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:48.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:48.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47480.54it/s]\n",
            "\u001b[32m2024-08-29 19:43:49.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1026\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:49.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 96%|█████████▌| 96/100 [1:04:31<00:13,  3.30s/it]\u001b[32m2024-08-29 19:43:49.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 96\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:49.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:50.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:51.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:51.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 48093.45it/s]\n",
            "\u001b[32m2024-08-29 19:43:52.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        947\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:52.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 97%|█████████▋| 97/100 [1:04:35<00:09,  3.29s/it]\u001b[32m2024-08-29 19:43:52.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 97\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:52.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:53.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:54.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:54.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:02<00:00, 27900.96it/s]\n",
            "\u001b[32m2024-08-29 19:43:56.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1079\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:56.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 98%|█████████▊| 98/100 [1:04:39<00:07,  3.57s/it]\u001b[32m2024-08-29 19:43:56.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 98\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:56.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:58.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:58.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:43:58.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 49090.89it/s]\n",
            "\u001b[32m2024-08-29 19:44:00.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1159\u001b[0m\n",
            "\u001b[32m2024-08-29 19:44:00.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            " 99%|█████████▉| 99/100 [1:04:42<00:03,  3.48s/it]\u001b[32m2024-08-29 19:44:00.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1msolving step 99\u001b[0m\n",
            "\u001b[32m2024-08-29 19:44:00.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m338\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-08-29 19:44:01.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 66049<0.001\u001b[0m\n",
            "\u001b[32m2024-08-29 19:44:02.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-08-29 19:44:02.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 66049/66049 [00:01<00:00, 47647.68it/s]\n",
            "\u001b[32m2024-08-29 19:44:03.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m397\u001b[0m - \u001b[1mThe number of updated different actions:                        1114\u001b[0m\n",
            "\u001b[32m2024-08-29 19:44:03.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "100%|██████████| 100/100 [1:04:45<00:00, 38.86s/it]\n",
            "\u001b[32m2024-08-29 19:44:05.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m427\u001b[0m - \u001b[1mPolicy and value function saved.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from PolicyIteration import PolicyIteration\n",
        "from utils.utils import plot_2D_value_function,\\\n",
        "                        plot_3D_value_function,\\\n",
        "                        test_enviroment\n",
        "\n",
        "# Train mountain car environment:\n",
        "\n",
        "\n",
        "\n",
        "from classic_control.continuous_mountain_car import Continuous_MountainCarEnv\n",
        "\n",
        "env=Continuous_MountainCarEnv()\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\":     np.linspace(env.min_position, env.max_position, 257),      # position space         (0)\n",
        "    \"x_dot_space\": np.linspace(-abs(env.max_speed), abs(env.max_speed), 257), # velocity space         (1)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=[-1, 1],\n",
        "    gamma=0.99,\n",
        "    theta=1e-3,\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1c97891",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import KDTree\n",
        "import matplotlib.pyplot as plt \n",
        "delta = 257\n",
        "# Extract the x and y values\n",
        "x_values = np.linspace(env.min_position, env.max_position, delta)\n",
        "y_values = np.linspace(-abs(env.max_speed), abs(env.max_speed), int(delta))\n",
        "X, Y = np.meshgrid(x_values, y_values)\n",
        "cartesian_product = np.c_[X.ravel(), Y.ravel()]\n",
        "#avoid repeating the same point\n",
        "cartesian_product = np.unique(cartesian_product, axis=0)\n",
        "# Highlighted point\n",
        "highlight_point = (0.25, 0.0606981)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(30,30))\n",
        "plt.plot(X.ravel(), Y.ravel(), 'go', label='Data Points', markersize=4)\n",
        "\n",
        "plt.plot(highlight_point[0], highlight_point[1], 'bx', label='Highlighted Point', markersize=2)\n",
        "\n",
        "tree = KDTree(np.c_[X.ravel(), Y.ravel()])\n",
        "\n",
        "dd, ii = tree.query((highlight_point[0], highlight_point[1]), k=20)\n",
        "#print(ii)\n",
        "# create a simplex\n",
        "simplex = np.c_[X.ravel()[ii], Y.ravel()[ii]]\n",
        "x_cord = simplex[0][0]\n",
        "index_x = 0\n",
        "for s in simplex:\n",
        "    if s[0] != x_cord: \n",
        "        break\n",
        "    index_x += 1\n",
        "\n",
        "#simplex = np.vstack([simplex, np.array([X.ravel()[ii][2], Y.ravel()[ii][2]])])\n",
        "simplex = np.c_[X.ravel()[ii[:2]], Y.ravel()[ii[:2]]]\n",
        "simplex = np.vstack([simplex, np.array([X.ravel()[ii[index_x]], Y.ravel()[ii[index_x]]])])\n",
        "\n",
        "# get bariocentric coordinates\n",
        "A = np.vstack([np.array(simplex).T, np.ones(len(simplex))])\n",
        "#b = np.hstack([highlight_point, [1]])\n",
        "\n",
        "# get the inverse of A\n",
        "A_inv = np.linalg.inv(A)\n",
        "\n",
        "# plot nearest neighbors\n",
        "#plt.plot(X.ravel()[ii[:2]], Y.ravel()[ii[:2]], 'yo', label='Nearest Neighbors', markersize=4)\n",
        "#plt.plot(X.ravel()[ii[index_x]], Y.ravel()[ii[index_x]], 'yo', label='Nearest Neighbors', markersize=4)\n",
        "# plot\n",
        "\n",
        "a = np.array([ 0.31875  ,  -0.07      ])\n",
        "b = np.array([ 0.31875 ,   -0.06945313])\n",
        "c = np.array ([ 0.32578125, -0.07      ])\n",
        "p = np.array([ 0.3190625 ,-0.07     ])\n",
        "plt.plot(a[0], a[1], 'ro', label='A', markersize=2)\n",
        "plt.plot(b[0], b[1], 'ro', label='B', markersize=2)\n",
        "plt.plot(c[0], c[1], 'ro', label='C', markersize=2)\n",
        "plt.plot(p[0], p[1], 'ro', label='P', markersize=5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9f556b5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 1 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 2 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 3 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 4 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 5 finished after 67 timesteps\n",
            "Total reward: 93.30000000000001\n",
            "Episode 6 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 7 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 8 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 9 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 10 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 11 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 12 finished after 67 timesteps\n",
            "Total reward: 93.30000000000001\n",
            "Episode 13 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 14 finished after 67 timesteps\n",
            "Total reward: 93.30000000000001\n",
            "Episode 15 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 16 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 17 finished after 67 timesteps\n",
            "Total reward: 93.30000000000001\n",
            "Episode 18 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 19 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 20 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 21 finished after 67 timesteps\n",
            "Total reward: 93.30000000000001\n",
            "Episode 22 finished after 67 timesteps\n",
            "Total reward: 93.30000000000001\n",
            "Episode 23 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 24 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 25 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 26 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 27 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 28 finished after 67 timesteps\n",
            "Total reward: 93.30000000000001\n",
            "Episode 29 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 30 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 31 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 32 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 33 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 34 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 35 finished after 67 timesteps\n",
            "Total reward: 93.30000000000001\n",
            "Episode 36 finished after 66 timesteps\n",
            "Total reward: 93.4\n",
            "Episode 37 finished after 67 timesteps\n",
            "Total reward: 93.30000000000001\n",
            "Episode 38 finished after 67 timesteps\n",
            "Total reward: 93.30000000000001\n",
            "Episode 39 finished after 65 timesteps\n",
            "Total reward: 93.50000000000001\n",
            "Episode 40 finished after 66 timesteps\n",
            "Total reward: 93.4\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(env\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     pi: PolicyIteration \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m----> 6\u001b[0m test_enviroment(Continuous_MountainCarEnv(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m), pi)\n",
            "File \u001b[1;32mc:\\users\\nicor\\dynamicprogramming\\src\\utils\\utils.py:183\u001b[0m, in \u001b[0;36mtest_enviroment\u001b[1;34m(task, pi, num_episodes, episode_lengh, option_reset)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, episode_lengh):\n\u001b[0;32m    182\u001b[0m     action \u001b[38;5;241m=\u001b[39m get_optimal_action(observation, pi)\n\u001b[1;32m--> 183\u001b[0m     observation, reward, terminated, _, _ \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    184\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated:\n",
            "File \u001b[1;32mc:\\Users\\nicor\\DynamicProgramming\\classic_control\\continuous_mountain_car.py:182\u001b[0m, in \u001b[0;36mContinuous_MountainCarEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([position, velocity], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
            "File \u001b[1;32mc:\\Users\\nicor\\DynamicProgramming\\classic_control\\continuous_mountain_car.py:296\u001b[0m, in \u001b[0;36mContinuous_MountainCarEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    295\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    297\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Test mountain car environment:\n",
        "\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi: PolicyIteration = pickle.load(f)\n",
        "\n",
        "test_enviroment(Continuous_MountainCarEnv(render_mode=\"human\"), pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80bb9473",
      "metadata": {},
      "outputs": [],
      "source": [
        "# graph the value function of the mountain car environment:\n",
        "plot_3D_value_function(pi.value_function)\n",
        "plot_2D_value_function(pi.value_function)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
