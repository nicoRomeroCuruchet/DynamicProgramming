{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b149d005",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoRomeroCuruchet/DynamicProgramming/blob/main/testing_bary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002626e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from utils.utils import test_enviroment\n",
        "from PolicyIteration import PolicyIteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b5a85a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import airplane\n",
        "\n",
        "glider = gym.make('ReducedSymmetricGliderPullout-v0')\n",
        "\n",
        "bins_space = {\n",
        "    \"flight_path_angle\": np.linspace(-np.pi, 0.5,    100,      dtype=np.float32),     # Flight Path Angle (γ)    (0)\n",
        "    \"airspeed_norm\":     np.linspace(0.7, 4.0,       100,      dtype=np.float32),     # Air Speed         (V)    (1)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=glider, \n",
        "    bins_space=bins_space,\n",
        "    action_space=np.linspace(-0.4, 1.0, 15, dtype=np.float32),\n",
        "    gamma=0.99,\n",
        "    theta=1e-3,\n",
        ")\n",
        "\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9c6a7123",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-02-13 13:29:44.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mLower bounds: [-3.1415927   0.7        -0.34906584]\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:44.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mUpper bounds: [0.5       4.        3.4906585]\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:44.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1mThe action space is: [[-0.4       -0.5235988]\n",
            " [-0.4       -0.2617994]\n",
            " [-0.4        0.       ]\n",
            " [-0.4        0.2617994]\n",
            " [-0.4        0.5235988]\n",
            " [-0.05      -0.5235988]\n",
            " [-0.05      -0.2617994]\n",
            " [-0.05       0.       ]\n",
            " [-0.05       0.2617994]\n",
            " [-0.05       0.5235988]\n",
            " [ 0.3       -0.5235988]\n",
            " [ 0.3       -0.2617994]\n",
            " [ 0.3        0.       ]\n",
            " [ 0.3        0.2617994]\n",
            " [ 0.3        0.5235988]\n",
            " [ 0.65      -0.5235988]\n",
            " [ 0.65      -0.2617994]\n",
            " [ 0.65       0.       ]\n",
            " [ 0.65       0.2617994]\n",
            " [ 0.65       0.5235988]\n",
            " [ 1.        -0.5235988]\n",
            " [ 1.        -0.2617994]\n",
            " [ 1.         0.       ]\n",
            " [ 1.         0.2617994]\n",
            " [ 1.         0.5235988]]\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:44.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m139\u001b[0m - \u001b[1mNumber of states: 8000\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:44.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mTotal states:200000\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:44.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mPolicy Iteration was correctly initialized.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:44.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m154\u001b[0m - \u001b[1mThe enviroment name is: TimeLimit\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:44.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m368\u001b[0m - \u001b[1mCreating Delaunay triangulation over the state space...\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:45.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m370\u001b[0m - \u001b[1mDelaunay triangulation created.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:45.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m380\u001b[0m - \u001b[1mGenerating transition and reward function table...\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:45.093\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:48.001\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:50.809\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:53.106\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:55.676\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:29:58.275\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:00.714\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:03.086\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:05.113\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:07.451\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:09.738\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:12.088\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:15.296\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:17.404\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:19.707\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:22.323\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:25.083\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:27.929\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:30.378\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:33.008\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:35.874\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:38.668\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:42.425\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:44.634\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:46.801\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mcalculate_transition_reward_table\u001b[0m:\u001b[36m245\u001b[0m - \u001b[33m\u001b[1mSome states are outside the bounds of the environment.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:49.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mTransition and reward function table generated.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:49.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1msolving step 0\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:49.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:49.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.4009999930858612 | Avg Error: 0.125 | 1117<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:50.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.05900000035762787 | Avg Error: 0.028999999165534973 | 1100<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:52.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.00800000037997961 | Avg Error: 0.004000000189989805 | 1399<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:54.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0020000000949949026 | Avg Error: 0.0010000000474974513 | 4250<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:56.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:56.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:56.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:56.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mThe number of updated different actions: 200000\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:56.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:56.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1msolving step 1\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:56.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:56.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 2.805999994277954 | Avg Error: 0.335999995470047 | 1080<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:57.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.04399999976158142 | Avg Error: 0.008999999612569809 | 3853<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:30:59.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.006000000052154064 | Avg Error: 0.0020000000949949026 | 4357<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:01.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0010000000474974513 | Avg Error: 0.0 | 7587<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:03.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:03.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:03.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:03.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mThe number of updated different actions: 6700\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:03.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:03.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1msolving step 2\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:03.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:03.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 1.0110000371932983 | Avg Error: 0.020999999716877937 | 4980<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:05.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0010000000474974513 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:07.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:07.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:07.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:07.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mThe number of updated different actions: 4250\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:07.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:07.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1msolving step 3\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:07.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:07.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.9120000004768372 | Avg Error: 0.009999999776482582 | 6239<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:10.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:10.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:10.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:10.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mThe number of updated different actions: 2664\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:10.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:10.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1msolving step 4\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:10.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:10.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.6650000214576721 | Avg Error: 0.006000000052154064 | 7079<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:13.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:13.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:13.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:13.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mThe number of updated different actions: 894\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:13.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:13.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1msolving step 5\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:13.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:13.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.6110000014305115 | Avg Error: 0.004999999888241291 | 7690<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:15.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0020000000949949026 | Avg Error: 0.0 | 7989<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:17.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:17.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:17.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:17.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mThe number of updated different actions: 674\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:17.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:17.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1msolving step 6\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:17.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:17.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.7559999823570251 | Avg Error: 0.003000000026077032 | 7737<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:18.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:18.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:18.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:18.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mThe number of updated different actions: 356\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:18.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:18.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1msolving step 7\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:18.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:18.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.210999995470047 | Avg Error: 0.0010000000474974513 | 7849<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:20.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:20.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:20.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:20.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mThe number of updated different actions: 102\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:20.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:20.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1msolving step 8\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:20.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:20.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.10599999874830246 | Avg Error: 0.0 | 7955<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:22.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:22.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:22.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:22.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mThe number of updated different actions: 30\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:22.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:22.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1msolving step 9\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:22.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m294\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:22.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.02500000037252903 | Avg Error: 0.0 | 7989<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:26.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mMax Error: 0.0 | Avg Error: 0.0 | 8000<0.001\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:26.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:26.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:26.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "\u001b[32m2025-02-13 13:31:26.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1mPolicy and value function saved.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import airplane\n",
        "import pickle\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from utils.utils import test_enviroment\n",
        "from PolicyIteration import PolicyIteration\n",
        "\n",
        "glider = gym.make('ReducedBankedGliderPullout-v0')\n",
        "\n",
        "bins_space = {\n",
        "    \"flight_path_angle\": np.linspace(-np.pi, 0.5,    20,      dtype=np.float32),     # Flight Path Angle (γ)    (0)\n",
        "    \"airspeed_norm\":     np.linspace(0.7, 4.0,       20,      dtype=np.float32),     # Air Speed         (V)    (1)\n",
        "    \"bank_angle\":        np.linspace( np.deg2rad(-20), np.deg2rad(200),  20,      dtype=np.float32),     # Bank Angle        (mu)    (2)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=glider, \n",
        "    bins_space=bins_space,\n",
        "    # cross product np.linspace(-0.4, 1.0, 15, dtype=np.float32) X np.linspace(np.deg2rad(-30), np.deg2rad(30), 15, dtype=np.float32)\n",
        "    action_space= np.array(np.meshgrid(np.linspace(-0.4, 1.0, 5, dtype=np.float32), np.linspace(np.deg2rad(-30), np.deg2rad(30), 5, dtype=np.float32))).T.reshape(-1, 2),\n",
        "    #action_space=np.linspace(-0.4, 1.0, 15, dtype=np.float32),\n",
        "    gamma=0.99,\n",
        "    theta=1e-3,\n",
        ")\n",
        "\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "626c9909",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.linspace(-0.4, 1.0, 15, dtype=np.float32)\n",
        "np.linspace(np.deg2rad(-30), np.deg2rad(30), 15, dtype=np.float32)\n",
        "# cartesian product of action space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2f30c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "STALL_AIRSPEED = 27.331231856346\n",
        "from utils.utils import get_optimal_action\n",
        "\n",
        "for action in np.linspace(-0.4, 1.0, 15, dtype=np.float32):\n",
        "\n",
        "    state = np.array([0.05, 1.83])\n",
        "    glider.airplane.flight_path_angle = state[0]\n",
        "    glider.airplane.airspeed_norm = state[1]\n",
        "    \n",
        "    total_reward = 0\n",
        "    episode_length = 0\n",
        "\n",
        "    while episode_length <100:\n",
        "        #action = get_optimal_action(state, pi)\n",
        "        prev_state  = state.copy()\n",
        "        state, reward, _, _, _ = glider.step(action)\n",
        "        state = state[0]\n",
        "        total_reward += reward*STALL_AIRSPEED\n",
        "        episode_length += 1\n",
        "    \n",
        "    print(f\"Action: {action} Reward: {total_reward} Episode Length: {episode_length}\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd13daf",
      "metadata": {},
      "outputs": [],
      "source": [
        "STALL_AIRSPEED = 27.331231856346\n",
        "from utils.utils import get_optimal_action\n",
        "from tqdm import tqdm\n",
        "with open(glider.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi: PolicyIteration = pickle.load(f)\n",
        "\n",
        "prom_episode_lenght = 0\n",
        "dict_result = {}\n",
        "dict_episode_length = {}\n",
        "state_spaces = [v for v in pi.states_space if v[0] > -np.pi/2]\n",
        "for state in tqdm(state_spaces):\n",
        "    initial_state = state.copy()\n",
        "    prev_state = state.copy()\n",
        "    glider.reset()\n",
        "    glider.airplane.flight_path_angle = state[0]\n",
        "    glider.airplane.airspeed_norm = state[1]\n",
        "    done = False\n",
        "    episode_length = 0\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = get_optimal_action(state, pi)\n",
        "        prev_state  = state.copy()\n",
        "        state, reward, done, _, _ = glider.step(action)\n",
        "        done  = bool(done)\n",
        "        if done:\n",
        "            break\n",
        "        state = state[0]\n",
        "        # check if our state is in the state space\n",
        "        index = pi.triangulation.find_simplex(state)\n",
        "        if not done:\n",
        "            total_reward -= reward#*STALL_AIRSPEED\n",
        "        if (index ==-1) or episode_length > 70: \n",
        "            done = True\n",
        "        episode_length += 1\n",
        "        \n",
        "    dict_result[tuple(initial_state)] = total_reward\n",
        "    dict_episode_length[tuple(initial_state)] = episode_length\n",
        "    prom_episode_lenght += episode_length / len(pi.states_space)\n",
        "    #print(f\"Initial state: {initial_state} - Total reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab055601",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "epsilon = 1*1e-1\n",
        "#dict_result = dict_episode_length\n",
        "# Your dictionary with keys as coordinates (x, y) and values as the color intensity (z)\n",
        "# Extracting the x, y and z values\n",
        "x = [np.degrees(coord[0]) for coord in dict_result.keys() if 0.05 >= coord[0] >= -np.pi/2 - epsilon and coord[1] > 0.7]\n",
        "#convert x to grad\n",
        "y = [coord[1] for coord in dict_result.keys() if 0.05 >= coord[0] >= -np.pi/2 - epsilon and coord[1] > 0.7]\n",
        "#convert y to Vs\n",
        "keys_list = list(dict_result.keys())\n",
        "values_list = list(dict_result.values())\n",
        "z = [v for e,v in zip(keys_list, values_list) if 0.05 >= e[0] >= -np.pi/2 - epsilon and e[1] > 0.7]\n",
        "\n",
        "# Creating a 2D scatter plot\n",
        "cmap = plt.get_cmap('viridis', 2048)\n",
        "plt.tricontourf(y, x, z, cmap=cmap, levels=18)   # Change 'viridis' to any other colormap you like\n",
        "plt.colorbar(label='', shrink=0.8,  )  # Add color bar for the z values\n",
        "\n",
        "# Add labels and a title\n",
        "plt.ylabel('Flight Path Angle (γ) [deg]')\n",
        "plt.xlabel('V/Vs')\n",
        "\n",
        "x_min, x_max = min(x), max(x)\n",
        "y_min, y_max = min(y), max(y)\n",
        "\n",
        "# Set the minor ticks for the grid, keeping the dense grid with smaller squares\n",
        "ax = plt.gca()  # Get current axes\n",
        "ax.set_yticks(np.linspace(x_min, x_max, 60), minor=True)  # 20 minor ticks for grid\n",
        "ax.set_xticks(np.linspace(y_min, y_max, 60), minor=True)  # 20 minor ticks for grid\n",
        "# put line x = 1\n",
        "plt.axvline(x=1, color='r', linestyle='--', linewidth=0.5)\n",
        "# put line y = 0\n",
        "#plt.axhline(y=0, color='k', linestyle='--')\n",
        "\n",
        "# Show the plot\n",
        "vals = np.round(np.linspace(-90, 0,6))\n",
        "plt.yticks(vals)  # Only 5 labels on x-axis\n",
        "plt.xticks(np.linspace(round(y_min), round(y_max), 6))  # Only 5 labels on y-axis\n",
        "# Enable the minor grid lines\n",
        "ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
        "\n",
        "# Habilitar la cuadrícula en las marcas menores\n",
        "ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
        "\n",
        "# Guardar la imagen sin borde blanco\n",
        "plt.savefig('output.png', bbox_inches='tight')\n",
        "\n",
        "# set size of the plot\n",
        "plt.gcf().set_size_inches(9, 5)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8416867",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "# Your dictionary with keys as coordinates (x, y) and values as the color intensity (z)\n",
        "# Extracting the x, y and z values\n",
        "x = [np.degrees(coord[0]) for coord in dict_result.keys() if 5 >= coord[0] >= -np.pi/2 and coord[1] > 0.7]\n",
        "y = [coord[1] for coord in dict_result.keys() if 5 >= coord[0] >= -np.pi/2 and coord[1] > 0.7]\n",
        "keys_list = list(dict_result.keys())\n",
        "values_list = list(dict_result.values())\n",
        "z = [v for e,v in zip(keys_list, values_list) if 5 >= e[0] >= -np.pi/2 and e[1] > 0.7]\n",
        "\n",
        "# Create a grid for interpolation\n",
        "x_grid = np.linspace(min(x), max(x), 100)\n",
        "y_grid = np.linspace(min(y), max(y), 100)\n",
        "X, Y = np.meshgrid(x_grid, y_grid)\n",
        "\n",
        "# Interpolate the scattered data into the grid\n",
        "Z = griddata((y, x), z, (Y, X), method='linear')\n",
        "\n",
        "# Create the scatter plot\n",
        "cmap = plt.get_cmap('viridis', 2048)\n",
        "plt.scatter(y, x, c=z, cmap=cmap)\n",
        "\n",
        "# Add color bar\n",
        "plt.colorbar(label='', shrink=0.8)\n",
        "contour_levels = np.linspace(np.min(z), np.max(z), 10)  # 10 levels\n",
        "contour_levels = contour_levels[contour_levels != 0]    # Remove zero from levels\n",
        "\n",
        "# Add contour lines on top of the scatter plot\n",
        "contour = plt.contour(Y, X, Z, levels=contour_levels, colors='black', linewidths=0.75)\n",
        "plt.clabel(contour, inline=True, fontsize=8)  # Optional: add labels to the contours\n",
        "\n",
        "# Add labels and a title\n",
        "plt.ylabel('Flight Path Angle (γ) [deg]')\n",
        "plt.xlabel('V/Vs')\n",
        "\n",
        "# Plot vertical line at x=1\n",
        "plt.axvline(x=1, color='k', linestyle='--')\n",
        "\n",
        "# Configure the minor ticks and grid\n",
        "x_min, x_max = min(x), max(x)\n",
        "y_min, y_max = min(y), max(y)\n",
        "ax = plt.gca()  # Get current axes\n",
        "ax.set_yticks(np.linspace(x_min, x_max, 60), minor=True)\n",
        "ax.set_xticks(np.linspace(y_min, y_max, 60), minor=True)\n",
        "plt.yticks(np.linspace(x_min, x_max, 6))  # Major ticks on y-axis\n",
        "plt.xticks(np.linspace(y_min, y_max, 4))  # Major ticks on x-axis\n",
        "\n",
        "# Set size of the plot\n",
        "plt.gcf().set_size_inches(9, 5)\n",
        "\n",
        "# Enable the minor grid lines\n",
        "ax.grid(which='minor', color='white', linestyle='-', linewidth=0.25)\n",
        "\n",
        "# Habilitar la cuadrícula en las marcas menores\n",
        "ax.grid(which='minor', color='white', linestyle='-', linewidth=0.25)\n",
        "\n",
        "# Save the image\n",
        "plt.savefig('output_with_contours.png', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204e0318",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3c8fe4a4",
      "metadata": {},
      "source": [
        "# CartPoleEnv \n",
        "\n",
        "### Observation Space\n",
        "\n",
        "The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
        "\n",
        "| Num | Observation           | Min                 | Max               |\n",
        "|-----|-----------------------|---------------------|-------------------|\n",
        "| 0   | Cart Position         | -4.8                | 4.8               |\n",
        "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
        "| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
        "\n",
        "### Action Space\n",
        "\n",
        "The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
        "of the fixed force the cart is pushed with.\n",
        "\n",
        "- 0: Push cart to the left\n",
        "- 1: Push cart to the right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c04b7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train cartpole environment:\n",
        "from classic_control.cartpole import CartPoleEnv\n",
        "# CartPole environment:\n",
        "env = CartPoleEnv(sutton_barto_reward=True)\n",
        "# position thresholds:\n",
        "x_lim         = 2.4\n",
        "theta_lim     = 0.418 \n",
        "# velocity thresholds:\n",
        "x_dot_lim     = 3.1\n",
        "theta_dot_lim = 3.1\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\"         : np.linspace(-x_lim, x_lim, 10,  dtype=np.float32),                     # position space          (0)\n",
        "    \"x_dot_space\"     : np.linspace(-x_dot_lim, x_dot_lim, 10,  dtype=np.float32),             # velocity space          (1)\n",
        "    \"theta_space\"     : np.linspace(-theta_lim, theta_lim, 10, dtype=np.float32),              # angle space             (2)\n",
        "    \"theta_dot_space\" : np.linspace(-theta_dot_lim, theta_dot_lim, 10, dtype=np.float32),      # angular velocity space  (3)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=np.array([0, 1], dtype=np.int32),\n",
        "    gamma=0.99,\n",
        "    theta=1e-3\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0462a904",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test cartpole environment:\n",
        "\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi = pickle.load(f)\n",
        "\n",
        "test_enviroment(CartPoleEnv(sutton_barto_reward=True, render_mode=\"human\"), pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "063b6002",
      "metadata": {},
      "source": [
        "## Observation Space\n",
        "\n",
        "The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
        "\n",
        "| Num | Observation                          | Min   | Max  | Unit         |\n",
        "|-----|--------------------------------------|-------|------|--------------|\n",
        "| 0   | position of the car along the x-axis | -1.2  | 0.6  | position (m) |\n",
        "| 1   | velocity of the car                  | -0.07 | 0.07 | velocity (v) |\n",
        "\n",
        "## Action Space\n",
        "\n",
        "There are 3 discrete deterministic actions:\n",
        "\n",
        "- 0: Accelerate to the left\n",
        "- 1: Don't accelerate\n",
        "- 2: Accelerate to the right\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d617686",
      "metadata": {},
      "outputs": [],
      "source": [
        "from classic_control.continuous_mountain_car import Continuous_MountainCarEnv\n",
        "\n",
        "env=Continuous_MountainCarEnv()\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\":     np.linspace(env.min_position, env.max_position, 100,      dtype=np.float32),    # position space    (0)\n",
        "    \"x_dot_space\": np.linspace(-abs(env.max_speed), abs(env.max_speed), 100, dtype=np.float32),    # velocity space    (1)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=np.linspace(-1.0, +1.0, 9, dtype=np.float32),\n",
        "    gamma=0.99,\n",
        "    theta=1e-3,\n",
        ")\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f556b5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test mountain car environment:\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi: PolicyIteration = pickle.load(f)\n",
        "\n",
        "test_enviroment(Continuous_MountainCarEnv(render_mode=\"human\"), pi)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
