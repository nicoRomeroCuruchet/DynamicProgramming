{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b149d005",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoRomeroCuruchet/DynamicProgramming/blob/main/testing_bary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "beeb377e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from PolicyIteration import PolicyIteration \n",
        "\n",
        "def get_optimal_action(state:np.array, optimal_policy:PolicyIteration):\n",
        "    \"\"\"\n",
        "    Aproximate the optimal action for a given state using the provided optimal policy\n",
        "    with barycentric interpolation.\n",
        "\n",
        "    Parameters:\n",
        "    state (np.array): The state for which to determine the optimal action.\n",
        "    optimal_policy (PolicyIteration): The optimal policy used to determine the action.\n",
        "\n",
        "    Returns:\n",
        "    action: The optimal action for the given state.\n",
        "    \"\"\"\n",
        "    _, neighbors  = optimal_policy.kd_tree.query([state], k=optimal_policy.num_simplex_points)\n",
        "    simplex       = optimal_policy.points[neighbors[0]]\n",
        "    lambdas       = optimal_policy.barycentric_coordinates(state, simplex)\n",
        "\n",
        "    actions = optimal_policy.action_space\n",
        "    probabilities = np.zeros(len(actions))\n",
        "\n",
        "    for i, l in enumerate(lambdas):\n",
        "        for j, action in enumerate(actions):\n",
        "            if optimal_policy.policy[tuple(simplex[i])][action] > 0:\n",
        "                probabilities[j] += l\n",
        "\n",
        "    argmax = lambda x: max(enumerate(x), key=lambda x: x[1])[0]\n",
        "    action = actions[argmax(probabilities)]\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "def test_enviroment(task: gym.Env, \n",
        "                    pi: PolicyIteration, \n",
        "                    num_episodes: int = 10000, \n",
        "                    episode_lengh: int = 1000):\n",
        "    \"\"\"\n",
        "    Test the environment using the given policy iteration algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    - task (gym.Env): The environment to test.\n",
        "    - pi (PolicyIteration): The policy iteration algorithm.\n",
        "    - num_episodes (int): The number of episodes to run. Default is 10000.\n",
        "    - episode_lengh (int): The maximum length of each episode. Default is 1000.\n",
        "    \"\"\"\n",
        "\n",
        "    for episode in range(0, num_episodes):\n",
        "        total_reward = 0\n",
        "        observation, _ = task.reset()\n",
        "        for timestep in range(1, episode_lengh):\n",
        "            action = get_optimal_action(observation, pi)\n",
        "            observation, reward, terminated, _, _ = task.step(action)\n",
        "            total_reward += reward\n",
        "            if terminated:\n",
        "                print(f\"Episode {episode} finished after {timestep} timesteps\")\n",
        "                print(f\"Total reward: {total_reward}\")\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c04b7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train cartpole environment:\n",
        "\n",
        "from classic_control.cartpole import CartPoleEnv\n",
        "\n",
        "env = CartPoleEnv(sutton_barto_reward=True)\n",
        "# position thresholds:\n",
        "x_lim = 2.5\n",
        "theta_lim = 0.25 \n",
        "# velocity thresholds:\n",
        "x_dot_lim = 2.5\n",
        "theta_dot_lim = 2.5\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\": np.linspace(-x_lim, x_lim, 20),                         # position space         (0)\n",
        "    \"x_dot_space\": np.linspace(-x_dot_lim, x_dot_lim, 20),             # velocity space         (1)\n",
        "    \"theta_space\": np.linspace(-theta_lim, theta_lim, 20),             # angle space            (2)\n",
        "    \"theta_dot_space\": np.linspace(-theta_dot_lim, theta_dot_lim, 20), # angular velocity space (3)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=[0, 1],\n",
        "    gamma=0.99,\n",
        "    theta=1e-3\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0462a904",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test cartpole environment:\n",
        "\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi = pickle.load(f)\n",
        "\n",
        "test_enviroment(CartPoleEnv(sutton_barto_reward=True, render_mode=\"human\"), \n",
        "                pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d617686",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train mountain car environment:\n",
        "\n",
        "from classic_control.continuous_mountain_car import Continuous_MountainCarEnv\n",
        "\n",
        "env=Continuous_MountainCarEnv()\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\":     np.linspace(env.min_position, env.max_position, 250),      # position space         (0)\n",
        "    \"x_dot_space\": np.linspace(-abs(env.max_speed), abs(env.max_speed), 250), # velocity space         (1)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=[-1, 1],\n",
        "    gamma=0.99,\n",
        "    theta=1e-3,\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f556b5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test mountain car environment:\n",
        "\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi = pickle.load(f)\n",
        "\n",
        "test_enviroment(Continuous_MountainCarEnv(render_mode=\"human\"), \n",
        "                pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f5ab6d8",
      "metadata": {},
      "source": [
        "train acrobot environment:\n",
        "\n",
        "| Num | Observation                  | Min                 | Max               |\n",
        "|-----|------------------------------|---------------------|-------------------|\n",
        "| 0   | Cosine of `theta1`           | -1                  | 1                 |\n",
        "| 1   | Sine of `theta1`             | -1                  | 1                 |\n",
        "| 2   | Cosine of `theta2`           | -1                  | 1                 |\n",
        "| 3   | Sine of `theta2`             | -1                  | 1                 |\n",
        "| 4   | Angular velocity of `theta1` | ~ -12.567 (-4 * pi) | ~ 12.567 (4 * pi) |\n",
        "| 5   | Angular velocity of `theta2` | ~ -28.274 (-9 * pi) | ~ 28.274 (9 * pi) |\n",
        "\n",
        "\n",
        "| Num | Action                                | Unit         |\n",
        "|-----|---------------------------------------|--------------|\n",
        "| 0   | apply -1 torque to the actuated joint | torque (N m) |\n",
        "| 1   | apply 0 torque to the actuated joint  | torque (N m) |\n",
        "| 2   | apply 1 torque to the actuated joint  | torque (N m) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c016bbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from classic_control.acrobot import AcrobotEnv\n",
        "\n",
        "env = AcrobotEnv()\n",
        "\n",
        "bins_space = { \"cosine_theta1_space\": np.linspace(-1, 1,5), # cos(theta1) space (0)\n",
        "                \"sine_theta1_space\": np.linspace(-1, 1, 5),  # sin(theta1) space (1)\n",
        "                \"cosine_theta2_space\": np.linspace(-1, 1, 5), # cos(theta2) space (2)\n",
        "                \"sine_theta2_space\": np.linspace(-1, 1, 5),   # sin(theta2) space (3)\n",
        "                \"angular_velocity1_space\": np.linspace(-env.MAX_VEL_1, env.MAX_VEL_1, 5), # angular velocity 1 space (4)\n",
        "                \"angular_velocity2_space\": np.linspace(-env.MAX_VEL_2, env.MAX_VEL_2, 5)  # angular velocity 2 space (5)\n",
        "            }\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=[-1, 0, 1],\n",
        "    gamma=0.99,\n",
        "    theta=1e-3,\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7344eb53",
      "metadata": {},
      "source": [
        "| Num | Observation      | Min  | Max |\n",
        "|-----|------------------|------|-----|\n",
        "| 0   | x = cos(theta)   | -1.0 | 1.0 |\n",
        "| 1   | y = sin(theta)   | -1.0 | 1.0 |\n",
        "| 2   | Angular Velocity | -8.0 | 8.0 |\n",
        "\n",
        "| Num | Action | Min  | Max |\n",
        "|-----|--------|------|-----|\n",
        "| 0   | Torque | -2.0 | 2.0 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2644baae",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-05-12 20:34:20.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mPolicy Iteration was correctly initialized.\u001b[0m\n",
            "\u001b[32m2024-05-12 20:34:20.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mThe enviroment name is: PendulumEnv\u001b[0m\n",
            "\u001b[32m2024-05-12 20:34:20.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mThe action space is: [-2.0, -1.7894736842105263, -1.5789473684210527, -1.368421052631579, -1.1578947368421053, -0.9473684210526316, -0.736842105263158, -0.5263157894736843, -0.3157894736842106, -0.10526315789473695, 0.10526315789473673, 0.3157894736842106, 0.5263157894736841, 0.7368421052631575, 0.9473684210526314, 1.1578947368421053, 1.3684210526315788, 1.5789473684210522, 1.789473684210526, 2.0]\u001b[0m\n",
            "\u001b[32m2024-05-12 20:34:20.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mNumber of states: 10000\u001b[0m\n",
            "\u001b[32m2024-05-12 20:34:20.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1mGenerating transition and reward function table...\u001b[0m\n",
            "100%|██████████| 10000/10000 [03:20<00:00, 49.96it/s]\n",
            "\u001b[32m2024-05-12 20:37:41.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mTransition and reward function table generated.\u001b[0m\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[32m2024-05-12 20:37:41.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m267\u001b[0m - \u001b[1msolving step 0\u001b[0m\n",
            "\u001b[32m2024-05-12 20:37:41.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-05-12 20:37:42.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 97.727 | Avg Error: 32.9212 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-12 20:38:09.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 9.043015835008162e+21 | Avg Error: 4.3185806735923316e+21 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-12 20:38:37.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 8.96325763267181e+41 | Avg Error: 4.760008864733361e+41 | 0<0.001\u001b[0m\n",
            "  0%|          | 0/1000 [01:22<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 18\u001b[0m\n\u001b[0;32m      6\u001b[0m bins_space \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi, \u001b[38;5;241m100\u001b[39m), \u001b[38;5;66;03m# angle space (0)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheta_dot\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m100\u001b[39m) \u001b[38;5;66;03m# angular velocity space (1)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m              } \n\u001b[0;32m     10\u001b[0m pi \u001b[38;5;241m=\u001b[39m PolicyIteration(\n\u001b[0;32m     11\u001b[0m     env\u001b[38;5;241m=\u001b[39menv, \n\u001b[0;32m     12\u001b[0m     bins_space\u001b[38;5;241m=\u001b[39mbins_space,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     theta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m \u001b[43mpi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nicor\\OneDrive\\Documents\\DynamicProgramming\\PolicyIteration.py:268\u001b[0m, in \u001b[0;36mPolicyIteration.run\u001b[1;34m(self, nsteps)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(nsteps)):\n\u001b[0;32m    267\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolving step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_improvement():\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nicor\\OneDrive\\Documents\\DynamicProgramming\\PolicyIteration.py:207\u001b[0m, in \u001b[0;36mPolicyIteration.policy_evaluation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m     reward, _, simplex, bar_coor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_reward_table[(state, action)]\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;66;03m# Checkout 'Variable Resolution Discretization in Optimal Control, eq 5'\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbar_coor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m     new_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy[state][action] \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_state_value)\n\u001b[0;32m    209\u001b[0m new_value_function[state] \u001b[38;5;241m=\u001b[39m new_val\n",
            "File \u001b[1;32mc:\\Users\\nicor\\OneDrive\\Documents\\DynamicProgramming\\PolicyIteration.py:183\u001b[0m, in \u001b[0;36mPolicyIteration.get_value\u001b[1;34m(self, lambdas, simplex, value_function)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([value_function[\u001b[38;5;28mtuple\u001b[39m(e)] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(simplex)])\n\u001b[1;32m--> 183\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;167;01mKeyError\u001b[39;00m\n\u001b[0;32m    186\u001b[0m ):\n\u001b[0;32m    187\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStates in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimplex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the value function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train pendulum environment:\n",
        "from classic_control.pendulum import PendulumEnv\n",
        "\n",
        "env = PendulumEnv()\n",
        "\n",
        "bins_space = {\"theta\": np.linspace(0, 2*np.pi, 100), # angle space (0)\n",
        "              \"theta_dot\": np.linspace(-1.0, 1.0, 100) # angular velocity space (1)\n",
        "             } \n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=list(np.linspace(-2.0, 2.0, 20)),\n",
        "    gamma=0.99,\n",
        "    theta=1e-3,\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0745c670",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(env\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     pi \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtest_enviroment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPendulumEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[1], line 55\u001b[0m, in \u001b[0;36mtest_enviroment\u001b[1;34m(task, pi, num_episodes, episode_lengh)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, episode_lengh):\n\u001b[0;32m     54\u001b[0m     action \u001b[38;5;241m=\u001b[39m get_optimal_action(observation, pi)\n\u001b[1;32m---> 55\u001b[0m     observation, reward, terminated, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated:\n",
            "File \u001b[1;32mc:\\Users\\nicor\\OneDrive\\Documents\\DynamicProgramming\\classic_control\\pendulum.py:147\u001b[0m, in \u001b[0;36mPendulumEnv.step\u001b[1;34m(self, u)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([newth, newthdot])\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(), \u001b[38;5;241m-\u001b[39mcosts, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
            "File \u001b[1;32mc:\\Users\\nicor\\OneDrive\\Documents\\DynamicProgramming\\classic_control\\pendulum.py:265\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    264\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# mode == \"rgb_array\":\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# test pendulum environment:\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi = pickle.load(f)\n",
        "\n",
        "test_enviroment(PendulumEnv(render_mode=\"human\"),\n",
        "                pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1135fe0",
      "metadata": {},
      "outputs": [],
      "source": [
        "#requirements:\n",
        "#numpy\n",
        "#tqdm \n",
        "#gymnasium\n",
        "#scipy\n",
        "#loguru"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
