{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b149d005",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoRomeroCuruchet/DynamicProgramming/blob/main/testing_bary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beeb377e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "from itertools import product\n",
        "from scipy.spatial import KDTree\n",
        "from scipy.optimize import minimize\n",
        "from PolicyIteration import PolicyIteration\n",
        "from classic_control.cartpole import CartPoleEnv \n",
        "from classic_control.continuous_mountain_car import Continuous_MountainCarEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c04b7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "env=CartPoleEnv(sutton_barto_reward=True)\n",
        "# position thresholds:\n",
        "x_lim = 2.5#env.x_threshold  \n",
        "theta_lim = 0.25#env.theta_threshold_radians \n",
        "# velocity thresholds:\n",
        "x_dot_lim = 2.5\n",
        "theta_dot_lim = 2.5\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\": np.linspace(-x_lim, x_lim, 12),                         # position space         (0)\n",
        "    \"x_dot_space\": np.linspace(-x_dot_lim, x_dot_lim, 12),             # velocity space         (1)\n",
        "    \"theta_space\": np.linspace(-theta_lim, theta_lim, 12),             # angle space            (2)\n",
        "    \"theta_dot_space\": np.linspace(-theta_dot_lim, theta_dot_lim, 12), # angular velocity space (3)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "df921921",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [08:26<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m pi \u001b[38;5;241m=\u001b[39m PolicyIteration(\n\u001b[0;32m      2\u001b[0m     env\u001b[38;5;241m=\u001b[39menv, \n\u001b[0;32m      3\u001b[0m     bins_space\u001b[38;5;241m=\u001b[39mbins_space,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     theta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m \u001b[43mpi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nicor\\OneDrive\\Documents\\DynamicProgramming\\PolicyIteration.py:266\u001b[0m, in \u001b[0;36mPolicyIteration.run\u001b[1;34m(self, nsteps)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(nsteps)):\n\u001b[0;32m    265\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolving step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_improvement():\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nicor\\OneDrive\\Documents\\DynamicProgramming\\PolicyIteration.py:206\u001b[0m, in \u001b[0;36mPolicyIteration.policy_evaluation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m     reward, _, simplex, bar_coor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_reward_table[(state, action)]\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;66;03m# Checkout 'Variable Resolution Discretization in Optimal Control, eq 5'\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbar_coor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     new_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy[state][action] \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_state_value)\n\u001b[0;32m    208\u001b[0m new_value_function[state] \u001b[38;5;241m=\u001b[39m new_val\n",
            "File \u001b[1;32mc:\\Users\\nicor\\OneDrive\\Documents\\DynamicProgramming\\PolicyIteration.py:182\u001b[0m, in \u001b[0;36mPolicyIteration.get_value\u001b[1;34m(self, lambdas, simplex, value_function)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([value_function[\u001b[38;5;28mtuple\u001b[39m(e)] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(simplex)])\n\u001b[1;32m--> 182\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;167;01mKeyError\u001b[39;00m\n\u001b[0;32m    185\u001b[0m ):\n\u001b[0;32m    186\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStates in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimplex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the value function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=[0, 1],\n",
        "    gamma=0.99,\n",
        "    theta=1e-2\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ab6bee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ab6bee",
        "outputId": "cc2eb18d-5a89-40d3-fd6c-0cf0e33ae32e"
      },
      "outputs": [],
      "source": [
        "def get_optimal_action(state:np.array, optimal_policy:PolicyIteration):\n",
        "    \"\"\"\n",
        "    Aproximate the optimal action for a given state using the provided optimal policy\n",
        "    with barycentric interpolation.\n",
        "\n",
        "    Parameters:\n",
        "    state (np.array): The state for which to determine the optimal action.\n",
        "    optimal_policy (PolicyIteration): The optimal policy used to determine the action.\n",
        "\n",
        "    Returns:\n",
        "    action: The optimal action for the given state.\n",
        "    \"\"\"\n",
        "    _, neighbors  = optimal_policy.kd_tree.query([state], k=optimal_policy.num_simplex_points)\n",
        "    simplex       = optimal_policy.points[neighbors[0]]\n",
        "    lambdas       = optimal_policy.barycentric_coordinates(state, simplex)\n",
        "\n",
        "    actions = optimal_policy.action_space\n",
        "    probabilities = np.zeros(len(actions))\n",
        "\n",
        "    for i, l in enumerate(lambdas):\n",
        "        for j, action in enumerate(actions):\n",
        "            if optimal_policy.policy[tuple(simplex[i])][action] > 0:\n",
        "                probabilities[j] += l\n",
        "\n",
        "    argmax = lambda x: max(enumerate(x), key=lambda x: x[1])[0]\n",
        "    action = actions[argmax(probabilities)]\n",
        "\n",
        "    return action\n",
        "\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi = pickle.load(f)\n",
        "\n",
        "num_episodes = 10000\n",
        "task = CartPoleEnv(render_mode=\"human\") # Continuous_MountainCarEnv(render_mode=\"human\")  | CartPoleEnv(render_mode=\"human\")\n",
        "max_velocity = -1\n",
        "max_theta_dot = -1\n",
        "for episode in range(0, num_episodes):\n",
        "    observation, _ = task.reset()\n",
        "    total_reward = 0\n",
        "    for timestep in range(1, 1000):\n",
        "        action = get_optimal_action(observation, pi)\n",
        "        observation, reward, terminated, done, info = task.step(action)\n",
        "        total_reward += reward\n",
        "        max_velocity = max(max_velocity, abs(observation[1]))\n",
        "        max_theta_dot = max(max_theta_dot, abs(observation[3]))\n",
        "        if terminated:\n",
        "            print(f\"Episode {episode} finished after {timestep} timesteps\")\n",
        "            print(f\"Total reward: {total_reward}\")\n",
        "            print(f\"Max velocity: {max_velocity}\")\n",
        "            print(f\"Max theta_dot: {max_theta_dot}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1135fe0",
      "metadata": {},
      "outputs": [],
      "source": [
        "#requirements:\n",
        "#numpy\n",
        "#tqdm \n",
        "#gymnasium\n",
        "#scipy\n",
        "#loguru"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
