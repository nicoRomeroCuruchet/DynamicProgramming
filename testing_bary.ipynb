{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b149d005",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoRomeroCuruchet/DynamicProgramming/blob/main/testing_bary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "beeb377e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "from itertools import product\n",
        "from scipy.spatial import KDTree\n",
        "from scipy.optimize import minimize\n",
        "from PolicyIteration import PolicyIteration\n",
        "from classic_control.cartpole import CartPoleEnv \n",
        "from classic_control.continuous_mountain_car import Continuous_MountainCarEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df921921",
      "metadata": {},
      "outputs": [],
      "source": [
        "env=CartPoleEnv(sutton_barto_reward=True)\n",
        "# position thresholds:\n",
        "x_lim = env.x_threshold + 0.5\n",
        "theta_lim = env.theta_threshold_radians + 0.5\n",
        "# velocity thresholds:\n",
        "x_dot_lim = 2.5\n",
        "theta_dot_lim = 2.5\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\": np.linspace(-x_lim, x_lim, 12),                         # position space         (0)\n",
        "    \"x_dot_space\": np.linspace(-x_dot_lim, x_dot_lim, 12),             # velocity space         (1)\n",
        "    \"theta_space\": np.linspace(-theta_lim, theta_lim, 12),             # angle space            (2)\n",
        "    \"theta_dot_space\": np.linspace(-theta_dot_lim, theta_dot_lim, 12), # angular velocity space (3)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    gamma=0.95,\n",
        "    action_space=[0, 1]\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ab6bee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ab6bee",
        "outputId": "cc2eb18d-5a89-40d3-fd6c-0cf0e33ae32e"
      },
      "outputs": [],
      "source": [
        "def get_optimal_action(state, optimal_policy):\n",
        "    \"\"\"Returns the optimal action for a given state based on the optimal policy.\n",
        "\n",
        "    Parameters:\n",
        "    state (int): The current state.\n",
        "    optimal_policy (dict): The optimal policy containing the action-value pairs for each state.\n",
        "\n",
        "    Returns:\n",
        "    int: The optimal action for the given state.\"\"\"\n",
        "\n",
        "    _, neighbors  = optimal_policy.kd_tree.query([state], k=5)\n",
        "    simplex = optimal_policy.points[neighbors[0]]\n",
        "    lambdas = optimal_policy.barycentric_coordinates(state, simplex)\n",
        "\n",
        "    zero = 0\n",
        "    one = 0\n",
        "\n",
        "    for i,l in enumerate(lambdas):\n",
        "\n",
        "        if optimal_policy.policy[tuple(simplex[i])][0] > 0:\n",
        "            zero +=l\n",
        "        else:\n",
        "            one +=l\n",
        "\n",
        "    return 0 if zero > one else 1\n",
        "\n",
        "del pi\n",
        "\n",
        "with open(env.__class__.__name__ + \".pkl\" \"rb\") as f:\n",
        "    pi = pickle.load(f)\n",
        "\n",
        "num_episodes = 10000\n",
        "mountain_car = CartPoleEnv(render_mode=\"human\") # Continuous_MountainCarEnv(render_mode=\"human\")  | CartPoleEnv(render_mode=\"human\")\n",
        "for episode in range(0, num_episodes):\n",
        "    observation, _ = mountain_car.reset()\n",
        "    total_reward = 0\n",
        "    for timestep in range(1, 1000):\n",
        "        action = get_optimal_action(observation, pi)\n",
        "        observation, reward, done, terminated, info = mountain_car.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            print(f\"Episode {episode} finished after {timestep} timesteps\")\n",
        "            print(f\"Total reward: {total_reward}\")\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
