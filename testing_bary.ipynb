{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b149d005",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoRomeroCuruchet/DynamicProgramming/blob/main/testing_bary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "beeb377e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "from itertools import product\n",
        "from scipy.spatial import KDTree\n",
        "from scipy.optimize import minimize\n",
        "from PolicyIteration import PolicyIteration\n",
        "from classic_control.cartpole import CartPoleEnv \n",
        "from classic_control.continuous_mountain_car import Continuous_MountainCarEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a1c04b7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "env=CartPoleEnv(sutton_barto_reward=True)\n",
        "# position thresholds:\n",
        "x_lim = env.x_threshold  \n",
        "theta_lim = env.theta_threshold_radians \n",
        "# velocity thresholds:\n",
        "x_dot_lim = 2.25\n",
        "theta_dot_lim = 2.25\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\": np.linspace(-x_lim, x_lim, 12),                         # position space         (0)\n",
        "    \"x_dot_space\": np.linspace(-x_dot_lim, x_dot_lim, 20),             # velocity space         (1)\n",
        "    \"theta_space\": np.linspace(-theta_lim, theta_lim, 12),             # angle space            (2)\n",
        "    \"theta_dot_space\": np.linspace(-theta_dot_lim, theta_dot_lim, 20), # angular velocity space (3)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "df921921",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-05-09 16:44:51.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPolicy Iteration was correctly initialized.\u001b[0m\n",
            "\u001b[32m2024-05-09 16:44:51.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mThe enviroment name is: CartPoleEnv\u001b[0m\n",
            "\u001b[32m2024-05-09 16:44:51.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mThe action space is: [0, 1]\u001b[0m\n",
            "\u001b[32m2024-05-09 16:44:51.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mNumber of states: 57600\u001b[0m\n",
            "\u001b[32m2024-05-09 16:44:51.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m262\u001b[0m - \u001b[1mGenerating transition and reward function table...\u001b[0m\n",
            "100%|██████████| 57600/57600 [02:18<00:00, 414.79it/s]\n",
            "\u001b[32m2024-05-09 16:47:10.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m264\u001b[0m - \u001b[1mTransition and reward function table generated.\u001b[0m\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[32m2024-05-09 16:47:10.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m266\u001b[0m - \u001b[1msolving step 0\u001b[0m\n",
            "\u001b[32m2024-05-09 16:47:10.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-05-09 16:47:12.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 1.0 | Avg Error: 1.0 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:47:38.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.9082 | Avg Error: 0.3985 | 2<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:48:04.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.6157 | Avg Error: 0.2837 | 70<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:48:31.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.4255 | Avg Error: 0.2028 | 14<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:48:58.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.2966 | Avg Error: 0.1446 | 94<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:49:26.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.2037 | Avg Error: 0.1032 | 172<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:49:52.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.1433 | Avg Error: 0.074 | 128<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:50:19.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.1048 | Avg Error: 0.0533 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:50:46.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.0778 | Avg Error: 0.0384 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:51:13.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.0578 | Avg Error: 0.0277 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:51:39.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.043 | Avg Error: 0.02 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:52:05.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.0319 | Avg Error: 0.0144 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:52:30.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.0237 | Avg Error: 0.0104 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:52:56.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.0176 | Avg Error: 0.0075 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:53:22.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.0131 | Avg Error: 0.0055 | 0<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:53:49.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.0097 | Avg Error: 0.0039 | 672<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:54:17.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.0072 | Avg Error: 0.0029 | 2640<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:54:46.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.0054 | Avg Error: 0.0021 | 6396<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:55:14.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.004 | Avg Error: 0.0015 | 14868<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:55:40.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.003 | Avg Error: 0.0011 | 29120<0.001\u001b[0m\n",
            "\u001b[32m2024-05-09 16:56:05.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mMax Error: 0.0022 | Avg Error: 0.0008 | 40994<0.001\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=[0, 1],\n",
        "    gamma=0.99,\n",
        "    theta=1e-3\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ab6bee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ab6bee",
        "outputId": "cc2eb18d-5a89-40d3-fd6c-0cf0e33ae32e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_optimal_action\u001b[39m(state:\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray, optimal_policy:PolicyIteration):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Aproximate the optimal action for a given state using the provided optimal policy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    with barycentric interpolation.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    action: The optimal action for the given state.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     _, neighbors  \u001b[38;5;241m=\u001b[39m optimal_policy\u001b[38;5;241m.\u001b[39mkd_tree\u001b[38;5;241m.\u001b[39mquery([state], k\u001b[38;5;241m=\u001b[39moptimal_policy\u001b[38;5;241m.\u001b[39mnum_simplex_points)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "def get_optimal_action(state:np.array, optimal_policy:PolicyIteration):\n",
        "    \"\"\"\n",
        "    Aproximate the optimal action for a given state using the provided optimal policy\n",
        "    with barycentric interpolation.\n",
        "\n",
        "    Parameters:\n",
        "    state (np.array): The state for which to determine the optimal action.\n",
        "    optimal_policy (PolicyIteration): The optimal policy used to determine the action.\n",
        "\n",
        "    Returns:\n",
        "    action: The optimal action for the given state.\n",
        "    \"\"\"\n",
        "    _, neighbors  = optimal_policy.kd_tree.query([state], k=optimal_policy.num_simplex_points)\n",
        "    simplex       = optimal_policy.points[neighbors[0]]\n",
        "    lambdas       = optimal_policy.barycentric_coordinates(state, simplex)\n",
        "\n",
        "    actions = optimal_policy.action_space\n",
        "    probabilities = np.zeros(len(actions))\n",
        "\n",
        "    for i, l in enumerate(lambdas):\n",
        "        for j, action in enumerate(actions):\n",
        "            if optimal_policy.policy[tuple(simplex[i])][action] > 0:\n",
        "                probabilities[j] += l\n",
        "\n",
        "    argmax = lambda x: max(enumerate(x), key=lambda x: x[1])[0]\n",
        "    action = actions[argmax(probabilities)]\n",
        "    return action\n",
        "\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi = pickle.load(f)\n",
        "\n",
        "num_episodes = 10000\n",
        "task = CartPoleEnv(render_mode=\"human\") # Continuous_MountainCarEnv(render_mode=\"human\")  | CartPoleEnv(render_mode=\"human\")\n",
        "max_velocity = -1\n",
        "max_theta_dot = -1\n",
        "for episode in range(0, num_episodes):\n",
        "    observation, _ = task.reset()\n",
        "    total_reward = 0\n",
        "    for timestep in range(1, 1000):\n",
        "        action = get_optimal_action(observation, pi)\n",
        "        observation, reward, terminated, done, info = task.step(action)\n",
        "        total_reward += reward\n",
        "        max_velocity = max(max_velocity, abs(observation[1]))\n",
        "        max_theta_dot = max(max_theta_dot, abs(observation[3]))\n",
        "        if terminated:\n",
        "            print(f\"Episode {episode} finished after {timestep} timesteps\")\n",
        "            print(f\"Total reward: {total_reward}\")\n",
        "            print(f\"Max velocity: {max_velocity}\")\n",
        "            print(f\"Max theta_dot: {max_theta_dot}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1135fe0",
      "metadata": {},
      "outputs": [],
      "source": [
        "#requirements:\n",
        "#numpy\n",
        "#tqdm \n",
        "#gymnasium\n",
        "#scipy\n",
        "#loguru"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
