{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b149d005",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoRomeroCuruchet/DynamicProgramming/blob/main/testing_bary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "beeb377e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "from itertools import product\n",
        "from scipy.spatial import KDTree\n",
        "from scipy.optimize import minimize\n",
        "from PolicyIteration import PolicyIteration\n",
        "from classic_control.cartpole import CartPoleEnv \n",
        "from classic_control.continuous_mountain_car import Continuous_MountainCarEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a1c04b7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "env=CartPoleEnv(sutton_barto_reward=True)\n",
        "# position thresholds:\n",
        "x_lim = 2.5#env.x_threshold  \n",
        "theta_lim = 0.25#env.theta_threshold_radians \n",
        "# velocity thresholds:\n",
        "x_dot_lim = 2.5\n",
        "theta_dot_lim = 2.5\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\": np.linspace(-x_lim, x_lim, 12),                         # position space         (0)\n",
        "    \"x_dot_space\": np.linspace(-x_dot_lim, x_dot_lim, 12),             # velocity space         (1)\n",
        "    \"theta_space\": np.linspace(-theta_lim, theta_lim, 20),             # angle space            (2)\n",
        "    \"theta_dot_space\": np.linspace(-theta_dot_lim, theta_dot_lim, 12), # angular velocity space (3)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "df921921",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-05-10 00:34:51.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mPolicy Iteration was correctly initialized.\u001b[0m\n",
            "\u001b[32m2024-05-10 00:34:51.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mThe enviroment name is: CartPoleEnv\u001b[0m\n",
            "\u001b[32m2024-05-10 00:34:51.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mThe action space is: [0, 1]\u001b[0m\n",
            "\u001b[32m2024-05-10 00:34:51.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mNumber of states: 34560\u001b[0m\n",
            "\u001b[32m2024-05-10 00:34:51.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m261\u001b[0m - \u001b[1mGenerating transition and reward function table...\u001b[0m\n",
            "100%|██████████| 34560/34560 [01:17<00:00, 446.01it/s]\n",
            "\u001b[32m2024-05-10 00:36:08.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1mTransition and reward function table generated.\u001b[0m\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[32m2024-05-10 00:36:08.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1msolving step 0\u001b[0m\n",
            "\u001b[32m2024-05-10 00:36:08.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-05-10 00:36:09.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 1.0 | Avg Error: 0.3403 | 22800<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:36:25.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.8199 | Avg Error: 0.4944 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:36:40.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.6545 | Avg Error: 0.4023 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:36:55.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.5219 | Avg Error: 0.3169 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:37:09.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.4161 | Avg Error: 0.2487 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:37:24.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.3317 | Avg Error: 0.1952 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:37:39.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.2645 | Avg Error: 0.1531 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:37:54.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.2109 | Avg Error: 0.1202 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:38:09.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.1681 | Avg Error: 0.0943 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:38:23.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.134 | Avg Error: 0.074 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:38:38.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.1068 | Avg Error: 0.0581 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:38:52.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0852 | Avg Error: 0.0456 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:39:06.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0679 | Avg Error: 0.0358 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:39:21.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0541 | Avg Error: 0.0281 | 136<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:39:35.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0432 | Avg Error: 0.022 | 624<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:39:50.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0344 | Avg Error: 0.0173 | 1920<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:40:05.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0274 | Avg Error: 0.0136 | 8144<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:40:20.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0219 | Avg Error: 0.0107 | 16016<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:40:35.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0174 | Avg Error: 0.0084 | 25096<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:40:50.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0139 | Avg Error: 0.0066 | 32288<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:41:05.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0111 | Avg Error: 0.0052 | 34288<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:41:20.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0088 | Avg Error: 0.0041 | 34560<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:41:20.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-05-10 00:41:20.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 34560/34560 [00:01<00:00, 27660.60it/s]\n",
            "\u001b[32m2024-05-10 00:41:21.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mThe number of updated different actions:                        34560\u001b[0m\n",
            "\u001b[32m2024-05-10 00:41:21.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  1%|          | 1/100 [05:12<8:35:52, 312.65s/it]\u001b[32m2024-05-10 00:41:21.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1msolving step 1\u001b[0m\n",
            "\u001b[32m2024-05-10 00:41:21.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-05-10 00:41:22.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 5.2977 | Avg Error: 1.5629 | 296<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:41:36.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 1.5696 | Avg Error: 0.5878 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:41:52.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.6514 | Avg Error: 0.3168 | 0<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:42:06.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.3877 | Avg Error: 0.1768 | 224<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:42:21.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.2667 | Avg Error: 0.1021 | 1840<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:42:37.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.1836 | Avg Error: 0.0605 | 3776<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:42:52.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.1263 | Avg Error: 0.0366 | 6888<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:43:08.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0869 | Avg Error: 0.0224 | 10208<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:43:23.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0598 | Avg Error: 0.0139 | 14680<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:43:38.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0412 | Avg Error: 0.0087 | 23288<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:43:54.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0283 | Avg Error: 0.0055 | 28480<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:44:09.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0195 | Avg Error: 0.0035 | 31880<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:44:24.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0134 | Avg Error: 0.0022 | 33952<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:44:39.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0092 | Avg Error: 0.0014 | 34560<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:44:39.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-05-10 00:44:39.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 34560/34560 [00:00<00:00, 36760.49it/s]\n",
            "\u001b[32m2024-05-10 00:44:40.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mThe number of updated different actions:                        5416\u001b[0m\n",
            "\u001b[32m2024-05-10 00:44:40.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  2%|▏         | 2/100 [08:31<6:41:10, 245.62s/it]\u001b[32m2024-05-10 00:44:40.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1msolving step 2\u001b[0m\n",
            "\u001b[32m2024-05-10 00:44:40.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-05-10 00:44:40.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.9235 | Avg Error: 0.0273 | 29224<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:44:55.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0306 | Avg Error: 0.0092 | 23424<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:10.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0116 | Avg Error: 0.0049 | 33960<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:25.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0083 | Avg Error: 0.0028 | 34560<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:25.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:25.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 34560/34560 [00:01<00:00, 27463.93it/s]\n",
            "\u001b[32m2024-05-10 00:45:26.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mThe number of updated different actions:                        1536\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:26.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  3%|▎         | 3/100 [09:17<4:10:12, 154.77s/it]\u001b[32m2024-05-10 00:45:26.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1msolving step 3\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:26.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:27.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.2718 | Avg Error: 0.0047 | 33280<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:42.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.006 | Avg Error: 0.0025 | 34560<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:42.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:42.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 34560/34560 [00:01<00:00, 26682.51it/s]\n",
            "\u001b[32m2024-05-10 00:45:44.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1mThe number of updated different actions:                        152\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:44.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  4%|▍         | 4/100 [09:35<2:40:50, 100.53s/it]\u001b[32m2024-05-10 00:45:44.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1msolving step 4\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:44.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mStarting policy evaluation\u001b[0m\n",
            "\u001b[32m2024-05-10 00:45:45.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0253 | Avg Error: 0.0025 | 34528<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:46:00.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mMax Error: 0.0043 | Avg Error: 0.0014 | 34560<0.01\u001b[0m\n",
            "\u001b[32m2024-05-10 00:46:00.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_evaluation\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mPolicy evaluation finished.\u001b[0m\n",
            "\u001b[32m2024-05-10 00:46:00.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mStarting policy improvement\u001b[0m\n",
            "100%|██████████| 34560/34560 [00:00<00:00, 37491.78it/s]\n",
            "\u001b[32m2024-05-10 00:46:01.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36mpolicy_improvement\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1mPolicy improvement finished.\u001b[0m\n",
            "  4%|▍         | 4/100 [09:52<3:57:01, 148.14s/it]\n",
            "\u001b[32m2024-05-10 00:46:02.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mPolicyIteration\u001b[0m:\u001b[36msave\u001b[0m:\u001b[36m279\u001b[0m - \u001b[1mPolicy and value function saved.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=[0, 1],\n",
        "    gamma=0.99,\n",
        "    theta=1e-2\n",
        ")\n",
        "\n",
        "pi.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b5ab6bee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ab6bee",
        "outputId": "cc2eb18d-5a89-40d3-fd6c-0cf0e33ae32e"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m     41\u001b[0m     action \u001b[38;5;241m=\u001b[39m get_optimal_action(observation, pi)\n\u001b[1;32m---> 42\u001b[0m     observation, reward, terminated, done, info \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     44\u001b[0m     max_velocity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_velocity, \u001b[38;5;28mabs\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m]))\n",
            "File \u001b[1;32mc:\\Users\\nicor\\OneDrive\\Documents\\DynamicProgramming\\classic_control\\cartpole.py:231\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    228\u001b[0m         reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
            "File \u001b[1;32mc:\\Users\\nicor\\OneDrive\\Documents\\DynamicProgramming\\classic_control\\cartpole.py:344\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    343\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def get_optimal_action(state:np.array, optimal_policy:PolicyIteration):\n",
        "    \"\"\"\n",
        "    Aproximate the optimal action for a given state using the provided optimal policy\n",
        "    with barycentric interpolation.\n",
        "\n",
        "    Parameters:\n",
        "    state (np.array): The state for which to determine the optimal action.\n",
        "    optimal_policy (PolicyIteration): The optimal policy used to determine the action.\n",
        "\n",
        "    Returns:\n",
        "    action: The optimal action for the given state.\n",
        "    \"\"\"\n",
        "    _, neighbors  = optimal_policy.kd_tree.query([state], k=optimal_policy.num_simplex_points)\n",
        "    simplex       = optimal_policy.points[neighbors[0]]\n",
        "    lambdas       = optimal_policy.barycentric_coordinates(state, simplex)\n",
        "\n",
        "    actions = optimal_policy.action_space\n",
        "    probabilities = np.zeros(len(actions))\n",
        "\n",
        "    for i, l in enumerate(lambdas):\n",
        "        for j, action in enumerate(actions):\n",
        "            if optimal_policy.policy[tuple(simplex[i])][action] > 0:\n",
        "                probabilities[j] += l\n",
        "\n",
        "    argmax = lambda x: max(enumerate(x), key=lambda x: x[1])[0]\n",
        "    action = actions[argmax(probabilities)]\n",
        "\n",
        "    return action\n",
        "\n",
        "with open(env.__class__.__name__ + \".pkl\", \"rb\") as f:\n",
        "    pi = pickle.load(f)\n",
        "\n",
        "num_episodes = 10000\n",
        "task = CartPoleEnv(render_mode=\"human\") # Continuous_MountainCarEnv(render_mode=\"human\")  | CartPoleEnv(render_mode=\"human\")\n",
        "max_velocity = -1\n",
        "max_theta_dot = -1\n",
        "for episode in range(0, num_episodes):\n",
        "    observation, _ = task.reset(options={\"low\":-0.05, \"high\":0.05})\n",
        "    total_reward = 0\n",
        "    for timestep in range(1, 1000):\n",
        "        action = get_optimal_action(observation, pi)\n",
        "        observation, reward, terminated, done, info = task.step(action)\n",
        "        total_reward += reward\n",
        "        max_velocity = max(max_velocity, abs(observation[1]))\n",
        "        max_theta_dot = max(max_theta_dot, abs(observation[3]))\n",
        "        if terminated:\n",
        "            print(f\"Episode {episode} finished after {timestep} timesteps\")\n",
        "            print(f\"Total reward: {total_reward}\")\n",
        "            print(f\"Max velocity: {max_velocity}\")\n",
        "            print(f\"Max theta_dot: {max_theta_dot}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1135fe0",
      "metadata": {},
      "outputs": [],
      "source": [
        "#requirements:\n",
        "#numpy\n",
        "#tqdm \n",
        "#gymnasium\n",
        "#scipy\n",
        "#loguru"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
