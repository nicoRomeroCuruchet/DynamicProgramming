{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed83e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Union\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import logger, spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "\n",
    "class CartPoleEnv(gym.Env[np.ndarray, Union[int, np.ndarray]]):\n",
    "    \"\"\"\n",
    "    ## Description\n",
    "\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "\n",
    "    ## Action Space\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "\n",
    "    - 0: Push cart to the left\n",
    "    - 1: Push cart to the right\n",
    "\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "    ## Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\n",
    "    ## Rewards\n",
    "    Since the goal is to keep the pole upright for as long as possible, by default, a reward of `+1` is given for every step taken, including the termination step. The default reward threshold is 500 for v1 and 200 for v0 due to the time limit on the environment.\n",
    "\n",
    "    If `sutton_barto_reward=True`, then a reward of `0` is awarded for every non-terminating step and `-1` for the terminating step. As a result, the reward threshold is 0 for v0 and v1.\n",
    "\n",
    "    ## Starting State\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "    ## Episode End\n",
    "    The episode ends if any one of the following occurs:\n",
    "\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "    ## Arguments\n",
    "\n",
    "    Cartpole only has `render_mode` as a keyword for `gymnasium.make`.\n",
    "    On reset, the `options` parameter allows the user to change the bounds used to determine the new random state.\n",
    "\n",
    "    ```python\n",
    "    >>> import gymnasium as gym\n",
    "    >>> env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    >>> env\n",
    "    <TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>\n",
    "    >>> env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})  # default low=-0.05, high=0.05\n",
    "    (array([ 0.03647037, -0.0892358 , -0.05592803, -0.06312564], dtype=float32), {})\n",
    "\n",
    "    ```\n",
    "\n",
    "    | Parameter               | Type       | Default                 | Description                                                                                   |\n",
    "    |-------------------------|------------|-------------------------|-----------------------------------------------------------------------------------------------|\n",
    "    | `sutton_barto_reward`   | **bool**   | `False`                 | If `True` the reward function matches the original sutton barto implementation                |\n",
    "\n",
    "    ## Vectorized environment\n",
    "\n",
    "    To increase steps per seconds, users can use a custom vector environment or with an environment vectorizor.\n",
    "\n",
    "    ```python\n",
    "    >>> import gymnasium as gym\n",
    "    >>> envs = gym.make_vec(\"CartPole-v1\", num_envs=3, vectorization_mode=\"vector_entry_point\")\n",
    "    >>> envs\n",
    "    CartPoleVectorEnv(CartPole-v1, num_envs=3)\n",
    "    >>> envs = gym.make_vec(\"CartPole-v1\", num_envs=3, vectorization_mode=\"sync\")\n",
    "    >>> envs\n",
    "    SyncVectorEnv(CartPole-v1, num_envs=3)\n",
    "\n",
    "    ```\n",
    "\n",
    "    ## Version History\n",
    "    * v1: `max_time_steps` raised to 500.\n",
    "        - In Gymnasium `1.0.0a2` the `sutton_barto_reward` argument was added (related [GitHub issue](https://github.com/Farama-Foundation/Gymnasium/issues/790))\n",
    "    * v0: Initial versions release.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, sutton_barto_reward: bool = False, render_mode: Optional[str] = None\n",
    "    ):\n",
    "        self._sutton_barto_reward = sutton_barto_reward\n",
    "\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(\n",
    "            action\n",
    "        ), f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            if self._sutton_barto_reward:\n",
    "                reward = 0.0\n",
    "            elif not self._sutton_barto_reward:\n",
    "                reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            if self._sutton_barto_reward:\n",
    "                reward = -1.0\n",
    "            elif not self._sutton_barto_reward:\n",
    "                reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            if self._sutton_barto_reward:\n",
    "                reward = -1.0\n",
    "            elif not self._sutton_barto_reward:\n",
    "                reward = 0.0\n",
    "\n",
    "            reward = -1.0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        low, high = utils.maybe_parse_reset_bounds(\n",
    "            options, -0.005, 0.005  # default low\n",
    "        )  # default high\n",
    "        #low = (-x_lim, -x_dot_lim, -theta_lim, -theta_dot_lim)\n",
    "        #high = (x_lim, x_dot_lim, theta_lim, theta_dot_lim)\n",
    "        self.state = self.np_random.uniform(low=0, high=0, size=(4,))\n",
    "        #elf.state=np.array([0.0, 0.0, 0.01, 0.0])\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            assert self.spec is not None\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gymnasium[classic-control]`\"\n",
    "            ) from e\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode == \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = self.screen_width / world_width\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 100  # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54115dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating transition and reward function table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000/160000 [07:36<00:00, 350.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Policy Iteration algorithm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solving step 0\n",
      "delta: 1.0\n",
      "delta: 1.0243957523147484\n",
      "delta: 1.0312483499915857\n",
      "delta: 1.037626088463456\n",
      "delta: 1.039609361421367\n",
      "delta: 1.041090393421829\n",
      "delta: 1.0403455756003304\n",
      "delta: 1.0390943228718292\n",
      "delta: 1.0364080609313415\n",
      "delta: 1.0332598377578606\n",
      "delta: 1.0290941730639904\n",
      "delta: 1.024532599239567\n",
      "delta: 1.0192120083867628\n",
      "delta: 1.013555733548733\n",
      "delta: 1.007310373197539\n",
      "delta: 1.0007790269298376\n",
      "delta: 0.9937778189916635\n",
      "delta: 0.9865338857429506\n",
      "delta: 0.9789103203978833\n",
      "delta: 0.9710840291767546\n",
      "delta: 0.9629506702770065\n",
      "delta: 0.9546521880092165\n",
      "delta: 0.946107108168917\n",
      "delta: 0.9374318509384238\n",
      "delta: 0.92856124094385\n",
      "delta: 0.9195923211709776\n",
      "delta: 0.9104717822558435\n",
      "delta: 0.9012814977048933\n",
      "delta: 0.8919770242463798\n",
      "delta: 0.8826280669479907\n",
      "delta: 0.8731969921164016\n",
      "delta: 0.8637435492427947\n",
      "delta: 0.8542354742936205\n",
      "delta: 0.8447242398034547\n",
      "delta: 0.8351819163835756\n",
      "delta: 0.8256530056468776\n",
      "delta: 0.8161131474711922\n",
      "delta: 0.8066009122256759\n",
      "delta: 0.797094926903199\n",
      "delta: 0.7876286752221091\n",
      "delta: 0.778183316678934\n",
      "delta: 0.7687879464675547\n",
      "delta: 0.7594258931091247\n",
      "delta: 0.7501224486650102\n",
      "delta: 0.7408628141030036\n",
      "delta: 0.7316689748750846\n",
      "delta: 0.7225277582642491\n",
      "delta: 0.713458268037428\n",
      "delta: 0.7044487506867938\n",
      "delta: 0.6955157944315289\n",
      "delta: 0.6866488887767304\n",
      "delta: 0.6778624234967339\n",
      "delta: 0.6691469799126679\n",
      "delta: 0.660515025034897\n",
      "delta: 0.6519581013884022\n",
      "delta: 0.6434869935603942\n",
      "delta: 0.6350940918715864\n",
      "delta: 0.6267887084447779\n",
      "delta: 0.6185639825415805\n",
      "delta: 0.6104279375092929\n",
      "delta: 0.6023743751242492\n",
      "delta: 0.5944101908350135\n",
      "delta: 0.5865297732004677\n",
      "delta: 0.5787390307743081\n",
      "delta: 0.5710328724196216\n",
      "delta: 0.5634163434465123\n",
      "delta: 0.5558848145864914\n",
      "delta: 0.548442576378946\n",
      "delta: 0.5410854100013225\n",
      "delta: 0.5338169464017426\n",
      "delta: 0.5266333319119241\n",
      "delta: 0.5195376214149618\n",
      "delta: 0.5125262864746958\n",
      "delta: 0.505601879213259\n",
      "delta: 0.4987611612139986\n",
      "delta: 0.4920062461705257\n",
      "delta: 0.4853341546084664\n",
      "delta: 0.47874661824869946\n",
      "delta: 0.4722408891160583\n",
      "delta: 0.4658183664972455\n",
      "delta: 0.4594765096691944\n",
      "delta: 0.4532164289471652\n",
      "delta: 0.44703576942521295\n",
      "delta: 0.44093539057251974\n",
      "delta: 0.43491310434063735\n",
      "delta: 0.42896955278899895\n",
      "delta: 0.4233021733249416\n",
      "delta: 0.4177643088992369\n",
      "delta: 0.4122876934170989\n",
      "delta: 0.40687955548604293\n",
      "delta: 0.40153211845073145\n",
      "delta: 0.3962518580811434\n",
      "delta: 0.39103166774323483\n",
      "delta: 0.38587734285607667\n",
      "delta: 0.3807823856786001\n",
      "delta: 0.3757519767524684\n",
      "delta: 0.37078017191541335\n",
      "delta: 0.3658715959955998\n",
      "delta: 0.36102080832758077\n",
      "delta: 0.35623193150870236\n",
      "delta: 0.35149998300137497\n",
      "delta: 0.34682863153648213\n",
      "delta: 0.34221331158427404\n",
      "delta: 0.3376572817586947\n",
      "delta: 0.33315635625120876\n",
      "delta: 0.32871342314628293\n",
      "delta: 0.3243246425305415\n",
      "delta: 0.31999256779002394\n",
      "delta: 0.31571367420696106\n",
      "delta: 0.3114902129080406\n",
      "delta: 0.3073189464981283\n",
      "delta: 0.30320185321943427\n",
      "delta: 0.29913595768255163\n",
      "delta: 0.2951229918512013\n",
      "delta: 0.29116021933819525\n",
      "delta: 0.2872491499309717\n",
      "delta: 0.2833872653355485\n",
      "delta: 0.27957587500092984\n",
      "delta: 0.27581265971465996\n",
      "delta: 0.2720987483760382\n",
      "delta: 0.268432003562026\n",
      "delta: 0.26481339155633066\n",
      "delta: 0.26124094099215256\n",
      "delta: 0.25771547179255094\n",
      "delta: 0.25423516432765325\n",
      "delta: 0.2508007068937985\n",
      "delta: 0.24741041856196944\n",
      "delta: 0.2440648693574019\n",
      "delta: 0.24076250518044162\n",
      "delta: 0.23750378989205956\n",
      "delta: 0.23428728540798716\n",
      "delta: 0.23111336039950459\n",
      "delta: 0.22798068294430607\n",
      "delta: 0.22488953647150822\n",
      "delta: 0.2218386862407442\n",
      "delta: 0.21882833945501545\n",
      "delta: 0.21585735036913434\n",
      "delta: 0.21292585813108644\n",
      "delta: 0.21003279852568824\n",
      "delta: 0.20717825004997792\n",
      "delta: 0.20436122320992922\n",
      "delta: 0.20158174255973904\n",
      "delta: 0.19883888711456166\n",
      "delta: 0.19613263356151833\n",
      "delta: 0.19346212375711502\n",
      "delta: 0.19082729202301607\n",
      "delta: 0.1882273378832764\n",
      "delta: 0.18566215827497956\n",
      "delta: 0.18313100566673768\n",
      "delta: 0.18063374411782718\n",
      "delta: 0.17816967472882084\n",
      "delta: 0.17573863275782742\n",
      "delta: 0.1733399639985862\n",
      "delta: 0.17097347859393608\n",
      "delta: 0.16863856343245232\n",
      "delta: 0.16633500687213143\n",
      "delta: 0.164062233608945\n",
      "delta: 0.16182001322368933\n",
      "delta: 0.15960780521503182\n",
      "delta: 0.15742536310078492\n",
      "delta: 0.1552721784360358\n",
      "delta: 0.15314799112306332\n",
      "delta: 0.1510523222623874\n",
      "delta: 0.1489849003459085\n",
      "delta: 0.14694527372336097\n",
      "delta: 0.14493316146104007\n",
      "delta: 0.14294813705777187\n",
      "delta: 0.14098991193876032\n",
      "delta: 0.13905808283054455\n",
      "delta: 0.13715235511985213\n",
      "delta: 0.1352723470027115\n",
      "delta: 0.13341775926467392\n",
      "delta: 0.1315882299621336\n",
      "delta: 0.12978345656637202\n",
      "delta: 0.1280030955211373\n",
      "delta: 0.12624684213392356\n",
      "delta: 0.12451436988691\n",
      "delta: 0.12280537295040972\n",
      "delta: 0.12111954060959818\n",
      "delta: 0.11945656681159278\n",
      "delta: 0.11781615551269908\n",
      "delta: 0.11619800124881863\n",
      "delta: 0.11460182161007992\n",
      "delta: 0.11302731244035158\n",
      "delta: 0.11147420401283625\n",
      "delta: 0.10994219411443851\n",
      "delta: 0.10843102482974132\n",
      "delta: 0.10694039644752706\n",
      "delta: 0.1054700620640574\n",
      "delta: 0.10401972495978384\n",
      "delta: 0.10258914850911083\n",
      "delta: 0.10117803941119519\n",
      "delta: 0.09978617064528805\n",
      "delta: 0.09841325269992751\n",
      "delta: 0.09705906754058446\n",
      "delta: 0.09572332976469511\n",
      "delta: 0.0944058297561412\n",
      "delta: 0.09310628649396335\n",
      "delta: 0.09182449825860317\n",
      "delta: 0.09056018864208681\n",
      "delta: 0.08931316334135886\n",
      "delta: 0.0880831507547839\n",
      "delta: 0.08686996355464771\n",
      "delta: 0.08567333510501385\n",
      "delta: 0.08449308464713567\n",
      "delta: 0.08332895063973922\n",
      "delta: 0.08218075851887363\n",
      "delta: 0.08104825193916554\n",
      "delta: 0.07993126218687507\n",
      "delta: 0.07882953818908334\n",
      "delta: 0.07774291676436462\n",
      "delta: 0.07667115216662523\n",
      "delta: 0.07561408645368317\n",
      "delta: 0.07457147924078811\n",
      "delta: 0.07354317755417128\n",
      "delta: 0.07252894638733665\n",
      "delta: 0.07152863748473237\n",
      "delta: 0.07054202121936726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [14:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 202\u001b[0m\n\u001b[1;32m    200\u001b[0m STEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# start the policy iteration algorithm\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m \u001b[43mpi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnsteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 178\u001b[0m, in \u001b[0;36mPolicyIteration.run\u001b[0;34m(self, nsteps)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(nsteps)):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolving step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition_and_reward_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimprove_policy(transition_and_reward_function):\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 124\u001b[0m, in \u001b[0;36mPolicyIteration.evaluate_policy\u001b[0;34m(self, transition_and_reward_function)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space:\n\u001b[1;32m    123\u001b[0m     reward, next_state, simplex, bar_coor \u001b[38;5;241m=\u001b[39m transition_and_reward_function[(state, action)]\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m--> 124\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbar_coor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplex\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     new_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy[state][action] \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_state_value)\n\u001b[1;32m    126\u001b[0m new_value_function[state] \u001b[38;5;241m=\u001b[39m new_val\n",
      "Cell \u001b[0;32mIn[8], line 98\u001b[0m, in \u001b[0;36mPolicyIteration.get_value\u001b[0;34m(self, lambdas, simplex, value_function)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     values           \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([value_function[\u001b[38;5;28mtuple\u001b[39m(e)] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(simplex)])    \n\u001b[0;32m---> 98\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;167;01mKeyError\u001b[39;00m\n\u001b[1;32m    101\u001b[0m ):  \u001b[38;5;66;03m# if next_state is not in value_function, assume it's a 'dead' state.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m500\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from itertools import product\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "class PolicyIteration(object):\n",
    "    \"\"\"Policy Iteration Algorithm for gymnasium environment\"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, gamma: float = 0.99, bins_space: dict = None):\n",
    "        \"\"\"Initializes the Policy Iteration.\n",
    "\n",
    "        Parameters:\n",
    "        - env (gym.Env): The environment in which the agent will interact.\n",
    "        - gamma (float): The discount factor for future rewards. Default is 0.99.\n",
    "        - bins_space (dict): A dictionary specifying the number of bins for each state variable. Default is None.\n",
    "\n",
    "        Returns: None\"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma  # discaunt factor\n",
    "\n",
    "        self.action_space = range(env.action_space.n)\n",
    "        self.bins_space = bins_space\n",
    "        \n",
    "        self.states_space = list(\n",
    "            set(product(*bins_space.values()))\n",
    "        )  # avoid repited states\n",
    "        \n",
    "        self.points = np.array([np.array(e) for e in self.states_space])\n",
    "        self.kd_tree = KDTree(self.points)\n",
    "        \n",
    "        self.policy = {state: {0: 0.5, 1: 0.5} for state in self.states_space}\n",
    "        self.value_function = {state: 0 for state in self.states_space}  # initialize value function\n",
    "\n",
    "\n",
    "    def barycentric_coordinates(self, point, simplex):\n",
    "        # Formulate the system of equations\n",
    "        A = np.vstack([np.array(simplex).T, np.ones(len(simplex))])\n",
    "        b = np.hstack([point, [1]])\n",
    "        objective_function = lambda x: np.linalg.norm(A.dot(x) - b)\n",
    "\n",
    "        # Define the constraint that the solution must be greater than zero\n",
    "        constraints = ({'type': 'ineq', 'fun': lambda x: x})\n",
    "\n",
    "        # Initial guess for the solution\n",
    "        x0 = np.array([0.33, 0.33, 0.33, 0.33, 0.33])\n",
    "\n",
    "        # Solve the optimization problem\n",
    "        result = minimize(objective_function, x0, constraints=constraints, tol=1e-3)\n",
    "\n",
    "        # The approximate solution\n",
    "        x_approx = result.x\n",
    "        return x_approx\n",
    "\n",
    "    def get_transition_reward_function(self) -> dict:\n",
    "        \"\"\"Generate a transition reward function table.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary representing the transition reward function table.\n",
    "                The keys are tuples of (state, action), and the values are dictionaries\n",
    "                with 'reward' and 'next_state' as keys.\"\"\"\n",
    "\n",
    "        table = {}\n",
    "        for state in tqdm(self.states_space):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                self.env.reset() # TODO: is this necessary? might be slow\n",
    "                self.env.state = np.array(state, dtype=np.float64)  # set the state\n",
    "                obs, _, terminated, done, info = self.env.step(action)\n",
    "                \n",
    "                _, neighbors  = self.kd_tree.query([obs], k=5)\n",
    "                simplex = self.points[neighbors[0]]\n",
    "                lambdas = self.barycentric_coordinates(state, simplex)\n",
    "                \n",
    "                reward = (\n",
    "                    0 if (-0.2 < obs[2] < 0.2) and (-2.4 < obs[0] < 2.4) else -1\n",
    "                )  # TODO remove this hardcoded reward\n",
    "                table[(state, action)] = {\"reward\": reward, \n",
    "                                          \"next_state\": obs,\n",
    "                                          \"simplex\": simplex,\n",
    "                                          \"barycentric_coordinates\":lambdas}\n",
    "                \n",
    "\n",
    "        return table\n",
    "\n",
    "    def get_value(self, lambdas, simplex, value_function):\n",
    "        \"\"\"Retrieves the value of a given state from the value function.\n",
    "\n",
    "        Parameters:\n",
    "            state (any): The state for which the value needs to be retrieved.\n",
    "            value_function (dict): A dictionary representing the value function.\n",
    "\n",
    "        Returns:\n",
    "            float: The value of the given state from the value function.\"\"\"\n",
    "\n",
    "        try:\n",
    "            values           = np.array([value_function[tuple(e)] for e in list(simplex)])    \n",
    "            next_state_value = np.dot(lambdas, values)\n",
    "        except (\n",
    "            KeyError\n",
    "        ):  # if next_state is not in value_function, assume it's a 'dead' state.\n",
    "            next_state_value = -500\n",
    "            \n",
    "        return next_state_value\n",
    "\n",
    "    def evaluate_policy(self, transition_and_reward_function: dict) -> dict:\n",
    "        \"\"\"Evaluates the given policy using the provided transition and reward function.\n",
    "\n",
    "        Args:\n",
    "            transition_and_reward_function (dict): A dictionary representing the transition and reward function.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary representing the new value function after evaluating the policy.\n",
    "        \"\"\"\n",
    "        theta = 1e-2 # convergence threshold\n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_value_function = {}\n",
    "            for state in self.states_space:\n",
    "                new_val = 0\n",
    "                for action in self.action_space:\n",
    "                    reward, next_state, simplex, bar_coor = transition_and_reward_function[(state, action)].values()\n",
    "                    next_state_value = self.get_value(bar_coor, simplex,self.value_function)\n",
    "                    new_val += self.policy[state][action] * (reward + self.gamma * next_state_value)\n",
    "                new_value_function[state] = new_val\n",
    "\n",
    "            delta = max(delta, max(abs(new_value_function[state] - self.value_function[state]) for state in self.states_space))\n",
    "            print(f\"delta: {delta}\")\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "            self.value_function = new_value_function\n",
    "        return new_value_function\n",
    "\n",
    "    def improve_policy(self, transition_and_reward_function: dict) -> dict:\n",
    "        \"\"\"Improves the current policy based on the given transition and reward function.\n",
    "\n",
    "        Args:\n",
    "            transition_and_reward_function (dict): A dictionary representing the transition and reward function.\n",
    "                The keys are tuples of (state, action) and the values are dictionaries with 'reward' and 'next_state' keys.\n",
    "\n",
    "        Returns:\n",
    "            dict: The new policy after improvement.\"\"\"\n",
    "        \n",
    "        policy_stable = True\n",
    "        new_policy = {}\n",
    "\n",
    "        for state in self.states_space:\n",
    "            action_values = {}\n",
    "            for action in self.action_space:\n",
    "                reward, next_state, simplex, bar_coor = transition_and_reward_function[(state, action)].values()\n",
    "                action_values[action] = reward + self.gamma * self.get_value(bar_coor, simplex,self.value_function)\n",
    "                \n",
    "            greedy_action, _ = max(action_values.items(), key=lambda pair: pair[1])\n",
    "            new_policy[state] = {\n",
    "                action: 1 if action is greedy_action else 0 for action in self.action_space\n",
    "            }\n",
    "        if self.policy != new_policy:\n",
    "            print(f\"number of different actions: {sum([self.policy[state][0] != new_policy[state][0] for state in self.states_space])}\")\n",
    "            policy_stable = False\n",
    "\n",
    "        self.policy = new_policy\n",
    "        return policy_stable\n",
    "\n",
    "    def run(self, nsteps):\n",
    "        \"\"\"Runs the policy iteration algorithm for a specified number of steps.\n",
    "\n",
    "        Parameters:\n",
    "        - nsteps (int): The number of steps to run the algorithm for. Default is 10 steps.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Generating transition and reward function table...\")\n",
    "        transition_and_reward_function = self.get_transition_reward_function()\n",
    "        print(\"Running Policy Iteration algorithm...\")\n",
    "        for n in tqdm(range(nsteps)):\n",
    "            print(f\"solving step {n}\")\n",
    "            self.evaluate_policy(transition_and_reward_function)\n",
    "            if self.improve_policy(transition_and_reward_function):\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_lim = 2.5\n",
    "    x_dot_lim = 2.5\n",
    "    theta_lim = 0.25\n",
    "    theta_dot_lim = 2.5\n",
    "\n",
    "\n",
    "    bins_space = {\n",
    "        \"x_space\": np.linspace(-x_lim, x_lim, 20),\n",
    "        \"x_dot_space\": np.linspace(-x_dot_lim, x_dot_lim, 20),\n",
    "        \"theta_space\": np.linspace(-theta_lim, theta_lim, 20),\n",
    "        \"theta_dot_space\": np.linspace(-theta_dot_lim, theta_dot_lim, 20),\n",
    "    }\n",
    "\n",
    "    pi = PolicyIteration(\n",
    "        env=CartPoleEnv(sutton_barto_reward=False), bins_space=bins_space\n",
    "    )\n",
    "    STEPS = 10000\n",
    "    # start the policy iteration algorithm\n",
    "    pi.run(nsteps=STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ef0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(state, optimal_policy):\n",
    "    \"\"\"Returns the optimal action for a given state based on the optimal policy.\n",
    "\n",
    "    Parameters:\n",
    "    state (int): The current state.\n",
    "    optimal_policy (dict): The optimal policy containing the action-value pairs for each state.\n",
    "\n",
    "    Returns:\n",
    "    int: The optimal action for the given state.\"\"\"\n",
    "    \n",
    "    _, neighbors  = optimal_policy.kd_tree.query([state], k=5)\n",
    "    simplex = optimal_policy.points[neighbors[0]]\n",
    "    lambdas = optimal_policy.barycentric_coordinates(state, simplex)\n",
    "    \n",
    "    zero = 0 \n",
    "    one = 0\n",
    "    \n",
    "    for i,l in enumerate(lambdas):\n",
    "    \n",
    "        if optimal_policy.policy[tuple(simplex[i])][0] > 0:\n",
    "            zero +=l\n",
    "        else:\n",
    "            one +=l\n",
    "                                 \n",
    "    return 0 if zero > one else 1\n",
    "\n",
    "\n",
    "num_episodes = 10000\n",
    "cartpole = CartPoleEnv(render_mode=\"human\")\n",
    "max_obs = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "min_obs = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "limits = np.array([x_lim, x_dot_lim, theta_lim, theta_dot_lim])\n",
    "for episode in range(0, num_episodes):\n",
    "    observation, _ = cartpole.reset()\n",
    "    for timestep in range(1, 1000):\n",
    "        action = get_optimal_action(observation, pi)\n",
    "        observation, reward, done, terminated, info = cartpole.step(action)\n",
    "        max_obs = np.maximum(max_obs, observation)\n",
    "        min_obs = np.minimum(min_obs, observation)\n",
    "        if done:\n",
    "            #print(f\"max_obs: {max_obs}\")\n",
    "            #print(f\"min_obs: {min_obs}\")\n",
    "            #check limits\n",
    "            #if np.all(max_obs <= limits) and np.all(min_obs >= -limits):\n",
    "            #    print(f\"Episode {episode} finished after {timestep} timesteps\")\n",
    "            #else:\n",
    "            #    print(f\"Episode {episode} finished after {timestep} timesteps with out of limits observations\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
