{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed83e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Union\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import logger, spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "\n",
    "class CartPoleEnv(gym.Env[np.ndarray, Union[int, np.ndarray]]):\n",
    "    \"\"\"\n",
    "    ## Description\n",
    "\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "\n",
    "    ## Action Space\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "\n",
    "    - 0: Push cart to the left\n",
    "    - 1: Push cart to the right\n",
    "\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "    ## Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\n",
    "    ## Rewards\n",
    "    Since the goal is to keep the pole upright for as long as possible, by default, a reward of `+1` is given for every step taken, including the termination step. The default reward threshold is 500 for v1 and 200 for v0 due to the time limit on the environment.\n",
    "\n",
    "    If `sutton_barto_reward=True`, then a reward of `0` is awarded for every non-terminating step and `-1` for the terminating step. As a result, the reward threshold is 0 for v0 and v1.\n",
    "\n",
    "    ## Starting State\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "    ## Episode End\n",
    "    The episode ends if any one of the following occurs:\n",
    "\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "    ## Arguments\n",
    "\n",
    "    Cartpole only has `render_mode` as a keyword for `gymnasium.make`.\n",
    "    On reset, the `options` parameter allows the user to change the bounds used to determine the new random state.\n",
    "\n",
    "    ```python\n",
    "    >>> import gymnasium as gym\n",
    "    >>> env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    >>> env\n",
    "    <TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>\n",
    "    >>> env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})  # default low=-0.05, high=0.05\n",
    "    (array([ 0.03647037, -0.0892358 , -0.05592803, -0.06312564], dtype=float32), {})\n",
    "\n",
    "    ```\n",
    "\n",
    "    | Parameter               | Type       | Default                 | Description                                                                                   |\n",
    "    |-------------------------|------------|-------------------------|-----------------------------------------------------------------------------------------------|\n",
    "    | `sutton_barto_reward`   | **bool**   | `False`                 | If `True` the reward function matches the original sutton barto implementation                |\n",
    "\n",
    "    ## Vectorized environment\n",
    "\n",
    "    To increase steps per seconds, users can use a custom vector environment or with an environment vectorizor.\n",
    "\n",
    "    ```python\n",
    "    >>> import gymnasium as gym\n",
    "    >>> envs = gym.make_vec(\"CartPole-v1\", num_envs=3, vectorization_mode=\"vector_entry_point\")\n",
    "    >>> envs\n",
    "    CartPoleVectorEnv(CartPole-v1, num_envs=3)\n",
    "    >>> envs = gym.make_vec(\"CartPole-v1\", num_envs=3, vectorization_mode=\"sync\")\n",
    "    >>> envs\n",
    "    SyncVectorEnv(CartPole-v1, num_envs=3)\n",
    "\n",
    "    ```\n",
    "\n",
    "    ## Version History\n",
    "    * v1: `max_time_steps` raised to 500.\n",
    "        - In Gymnasium `1.0.0a2` the `sutton_barto_reward` argument was added (related [GitHub issue](https://github.com/Farama-Foundation/Gymnasium/issues/790))\n",
    "    * v0: Initial versions release.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, sutton_barto_reward: bool = False, render_mode: Optional[str] = None\n",
    "    ):\n",
    "        self._sutton_barto_reward = sutton_barto_reward\n",
    "\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(\n",
    "            action\n",
    "        ), f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            if self._sutton_barto_reward:\n",
    "                reward = 0.0\n",
    "            elif not self._sutton_barto_reward:\n",
    "                reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            if self._sutton_barto_reward:\n",
    "                reward = -1.0\n",
    "            elif not self._sutton_barto_reward:\n",
    "                reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            if self._sutton_barto_reward:\n",
    "                reward = -1.0\n",
    "            elif not self._sutton_barto_reward:\n",
    "                reward = 0.0\n",
    "\n",
    "            reward = -1.0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        low, high = utils.maybe_parse_reset_bounds(\n",
    "            options, -0.005, 0.005  # default low\n",
    "        )  # default high\n",
    "        #low = (-x_lim, -x_dot_lim, -theta_lim, -theta_dot_lim)\n",
    "        #high = (x_lim, x_dot_lim, theta_lim, theta_dot_lim)\n",
    "        self.state = self.np_random.uniform(low=0, high=0, size=(4,))\n",
    "        #elf.state=np.array([0.0, 0.0, 0.01, 0.0])\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            assert self.spec is not None\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gymnasium[classic-control]`\"\n",
    "            ) from e\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode == \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = self.screen_width / world_width\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 100  # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54115dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating transition and reward function table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000/160000 [07:36<00:00, 350.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Policy Iteration algorithm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solving step 0\n",
      "delta: 1.0\n",
      "delta: 1.0243957523147484\n",
      "delta: 1.0312483499915857\n",
      "delta: 1.037626088463456\n",
      "delta: 1.039609361421367\n",
      "delta: 1.041090393421829\n",
      "delta: 1.0403455756003304\n",
      "delta: 1.0390943228718292\n",
      "delta: 1.0364080609313415\n",
      "delta: 1.0332598377578606\n",
      "delta: 1.0290941730639904\n",
      "delta: 1.024532599239567\n",
      "delta: 1.0192120083867628\n",
      "delta: 1.013555733548733\n",
      "delta: 1.007310373197539\n",
      "delta: 1.0007790269298376\n",
      "delta: 0.9937778189916635\n",
      "delta: 0.9865338857429506\n",
      "delta: 0.9789103203978833\n",
      "delta: 0.9710840291767546\n",
      "delta: 0.9629506702770065\n",
      "delta: 0.9546521880092165\n",
      "delta: 0.946107108168917\n",
      "delta: 0.9374318509384238\n",
      "delta: 0.92856124094385\n",
      "delta: 0.9195923211709776\n",
      "delta: 0.9104717822558435\n",
      "delta: 0.9012814977048933\n",
      "delta: 0.8919770242463798\n",
      "delta: 0.8826280669479907\n",
      "delta: 0.8731969921164016\n",
      "delta: 0.8637435492427947\n",
      "delta: 0.8542354742936205\n",
      "delta: 0.8447242398034547\n",
      "delta: 0.8351819163835756\n",
      "delta: 0.8256530056468776\n",
      "delta: 0.8161131474711922\n",
      "delta: 0.8066009122256759\n",
      "delta: 0.797094926903199\n",
      "delta: 0.7876286752221091\n",
      "delta: 0.778183316678934\n",
      "delta: 0.7687879464675547\n",
      "delta: 0.7594258931091247\n",
      "delta: 0.7501224486650102\n",
      "delta: 0.7408628141030036\n",
      "delta: 0.7316689748750846\n",
      "delta: 0.7225277582642491\n",
      "delta: 0.713458268037428\n",
      "delta: 0.7044487506867938\n",
      "delta: 0.6955157944315289\n",
      "delta: 0.6866488887767304\n",
      "delta: 0.6778624234967339\n",
      "delta: 0.6691469799126679\n",
      "delta: 0.660515025034897\n",
      "delta: 0.6519581013884022\n",
      "delta: 0.6434869935603942\n",
      "delta: 0.6350940918715864\n",
      "delta: 0.6267887084447779\n",
      "delta: 0.6185639825415805\n",
      "delta: 0.6104279375092929\n",
      "delta: 0.6023743751242492\n",
      "delta: 0.5944101908350135\n",
      "delta: 0.5865297732004677\n",
      "delta: 0.5787390307743081\n",
      "delta: 0.5710328724196216\n",
      "delta: 0.5634163434465123\n",
      "delta: 0.5558848145864914\n",
      "delta: 0.548442576378946\n",
      "delta: 0.5410854100013225\n",
      "delta: 0.5338169464017426\n",
      "delta: 0.5266333319119241\n",
      "delta: 0.5195376214149618\n",
      "delta: 0.5125262864746958\n",
      "delta: 0.505601879213259\n",
      "delta: 0.4987611612139986\n",
      "delta: 0.4920062461705257\n",
      "delta: 0.4853341546084664\n",
      "delta: 0.47874661824869946\n",
      "delta: 0.4722408891160583\n",
      "delta: 0.4658183664972455\n",
      "delta: 0.4594765096691944\n",
      "delta: 0.4532164289471652\n",
      "delta: 0.44703576942521295\n",
      "delta: 0.44093539057251974\n",
      "delta: 0.43491310434063735\n",
      "delta: 0.42896955278899895\n",
      "delta: 0.4233021733249416\n",
      "delta: 0.4177643088992369\n",
      "delta: 0.4122876934170989\n",
      "delta: 0.40687955548604293\n",
      "delta: 0.40153211845073145\n",
      "delta: 0.3962518580811434\n",
      "delta: 0.39103166774323483\n",
      "delta: 0.38587734285607667\n",
      "delta: 0.3807823856786001\n",
      "delta: 0.3757519767524684\n",
      "delta: 0.37078017191541335\n",
      "delta: 0.3658715959955998\n",
      "delta: 0.36102080832758077\n",
      "delta: 0.35623193150870236\n",
      "delta: 0.35149998300137497\n",
      "delta: 0.34682863153648213\n",
      "delta: 0.34221331158427404\n",
      "delta: 0.3376572817586947\n",
      "delta: 0.33315635625120876\n",
      "delta: 0.32871342314628293\n",
      "delta: 0.3243246425305415\n",
      "delta: 0.31999256779002394\n",
      "delta: 0.31571367420696106\n",
      "delta: 0.3114902129080406\n",
      "delta: 0.3073189464981283\n",
      "delta: 0.30320185321943427\n",
      "delta: 0.29913595768255163\n",
      "delta: 0.2951229918512013\n",
      "delta: 0.29116021933819525\n",
      "delta: 0.2872491499309717\n",
      "delta: 0.2833872653355485\n",
      "delta: 0.27957587500092984\n",
      "delta: 0.27581265971465996\n",
      "delta: 0.2720987483760382\n",
      "delta: 0.268432003562026\n",
      "delta: 0.26481339155633066\n",
      "delta: 0.26124094099215256\n",
      "delta: 0.25771547179255094\n",
      "delta: 0.25423516432765325\n",
      "delta: 0.2508007068937985\n",
      "delta: 0.24741041856196944\n",
      "delta: 0.2440648693574019\n",
      "delta: 0.24076250518044162\n",
      "delta: 0.23750378989205956\n",
      "delta: 0.23428728540798716\n",
      "delta: 0.23111336039950459\n",
      "delta: 0.22798068294430607\n",
      "delta: 0.22488953647150822\n",
      "delta: 0.2218386862407442\n",
      "delta: 0.21882833945501545\n",
      "delta: 0.21585735036913434\n",
      "delta: 0.21292585813108644\n",
      "delta: 0.21003279852568824\n",
      "delta: 0.20717825004997792\n",
      "delta: 0.20436122320992922\n",
      "delta: 0.20158174255973904\n",
      "delta: 0.19883888711456166\n",
      "delta: 0.19613263356151833\n",
      "delta: 0.19346212375711502\n",
      "delta: 0.19082729202301607\n",
      "delta: 0.1882273378832764\n",
      "delta: 0.18566215827497956\n",
      "delta: 0.18313100566673768\n",
      "delta: 0.18063374411782718\n",
      "delta: 0.17816967472882084\n",
      "delta: 0.17573863275782742\n",
      "delta: 0.1733399639985862\n",
      "delta: 0.17097347859393608\n",
      "delta: 0.16863856343245232\n",
      "delta: 0.16633500687213143\n",
      "delta: 0.164062233608945\n",
      "delta: 0.16182001322368933\n",
      "delta: 0.15960780521503182\n",
      "delta: 0.15742536310078492\n",
      "delta: 0.1552721784360358\n",
      "delta: 0.15314799112306332\n",
      "delta: 0.1510523222623874\n",
      "delta: 0.1489849003459085\n",
      "delta: 0.14694527372336097\n",
      "delta: 0.14493316146104007\n",
      "delta: 0.14294813705777187\n",
      "delta: 0.14098991193876032\n",
      "delta: 0.13905808283054455\n",
      "delta: 0.13715235511985213\n",
      "delta: 0.1352723470027115\n",
      "delta: 0.13341775926467392\n",
      "delta: 0.1315882299621336\n",
      "delta: 0.12978345656637202\n",
      "delta: 0.1280030955211373\n",
      "delta: 0.12624684213392356\n",
      "delta: 0.12451436988691\n",
      "delta: 0.12280537295040972\n",
      "delta: 0.12111954060959818\n",
      "delta: 0.11945656681159278\n",
      "delta: 0.11781615551269908\n",
      "delta: 0.11619800124881863\n",
      "delta: 0.11460182161007992\n",
      "delta: 0.11302731244035158\n",
      "delta: 0.11147420401283625\n",
      "delta: 0.10994219411443851\n",
      "delta: 0.10843102482974132\n",
      "delta: 0.10694039644752706\n",
      "delta: 0.1054700620640574\n",
      "delta: 0.10401972495978384\n",
      "delta: 0.10258914850911083\n",
      "delta: 0.10117803941119519\n",
      "delta: 0.09978617064528805\n",
      "delta: 0.09841325269992751\n",
      "delta: 0.09705906754058446\n",
      "delta: 0.09572332976469511\n",
      "delta: 0.0944058297561412\n",
      "delta: 0.09310628649396335\n",
      "delta: 0.09182449825860317\n",
      "delta: 0.09056018864208681\n",
      "delta: 0.08931316334135886\n",
      "delta: 0.0880831507547839\n",
      "delta: 0.08686996355464771\n",
      "delta: 0.08567333510501385\n",
      "delta: 0.08449308464713567\n",
      "delta: 0.08332895063973922\n",
      "delta: 0.08218075851887363\n",
      "delta: 0.08104825193916554\n",
      "delta: 0.07993126218687507\n",
      "delta: 0.07882953818908334\n",
      "delta: 0.07774291676436462\n",
      "delta: 0.07667115216662523\n",
      "delta: 0.07561408645368317\n",
      "delta: 0.07457147924078811\n",
      "delta: 0.07354317755417128\n",
      "delta: 0.07252894638733665\n",
      "delta: 0.07152863748473237\n",
      "delta: 0.07054202121936726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [14:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 202\u001b[0m\n\u001b[1;32m    200\u001b[0m STEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# start the policy iteration algorithm\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m \u001b[43mpi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnsteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 178\u001b[0m, in \u001b[0;36mPolicyIteration.run\u001b[0;34m(self, nsteps)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(nsteps)):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolving step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition_and_reward_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimprove_policy(transition_and_reward_function):\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 124\u001b[0m, in \u001b[0;36mPolicyIteration.evaluate_policy\u001b[0;34m(self, transition_and_reward_function)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space:\n\u001b[1;32m    123\u001b[0m     reward, next_state, simplex, bar_coor \u001b[38;5;241m=\u001b[39m transition_and_reward_function[(state, action)]\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m--> 124\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbar_coor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplex\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     new_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy[state][action] \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_state_value)\n\u001b[1;32m    126\u001b[0m new_value_function[state] \u001b[38;5;241m=\u001b[39m new_val\n",
      "Cell \u001b[0;32mIn[8], line 98\u001b[0m, in \u001b[0;36mPolicyIteration.get_value\u001b[0;34m(self, lambdas, simplex, value_function)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     values           \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([value_function[\u001b[38;5;28mtuple\u001b[39m(e)] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(simplex)])    \n\u001b[0;32m---> 98\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;167;01mKeyError\u001b[39;00m\n\u001b[1;32m    101\u001b[0m ):  \u001b[38;5;66;03m# if next_state is not in value_function, assume it's a 'dead' state.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m500\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from itertools import product\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "class PolicyIteration(object):\n",
    "    \"\"\"Policy Iteration Algorithm for gymnasium environment\"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, gamma: float = 0.99, bins_space: dict = None):\n",
    "        \"\"\"Initializes the Policy Iteration.\n",
    "\n",
    "        Parameters:\n",
    "        - env (gym.Env): The environment in which the agent will interact.\n",
    "        - gamma (float): The discount factor for future rewards. Default is 0.99.\n",
    "        - bins_space (dict): A dictionary specifying the number of bins for each state variable. Default is None.\n",
    "\n",
    "        Returns: None\"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma  # discaunt factor\n",
    "\n",
    "        self.action_space = range(env.action_space.n)\n",
    "        self.bins_space = bins_space\n",
    "        \n",
    "        self.states_space = list(\n",
    "            set(product(*bins_space.values()))\n",
    "        )  # avoid repited states\n",
    "        \n",
    "        self.points = np.array([np.array(e) for e in self.states_space])\n",
    "        self.kd_tree = KDTree(self.points)\n",
    "        \n",
    "        self.policy = {state: {0: 0.5, 1: 0.5} for state in self.states_space}\n",
    "        self.value_function = {state: 0 for state in self.states_space}  # initialize value function\n",
    "\n",
    "\n",
    "    def barycentric_coordinates(self, point, simplex):\n",
    "        # Formulate the system of equations\n",
    "        A = np.vstack([np.array(simplex).T, np.ones(len(simplex))])\n",
    "        b = np.hstack([point, [1]])\n",
    "        objective_function = lambda x: np.linalg.norm(A.dot(x) - b)\n",
    "\n",
    "        # Define the constraint that the solution must be greater than zero\n",
    "        constraints = ({'type': 'ineq', 'fun': lambda x: x})\n",
    "\n",
    "        # Initial guess for the solution\n",
    "        x0 = np.array([0.33, 0.33, 0.33, 0.33, 0.33])\n",
    "\n",
    "        # Solve the optimization problem\n",
    "        result = minimize(objective_function, x0, constraints=constraints, tol=1e-3)\n",
    "\n",
    "        # The approximate solution\n",
    "        x_approx = result.x\n",
    "        return x_approx\n",
    "\n",
    "    def get_transition_reward_function(self) -> dict:\n",
    "        \"\"\"Generate a transition reward function table.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary representing the transition reward function table.\n",
    "                The keys are tuples of (state, action), and the values are dictionaries\n",
    "                with 'reward' and 'next_state' as keys.\"\"\"\n",
    "\n",
    "        table = {}\n",
    "        for state in tqdm(self.states_space):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                self.env.reset() # TODO: is this necessary? might be slow\n",
    "                self.env.state = np.array(state, dtype=np.float64)  # set the state\n",
    "                obs, _, terminated, done, info = self.env.step(action)\n",
    "                \n",
    "                _, neighbors  = self.kd_tree.query([obs], k=5)\n",
    "                simplex = self.points[neighbors[0]]\n",
    "                lambdas = self.barycentric_coordinates(state, simplex)\n",
    "                \n",
    "                reward = (\n",
    "                    0 if (-0.2 < obs[2] < 0.2) and (-2.4 < obs[0] < 2.4) else -1\n",
    "                )  # TODO remove this hardcoded reward\n",
    "                table[(state, action)] = {\"reward\": reward, \n",
    "                                          \"next_state\": obs,\n",
    "                                          \"simplex\": simplex,\n",
    "                                          \"barycentric_coordinates\":lambdas}\n",
    "                \n",
    "\n",
    "        return table\n",
    "\n",
    "    def get_value(self, lambdas, simplex, value_function):\n",
    "        \"\"\"Retrieves the value of a given state from the value function.\n",
    "\n",
    "        Parameters:\n",
    "            state (any): The state for which the value needs to be retrieved.\n",
    "            value_function (dict): A dictionary representing the value function.\n",
    "\n",
    "        Returns:\n",
    "            float: The value of the given state from the value function.\"\"\"\n",
    "\n",
    "        try:\n",
    "            values           = np.array([value_function[tuple(e)] for e in list(simplex)])    \n",
    "            next_state_value = np.dot(lambdas, values)\n",
    "        except (\n",
    "            KeyError\n",
    "        ):  # if next_state is not in value_function, assume it's a 'dead' state.\n",
    "            next_state_value = -500\n",
    "            \n",
    "        return next_state_value\n",
    "\n",
    "    def evaluate_policy(self, transition_and_reward_function: dict) -> dict:\n",
    "        \"\"\"Evaluates the given policy using the provided transition and reward function.\n",
    "\n",
    "        Args:\n",
    "            transition_and_reward_function (dict): A dictionary representing the transition and reward function.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary representing the new value function after evaluating the policy.\n",
    "        \"\"\"\n",
    "        theta = 1e-2 # convergence threshold\n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_value_function = {}\n",
    "            for state in self.states_space:\n",
    "                new_val = 0\n",
    "                for action in self.action_space:\n",
    "                    reward, next_state, simplex, bar_coor = transition_and_reward_function[(state, action)].values()\n",
    "                    next_state_value = self.get_value(bar_coor, simplex,self.value_function)\n",
    "                    new_val += self.policy[state][action] * (reward + self.gamma * next_state_value)\n",
    "                new_value_function[state] = new_val\n",
    "\n",
    "            delta = max(delta, max(abs(new_value_function[state] - self.value_function[state]) for state in self.states_space))\n",
    "            print(f\"delta: {delta}\")\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "            self.value_function = new_value_function\n",
    "        return new_value_function\n",
    "\n",
    "    def improve_policy(self, transition_and_reward_function: dict) -> dict:\n",
    "        \"\"\"Improves the current policy based on the given transition and reward function.\n",
    "\n",
    "        Args:\n",
    "            transition_and_reward_function (dict): A dictionary representing the transition and reward function.\n",
    "                The keys are tuples of (state, action) and the values are dictionaries with 'reward' and 'next_state' keys.\n",
    "\n",
    "        Returns:\n",
    "            dict: The new policy after improvement.\"\"\"\n",
    "        \n",
    "        policy_stable = True\n",
    "        new_policy = {}\n",
    "\n",
    "        for state in self.states_space:\n",
    "            action_values = {}\n",
    "            for action in self.action_space:\n",
    "                reward, next_state, simplex, bar_coor = transition_and_reward_function[(state, action)].values()\n",
    "                action_values[action] = reward + self.gamma * self.get_value(bar_coor, simplex,self.value_function)\n",
    "                \n",
    "            greedy_action, _ = max(action_values.items(), key=lambda pair: pair[1])\n",
    "            new_policy[state] = {\n",
    "                action: 1 if action is greedy_action else 0 for action in self.action_space\n",
    "            }\n",
    "        if self.policy != new_policy:\n",
    "            print(f\"number of different actions: {sum([self.policy[state][0] != new_policy[state][0] for state in self.states_space])}\")\n",
    "            policy_stable = False\n",
    "\n",
    "        self.policy = new_policy\n",
    "        return policy_stable\n",
    "\n",
    "    def run(self, nsteps):\n",
    "        \"\"\"Runs the policy iteration algorithm for a specified number of steps.\n",
    "\n",
    "        Parameters:\n",
    "        - nsteps (int): The number of steps to run the algorithm for. Default is 10 steps.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Generating transition and reward function table...\")\n",
    "        transition_and_reward_function = self.get_transition_reward_function()\n",
    "        print(\"Running Policy Iteration algorithm...\")\n",
    "        for n in tqdm(range(nsteps)):\n",
    "            print(f\"solving step {n}\")\n",
    "            self.evaluate_policy(transition_and_reward_function)\n",
    "            if self.improve_policy(transition_and_reward_function):\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_lim = 2.5\n",
    "    x_dot_lim = 2.5\n",
    "    theta_lim = 0.25\n",
    "    theta_dot_lim = 2.5\n",
    "\n",
    "\n",
    "    bins_space = {\n",
    "        \"x_space\": np.linspace(-x_lim, x_lim, 20),\n",
    "        \"x_dot_space\": np.linspace(-x_dot_lim, x_dot_lim, 20),\n",
    "        \"theta_space\": np.linspace(-theta_lim, theta_lim, 20),\n",
    "        \"theta_dot_space\": np.linspace(-theta_dot_lim, theta_dot_lim, 20),\n",
    "    }\n",
    "\n",
    "    pi = PolicyIteration(\n",
    "        env=CartPoleEnv(sutton_barto_reward=False), bins_space=bins_space\n",
    "    )\n",
    "    STEPS = 10000\n",
    "    # start the policy iteration algorithm\n",
    "    pi.run(nsteps=STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ef0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(state, optimal_policy):\n",
    "    \"\"\"Returns the optimal action for a given state based on the optimal policy.\n",
    "\n",
    "    Parameters:\n",
    "    state (int): The current state.\n",
    "    optimal_policy (dict): The optimal policy containing the action-value pairs for each state.\n",
    "\n",
    "    Returns:\n",
    "    int: The optimal action for the given state.\"\"\"\n",
    "    \n",
    "    _, neighbors  = optimal_policy.kd_tree.query([state], k=5)\n",
    "    simplex = optimal_policy.points[neighbors[0]]\n",
    "    lambdas = optimal_policy.barycentric_coordinates(state, simplex)\n",
    "    \n",
    "    zero = 0 \n",
    "    one = 0\n",
    "    \n",
    "    for i,l in enumerate(lambdas):\n",
    "    \n",
    "        if optimal_policy.policy[tuple(simplex[i])][0] > 0:\n",
    "            zero +=l\n",
    "        else:\n",
    "            one +=l\n",
    "                                 \n",
    "    return 0 if zero > one else 1\n",
    "\n",
    "\n",
    "num_episodes = 10000\n",
    "cartpole = CartPoleEnv(render_mode=\"human\")\n",
    "max_obs = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "min_obs = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "limits = np.array([x_lim, x_dot_lim, theta_lim, theta_dot_lim])\n",
    "for episode in range(0, num_episodes):\n",
    "    observation, _ = cartpole.reset()\n",
    "    for timestep in range(1, 1000):\n",
    "        action = get_optimal_action(observation, pi)\n",
    "        observation, reward, done, terminated, info = cartpole.step(action)\n",
    "        max_obs = np.maximum(max_obs, observation)\n",
    "        min_obs = np.minimum(min_obs, observation)\n",
    "        if done:\n",
    "            #print(f\"max_obs: {max_obs}\")\n",
    "            #print(f\"min_obs: {min_obs}\")\n",
    "            #check limits\n",
    "            #if np.all(max_obs <= limits) and np.all(min_obs >= -limits):\n",
    "            #    print(f\"Episode {episode} finished after {timestep} timesteps\")\n",
    "            #else:\n",
    "            #    print(f\"Episode {episode} finished after {timestep} timesteps with out of limits observations\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
=======
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b149d005",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoRomeroCuruchet/DynamicProgramming/blob/main/testing_bary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "beeb377e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "from itertools import product\n",
        "from scipy.spatial import KDTree\n",
        "from scipy.optimize import minimize\n",
        "from classic_control.cartpole import CartPoleEnv \n",
        "from classic_control.continuous_mountain_car import Continuous_MountainCarEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "54115dcd",
      "metadata": {
        "id": "54115dcd"
      },
      "outputs": [],
      "source": [
        "class PolicyIteration(object):\n",
        "    \"\"\"\n",
        "    A class to perform Policy Iteration on discretized continuous environments.\n",
        "\n",
        "    Attributes:\n",
        "        env (gym.Env): The Gym environment to work with.\n",
        "        bins_space (dict): A dictionary specifying the discretization bins for each state dimension.\n",
        "        action_space (list): A list of all possible actions in the environment.\n",
        "        gamma (float): The discount factor for future rewards.\n",
        "        theta (float): The threshold for determining convergence in policy evaluation.\n",
        "        states_space (list): The product of bins_space values, representing all possible states.\n",
        "        points (np.array): Numpy array of all states, used for KDTree construction.\n",
        "        kd_tree (KDTree): A KDTree built from the points for efficient nearest neighbor queries.\n",
        "        num_simplex_points (int): Number of points in a simplex for barycentric coordinate calculations.\n",
        "        policy (dict): Current policy mapping from states to probabilities of selecting each action.\n",
        "        value_function (dict): Current estimate of the value function.\n",
        "        transition_reward_table (dict): A table storing transition probabilities and rewards for each state-action pair.\n",
        "\n",
        "        Example:\n",
        "\n",
        "            from classic_control.cartpole import CartPoleEnv \n",
        "            \n",
        "            env = CartPoleEnv()\n",
        "            bins_space = {\n",
        "                \"x_space\": np.linspace(-x_lim, x_lim, 12), # position space (0)\n",
        "                \"x_dot_space\": np.linspace(-x_dot_lim, x_dot_lim, 12), # velocity space (1)\n",
        "                \"theta_space\": np.linspace(-theta_lim, theta_lim, 12), # angle space (2)\n",
        "                \"theta_dot_space\": np.linspace(-theta_dot_lim, theta_dot_lim, 12), # angular velocity space (3)\n",
        "            }\n",
        "            action_space = [0, 1]\n",
        "            pi = PolicyIteration(env, bins_space, action_space)\n",
        "            pi.run()\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env,\n",
        "                 bins_space: dict,\n",
        "                 action_space,\n",
        "                 gamma: float= 0.999,\n",
        "                 theta:float= 5e-2):\n",
        "        \"\"\" \n",
        "        Initializes the PolicyIteration object with the environment, state and action spaces, \n",
        "        and algorithm parameters.\n",
        "\n",
        "        Parameters:\n",
        "            env (gym.Env): The Gym environment to perform policy iteration on.\n",
        "            bins_space (dict): The discretization of the state space.\n",
        "            action_space (list): List of all possible actions.\n",
        "            gamma (float): Discount factor for future rewards.\n",
        "            theta (float): Small threshold for determining the convergence of the policy evaluation.\n",
        "        \n",
        "        Raises:\n",
        "            ValueError: If action_space or bins_space is not provided or empty.\n",
        "            TypeError: If action_space or bins_space is not of the correct type.\n",
        "        \"\"\"\n",
        "        self.env   = env\n",
        "        self.gamma = gamma  # discount factor\n",
        "        self.theta = theta  # convergence threshold for policy evaluation\n",
        "\n",
        "        # if action space is not provided, raise an error\n",
        "        if action_space is None: \n",
        "            raise ValueError(\"Action space must be provided.\")\n",
        "        if not isinstance(action_space, list):\n",
        "            raise TypeError(\"Action space must be a list.\")\n",
        "        if not action_space:\n",
        "            raise ValueError(\"Action space cannot be empty.\")\n",
        "        \n",
        "        # if bins_space is not provided, raise an error\n",
        "        if bins_space is None:\n",
        "            raise ValueError(\"Bins space must be provided.\")\n",
        "        if not isinstance(bins_space, dict):    \n",
        "            raise TypeError(\"Bins space must be a dictionary.\")\n",
        "        if not bins_space:\n",
        "            raise ValueError(\"Bins space cannot be empty.\")\n",
        "\n",
        "        self.action_space = action_space\n",
        "        self.bins_space   = bins_space\n",
        "\n",
        "        self.states_space = list(\n",
        "            set(product(*bins_space.values())) # to avoid repeated states\n",
        "        )\n",
        "\n",
        "        self.points = np.array([np.array(e) for e in self.states_space])\n",
        "        self.kd_tree = KDTree(self.points)\n",
        "        self.num_simplex_points = int(self.points[0].shape[0] + 1)\n",
        "        self.policy = {state: {action: 0.5 for action in self.action_space} for state in self.states_space}\n",
        "        self.value_function = {state: 0 for state in self.states_space}\n",
        "        self.transition_reward_table = None\n",
        "\n",
        "    def barycentric_coordinates(self, point:np.array, simplex:list)->np.array:\n",
        "        \"\"\"\n",
        "        Calculates the barycentric coordinates of a point with respect to a given simplex.\n",
        "\n",
        "        Parameters:\n",
        "            point (np.array): The point for which to calculate the barycentric coordinates.\n",
        "            simplex (list): The simplex as a list of points defining the simplex vertices.\n",
        "\n",
        "        Returns:\n",
        "            np.array: The barycentric coordinates of the point.\n",
        "        \"\"\"\n",
        "        # Formulate the system of equations\n",
        "        A = np.vstack([np.array(simplex).T, np.ones(len(simplex))])\n",
        "        b = np.hstack([point, [1]])\n",
        "        objective_function = lambda x: np.linalg.norm(A.dot(x) - b)\n",
        "        # Define the constraint that the solution must be greater than zero\n",
        "        constraints = ({'type': 'ineq', 'fun': lambda x: x})\n",
        "        # Initial guess for the solution\n",
        "        x0 = np.ones(len(simplex)) / self.num_simplex_points\n",
        "        # Solve the optimization problem\n",
        "        result = minimize(objective_function,\n",
        "                          x0,\n",
        "                          constraints=constraints,\n",
        "                          tol=1e-3)\n",
        "        # The approximate solution\n",
        "        x_approx = result.x\n",
        "        return x_approx\n",
        "\n",
        "    def transition_reward_function(self):\n",
        "        \"\"\"\n",
        "        Generates a table mapping each state and action to its reward, next state, and other \n",
        "        transition information. Populates the transition_reward_table attribute with this data.\n",
        "        \"\"\"\n",
        "        table = {}\n",
        "        for state in tqdm(self.states_space):\n",
        "            for action in self.action_space:\n",
        "                self.env.reset()    # TODO: is this necessary? might be slow, to avoid warnings\n",
        "                self.env.state = np.array(state, dtype=np.float64)  # set the state\n",
        "                obs, reward, terminated, done, info = self.env.step([action])\n",
        "                _, neighbors  = self.kd_tree.query([obs], k=self.num_simplex_points)\n",
        "                simplex = self.points[neighbors[0]]\n",
        "                lambdas = self.barycentric_coordinates(state, simplex)\n",
        "\n",
        "                table[(state, action)] = {\"reward\": reward,\n",
        "                                          \"next_state\": obs,\n",
        "                                          \"simplex\": simplex,\n",
        "                                          \"barycentric_coordinates\":lambdas}\n",
        "                \n",
        "        self.transition_reward_table = table\n",
        "        \n",
        "\n",
        "    def get_value(self, lambdas, simplex, value_function)->float:\n",
        "        \"\"\"\n",
        "        Calculates the value for a state given the barycentric coordinates and simplex.\n",
        "\n",
        "        Parameters:\n",
        "            lambdas (np.array): Barycentric coordinates within the simplex.\n",
        "            simplex (list): List of points defining the simplex.\n",
        "            value_function (dict): The current value function.\n",
        "\n",
        "        Returns:\n",
        "            float: The calculated value of the state.\n",
        "\n",
        "        Raises:\n",
        "            Exception: If a state in the simplex is not found in the value function.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            values = np.array([value_function[tuple(e)] for e in list(simplex)])\n",
        "            next_state_value = np.dot(lambdas, values)\n",
        "        except (\n",
        "            KeyError\n",
        "        ):\n",
        "            raise Exception(f\"States in {simplex} not found in the value function.\")\n",
        "\n",
        "        return next_state_value\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        \"\"\"\n",
        "        Performs the policy evaluation step of the Policy Iteration, updating the \n",
        "        value_function attribute.\n",
        "        \"\"\"\n",
        "        max_error = -1.0\n",
        "        while abs(max_error) > self.theta:\n",
        "            new_value_function = {}\n",
        "            errors = []\n",
        "            for state in self.states_space:\n",
        "                new_val = 0\n",
        "                for action in self.action_space:\n",
        "                    reward, next_state, simplex, bar_coor = self.transition_reward_table[(state, action)].values()\n",
        "                    next_state_value = self.get_value(bar_coor, simplex, self.value_function)\n",
        "                    # Checkout 'Variable Resolution Discretization in Optimal Control, eq 5'\n",
        "                    new_val += self.policy[state][action] * (reward + self.gamma * next_state_value)\n",
        "                new_value_function[state] = new_val\n",
        "                # update the error: the maximum difference between the new and old value functions\n",
        "                errors.append(abs(new_value_function[state] - self.value_function[state]))\n",
        "\n",
        "            self.value_function = new_value_function # update the value function\n",
        "            \n",
        "            mean = np.round(np.mean(errors), 4)\n",
        "            max_error = np.round(np.max(errors),4)\n",
        "            errs = np.array(errors)\n",
        "            indices = np.where(errs < self.theta)\n",
        "            \n",
        "            print(f\"Max Error: {max_error} |\\\n",
        "                    Avg Error: {mean} |\\\n",
        "                    {errs[indices].shape[0]}<{self.theta}\")\n",
        "\n",
        "    def policy_improvement(self)->bool:\n",
        "        \"\"\"\n",
        "        Performs the policy improvement step, updating the policy based on the current value_function.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the policy is stable and no changes were made, False otherwise.\n",
        "        \"\"\"\n",
        "        policy_stable = True\n",
        "        new_policy = {}\n",
        "        for state in tqdm(self.states_space):\n",
        "            action_values = {}\n",
        "            for action in self.action_space:\n",
        "                reward, next_state, simplex, bar_coor = self.transition_reward_table[(state, action)].values()\n",
        "                action_values[action] = reward + self.gamma * self.get_value(bar_coor, simplex, self.value_function)\n",
        "\n",
        "            greedy_action, _ = max(action_values.items(), key=lambda pair: pair[1])\n",
        "            \n",
        "            new_policy[state] = {\n",
        "                action: int(action == greedy_action) for action in self.action_space\n",
        "            }\n",
        "        if self.policy != new_policy:\n",
        "            print(f\"number of different actions: {sum([self.policy[state] != new_policy[state] for state in self.states_space])}\")\n",
        "            policy_stable = False\n",
        "\n",
        "        self.policy = new_policy\n",
        "        return policy_stable\n",
        "        \n",
        "\n",
        "    def run(self, nsteps:int=1000):\n",
        "        \"\"\"\n",
        "        Executes the Policy Iteration algorithm for a specified number of steps or until convergence.\n",
        "\n",
        "        Parameters:\n",
        "            nsteps (int): Maximum number of iterations to run the policy iteration.\n",
        "        \"\"\"\n",
        "        print(\"Generating transition and reward function table...\")\n",
        "        self.transition_reward_function()\n",
        "        print(\"Running Policy Iteration algorithm...\")\n",
        "        for n in tqdm(range(nsteps)):\n",
        "            print(f\"solving step {n}\")\n",
        "            self.evaluate_policy()\n",
        "            if self.improve_policy():\n",
        "                break\n",
        "        self.env.close()\n",
        "    \n",
        "    def save_policy(self, filename:str):\n",
        "        \"\"\"\n",
        "        Saves the policy with pickle, to be used later.\n",
        "        \"\"\"\n",
        "        pickle.dump(self.policy, open(filename, \"wb\"))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df921921",
      "metadata": {},
      "outputs": [],
      "source": [
        "env=CartPoleEnv(sutton_barto_reward=True)\n",
        "# position thresholds:\n",
        "x_lim = env.x_threshold\n",
        "theta_lim = env.theta_threshold_radians\n",
        "# velocity thresholds:\n",
        "x_dot_lim = 2.5\n",
        "theta_dot_lim = 2.5\n",
        "\n",
        "bins_space = {\n",
        "    \"x_space\": np.linspace(-x_lim, x_lim, 12),                         # position space         (0)\n",
        "    \"x_dot_space\": np.linspace(-x_dot_lim, x_dot_lim, 12),             # velocity space         (1)\n",
        "    \"theta_space\": np.linspace(-theta_lim, theta_lim, 12),             # angle space            (2)\n",
        "    \"theta_dot_space\": np.linspace(-theta_dot_lim, theta_dot_lim, 12), # angular velocity space (3)\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=env, \n",
        "    bins_space=bins_space,\n",
        "    action_space=[0, 1]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3822ee56",
      "metadata": {},
      "outputs": [],
      "source": [
        "bins_space = {\n",
        "    \"x_space\": np.linspace(-1.2, 0.6, 150),\n",
        "    \"x_dot_space\": np.linspace(-0.07, 0.07, 70),\n",
        "}\n",
        "\n",
        "pi = PolicyIteration(\n",
        "    env=Continuous_MountainCarEnv(), \n",
        "    bins_space=bins_space,\n",
        "    action_space=[-1, 1]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "aijVMe6TC2vP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aijVMe6TC2vP",
        "outputId": "aa16c645-69a9-4de0-ffe7-e46ce8864ca6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating transition and reward function table...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10500/10500 [00:23<00:00, 456.01it/s]\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating transition and reward function table...\")\n",
        "pi.transition_reward_function()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "TtT-AyasDDFi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtT-AyasDDFi",
        "outputId": "4e8a4c23-5081-4616-ba3f-4bec84c33cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Error: 0.5868 |                    Avg Error: 0.4089 |                    0<0.05\n",
            "Max Error: 0.5862 |                    Avg Error: 0.4085 |                    0<0.05\n",
            "Max Error: 0.5857 |                    Avg Error: 0.4081 |                    0<0.05\n",
            "Max Error: 0.5851 |                    Avg Error: 0.4077 |                    0<0.05\n",
            "Max Error: 0.5845 |                    Avg Error: 0.4073 |                    0<0.05\n",
            "Max Error: 0.5839 |                    Avg Error: 0.4069 |                    0<0.05\n",
            "Max Error: 0.5833 |                    Avg Error: 0.4065 |                    0<0.05\n",
            "Max Error: 0.5827 |                    Avg Error: 0.4061 |                    0<0.05\n",
            "Max Error: 0.5822 |                    Avg Error: 0.4057 |                    0<0.05\n",
            "Max Error: 0.5816 |                    Avg Error: 0.4052 |                    0<0.05\n",
            "Max Error: 0.581 |                    Avg Error: 0.4048 |                    0<0.05\n",
            "Max Error: 0.5804 |                    Avg Error: 0.4044 |                    0<0.05\n",
            "Max Error: 0.5798 |                    Avg Error: 0.404 |                    0<0.05\n",
            "Max Error: 0.5793 |                    Avg Error: 0.4036 |                    0<0.05\n",
            "Max Error: 0.5787 |                    Avg Error: 0.4032 |                    0<0.05\n",
            "Max Error: 0.5781 |                    Avg Error: 0.4028 |                    0<0.05\n",
            "Max Error: 0.5775 |                    Avg Error: 0.4024 |                    0<0.05\n",
            "Max Error: 0.5769 |                    Avg Error: 0.402 |                    0<0.05\n",
            "Max Error: 0.5764 |                    Avg Error: 0.4016 |                    0<0.05\n",
            "Max Error: 0.5758 |                    Avg Error: 0.4012 |                    0<0.05\n",
            "Max Error: 0.5752 |                    Avg Error: 0.4008 |                    0<0.05\n",
            "Max Error: 0.5746 |                    Avg Error: 0.4004 |                    0<0.05\n",
            "Max Error: 0.5741 |                    Avg Error: 0.4 |                    0<0.05\n",
            "Max Error: 0.5735 |                    Avg Error: 0.3996 |                    0<0.05\n",
            "Max Error: 0.5729 |                    Avg Error: 0.3992 |                    0<0.05\n",
            "Max Error: 0.5723 |                    Avg Error: 0.3988 |                    0<0.05\n",
            "Max Error: 0.5718 |                    Avg Error: 0.3984 |                    0<0.05\n",
            "Max Error: 0.5712 |                    Avg Error: 0.398 |                    0<0.05\n",
            "Max Error: 0.5706 |                    Avg Error: 0.3976 |                    0<0.05\n",
            "Max Error: 0.57 |                    Avg Error: 0.3972 |                    0<0.05\n",
            "Max Error: 0.5695 |                    Avg Error: 0.3968 |                    0<0.05\n",
            "Max Error: 0.5689 |                    Avg Error: 0.3964 |                    0<0.05\n",
            "Max Error: 0.5683 |                    Avg Error: 0.396 |                    0<0.05\n",
            "Max Error: 0.5678 |                    Avg Error: 0.3956 |                    0<0.05\n",
            "Max Error: 0.5672 |                    Avg Error: 0.3952 |                    0<0.05\n",
            "Max Error: 0.5666 |                    Avg Error: 0.3948 |                    0<0.05\n",
            "Max Error: 0.5661 |                    Avg Error: 0.3945 |                    0<0.05\n",
            "Max Error: 0.5655 |                    Avg Error: 0.3941 |                    0<0.05\n",
            "Max Error: 0.5649 |                    Avg Error: 0.3937 |                    0<0.05\n",
            "Max Error: 0.5644 |                    Avg Error: 0.3933 |                    0<0.05\n",
            "Max Error: 0.5638 |                    Avg Error: 0.3929 |                    0<0.05\n",
            "Max Error: 0.5632 |                    Avg Error: 0.3925 |                    0<0.05\n",
            "Max Error: 0.5627 |                    Avg Error: 0.3921 |                    0<0.05\n",
            "Max Error: 0.5621 |                    Avg Error: 0.3917 |                    0<0.05\n",
            "Max Error: 0.5616 |                    Avg Error: 0.3913 |                    0<0.05\n",
            "Max Error: 0.561 |                    Avg Error: 0.3909 |                    0<0.05\n",
            "Max Error: 0.5604 |                    Avg Error: 0.3905 |                    0<0.05\n",
            "Max Error: 0.5599 |                    Avg Error: 0.3901 |                    0<0.05\n",
            "Max Error: 0.5593 |                    Avg Error: 0.3897 |                    0<0.05\n",
            "Max Error: 0.5588 |                    Avg Error: 0.3894 |                    0<0.05\n",
            "Max Error: 0.5582 |                    Avg Error: 0.389 |                    0<0.05\n",
            "Max Error: 0.5576 |                    Avg Error: 0.3886 |                    0<0.05\n",
            "Max Error: 0.5571 |                    Avg Error: 0.3882 |                    0<0.05\n",
            "Max Error: 0.5565 |                    Avg Error: 0.3878 |                    0<0.05\n",
            "Max Error: 0.556 |                    Avg Error: 0.3874 |                    0<0.05\n",
            "Max Error: 0.5554 |                    Avg Error: 0.387 |                    0<0.05\n",
            "Max Error: 0.5549 |                    Avg Error: 0.3866 |                    0<0.05\n",
            "Max Error: 0.5543 |                    Avg Error: 0.3863 |                    0<0.05\n",
            "Max Error: 0.5537 |                    Avg Error: 0.3859 |                    0<0.05\n",
            "Max Error: 0.5532 |                    Avg Error: 0.3855 |                    0<0.05\n",
            "Max Error: 0.5526 |                    Avg Error: 0.3851 |                    0<0.05\n",
            "Max Error: 0.5521 |                    Avg Error: 0.3847 |                    0<0.05\n",
            "Max Error: 0.5515 |                    Avg Error: 0.3843 |                    0<0.05\n",
            "Max Error: 0.551 |                    Avg Error: 0.3839 |                    0<0.05\n",
            "Max Error: 0.5504 |                    Avg Error: 0.3836 |                    0<0.05\n",
            "Max Error: 0.5499 |                    Avg Error: 0.3832 |                    0<0.05\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Policy Iteration algorithm...\")\n",
        "for n in range(1000):\n",
        "    print(f\"solving step {n}\")\n",
        "    pi.policy_evaluation()\n",
        "    if pi.policy_improvement():\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b5ab6bee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ab6bee",
        "outputId": "cc2eb18d-5a89-40d3-fd6c-0cf0e33ae32e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 1 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 2 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 3 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 4 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 5 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 6 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 7 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 8 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 9 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 10 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 11 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 12 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 13 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 14 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 15 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 16 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 17 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 18 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 19 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 20 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 21 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 22 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 23 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 24 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 25 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 26 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 27 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 28 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 29 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 30 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 31 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 32 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 33 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 34 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 35 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 36 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 37 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 38 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 39 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 40 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 41 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 42 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 43 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 44 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 45 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 46 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 47 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 48 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 49 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 50 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 51 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 52 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 53 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 54 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 55 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 56 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 57 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 58 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 59 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 60 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 61 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 62 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 63 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 64 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 65 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 66 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 67 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 68 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 69 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 70 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 71 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 72 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 73 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 74 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 75 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 76 finished after 105 timesteps\n",
            "Total reward: 89.50000000000003\n",
            "Episode 77 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 78 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 79 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 80 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 81 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 82 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 83 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 84 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 85 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 86 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 87 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 88 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 89 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 90 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 91 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 92 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 93 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 94 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 95 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 96 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 97 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 98 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 99 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 100 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 101 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 102 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 103 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 104 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 105 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 106 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 107 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 108 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 109 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 110 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 111 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 112 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 113 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 114 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 115 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 116 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 117 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 118 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 119 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 120 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 121 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 122 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 123 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 124 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 125 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 126 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 127 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 128 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 129 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 130 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 131 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 132 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 133 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 134 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 135 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 136 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 137 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 138 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 139 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 140 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 141 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 142 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 143 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 144 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 145 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 146 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 147 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 148 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 149 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 150 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 151 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 152 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 153 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 154 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 155 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 156 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 157 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 158 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 159 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 160 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 161 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 162 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 163 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 164 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 165 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 166 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 167 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 168 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 169 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 170 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 171 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 172 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 173 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 174 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 175 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 176 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 177 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 178 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 179 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 180 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 181 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 182 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 183 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 184 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 185 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 186 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 187 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 188 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 189 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 190 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 191 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 192 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 193 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 194 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 195 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 196 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 197 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 198 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 199 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 200 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 201 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 202 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 203 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 204 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 205 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 206 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 207 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 208 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 209 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 210 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 211 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 212 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 213 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 214 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 215 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 216 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 217 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 218 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 219 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 220 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 221 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 222 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 223 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 224 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 225 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 226 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 227 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 228 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 229 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 230 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 231 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 232 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 233 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 234 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 235 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 236 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 237 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 238 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 239 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 240 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 241 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 242 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 243 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 244 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 245 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 246 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 247 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 248 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 249 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 250 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 251 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 252 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 253 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 254 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 255 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 256 finished after 103 timesteps\n",
            "Total reward: 89.70000000000003\n",
            "Episode 257 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 258 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 259 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 260 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 261 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 262 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 263 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 264 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 265 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n",
            "Episode 266 finished after 101 timesteps\n",
            "Total reward: 89.90000000000002\n",
            "Episode 267 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 268 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 269 finished after 100 timesteps\n",
            "Total reward: 90.00000000000003\n",
            "Episode 270 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 271 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 272 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 273 finished after 71 timesteps\n",
            "Total reward: 92.90000000000002\n",
            "Episode 274 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 275 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 276 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 277 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 278 finished after 102 timesteps\n",
            "Total reward: 89.80000000000003\n",
            "Episode 279 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 280 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 281 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 282 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 283 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 284 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 285 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 286 finished after 104 timesteps\n",
            "Total reward: 89.60000000000002\n",
            "Episode 287 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 288 finished after 70 timesteps\n",
            "Total reward: 93.00000000000001\n",
            "Episode 289 finished after 69 timesteps\n",
            "Total reward: 93.10000000000001\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m     34\u001b[0m     action \u001b[38;5;241m=\u001b[39m get_optimal_action(observation, pi)\n\u001b[0;32m---> 35\u001b[0m     observation, reward, done, terminated, info \u001b[38;5;241m=\u001b[39m \u001b[43mmountain_car\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
            "File \u001b[0;32m~/DynamicProgramming/classic_control/continuous_mountain_car.py:182\u001b[0m, in \u001b[0;36mContinuous_MountainCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([position, velocity], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
            "File \u001b[0;32m~/DynamicProgramming/classic_control/continuous_mountain_car.py:296\u001b[0m, in \u001b[0;36mContinuous_MountainCarEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    295\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def get_optimal_action(state, optimal_policy):\n",
        "    \"\"\"Returns the optimal action for a given state based on the optimal policy.\n",
        "\n",
        "    Parameters:\n",
        "    state (int): The current state.\n",
        "    optimal_policy (dict): The optimal policy containing the action-value pairs for each state.\n",
        "\n",
        "    Returns:\n",
        "    int: The optimal action for the given state.\"\"\"\n",
        "\n",
        "    _, neighbors  = optimal_policy.kd_tree.query([state], k=5)\n",
        "    simplex = optimal_policy.points[neighbors[0]]\n",
        "    lambdas = optimal_policy.barycentric_coordinates(state, simplex)\n",
        "\n",
        "    zero = 0\n",
        "    one = 0\n",
        "\n",
        "    for i,l in enumerate(lambdas):\n",
        "\n",
        "        if optimal_policy.policy[tuple(simplex[i])][-1] > 0:\n",
        "            zero +=l\n",
        "        else:\n",
        "            one +=l\n",
        "\n",
        "    return -1 if zero > one else 1\n",
        "\n",
        "\n",
        "num_episodes = 10000\n",
        "mountain_car = Continuous_MountainCarEnv(render_mode=\"human\")  #CartPoleEnv(render_mode=\"human\") # Continuous_MountainCarEnv(render_mode=\"human\")  | CartPoleEnv(render_mode=\"human\")\n",
        "for episode in range(0, num_episodes):\n",
        "    observation, _ = mountain_car.reset()\n",
        "    total_reward = 0\n",
        "    for timestep in range(1, 1000):\n",
        "        action = get_optimal_action(observation, pi)\n",
        "        observation, reward, done, terminated, info = mountain_car.step([action])\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            print(f\"Episode {episode} finished after {timestep} timesteps\")\n",
        "            print(f\"Total reward: {total_reward}\")\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
>>>>>>> a5b6f84f0f4eb0b7ca40107d01eee8a00bb2ca0c
}
