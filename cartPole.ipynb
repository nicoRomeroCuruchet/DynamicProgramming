{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf58020-0580-4a26-bf33-afc32fc50aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import A2C\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import logger, spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from gymnasium.vector import VectorEnv\n",
    "from gymnasium.vector.utils import batch_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "652f30a8-0853-49d6-9afe-2a82f642ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnv(gym.Env[np.ndarray, Union[int, np.ndarray]]):\n",
    "    \"\"\"\n",
    "    ## Description\n",
    "\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "\n",
    "    ## Action Space\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "\n",
    "    - 0: Push cart to the left\n",
    "    - 1: Push cart to the right\n",
    "\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "    ## Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\n",
    "    ## Rewards\n",
    "    Since the goal is to keep the pole upright for as long as possible, by default, a reward of `+1` is given for every step taken, including the termination step. The default reward threshold is 500 for v1 and 200 for v0 due to the time limit on the environment.\n",
    "\n",
    "    If `sutton_barto_reward=True`, then a reward of `0` is awarded for every non-terminating step and `-1` for the terminating step. As a result, the reward threshold is 0 for v0 and v1.\n",
    "\n",
    "    ## Starting State\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "    ## Episode End\n",
    "    The episode ends if any one of the following occurs:\n",
    "\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "    ## Arguments\n",
    "\n",
    "    Cartpole only has `render_mode` as a keyword for `gymnasium.make`.\n",
    "    On reset, the `options` parameter allows the user to change the bounds used to determine the new random state.\n",
    "\n",
    "    ```python\n",
    "    >>> import gymnasium as gym\n",
    "    >>> env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    >>> env\n",
    "    <TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>\n",
    "    >>> env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})  # default low=-0.05, high=0.05\n",
    "    (array([ 0.03647037, -0.0892358 , -0.05592803, -0.06312564], dtype=float32), {})\n",
    "\n",
    "    ```\n",
    "\n",
    "    | Parameter               | Type       | Default                 | Description                                                                                   |\n",
    "    |-------------------------|------------|-------------------------|-----------------------------------------------------------------------------------------------|\n",
    "    | `sutton_barto_reward`   | **bool**   | `False`                 | If `True` the reward function matches the original sutton barto implementation                |\n",
    "\n",
    "    ## Vectorized environment\n",
    "\n",
    "    To increase steps per seconds, users can use a custom vector environment or with an environment vectorizor.\n",
    "\n",
    "    ```python\n",
    "    >>> import gymnasium as gym\n",
    "    >>> envs = gym.make_vec(\"CartPole-v1\", num_envs=3, vectorization_mode=\"vector_entry_point\")\n",
    "    >>> envs\n",
    "    CartPoleVectorEnv(CartPole-v1, num_envs=3)\n",
    "    >>> envs = gym.make_vec(\"CartPole-v1\", num_envs=3, vectorization_mode=\"sync\")\n",
    "    >>> envs\n",
    "    SyncVectorEnv(CartPole-v1, num_envs=3)\n",
    "\n",
    "    ```\n",
    "\n",
    "    ## Version History\n",
    "    * v1: `max_time_steps` raised to 500.\n",
    "        - In Gymnasium `1.0.0a2` the `sutton_barto_reward` argument was added (related [GitHub issue](https://github.com/Farama-Foundation/Gymnasium/issues/790))\n",
    "    * v0: Initial versions release.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, sutton_barto_reward: bool = False, render_mode: Optional[str] = None\n",
    "    ):\n",
    "        self._sutton_barto_reward = sutton_barto_reward\n",
    "\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(\n",
    "            action\n",
    "        ), f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            if self._sutton_barto_reward:\n",
    "                reward = 0.0\n",
    "            elif not self._sutton_barto_reward:\n",
    "                reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            if self._sutton_barto_reward:\n",
    "                reward = -1.0\n",
    "            elif not self._sutton_barto_reward:\n",
    "                reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            if self._sutton_barto_reward:\n",
    "                reward = -1.0\n",
    "            elif not self._sutton_barto_reward:\n",
    "                reward = 0.0\n",
    "                \n",
    "            reward = -1.0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        low, high = utils.maybe_parse_reset_bounds(\n",
    "            options, -0.05, 0.05  # default low\n",
    "        )  # default high\n",
    "        self.state = self.np_random.uniform(low=low, high=high, size=(4,))\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            assert self.spec is not None\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gymnasium[classic-control]`\"\n",
    "            ) from e\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode == \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = self.screen_width / world_width\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 100  # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0c43d1-e931-478e-a078-ba553e8c5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration(object):\n",
    "\n",
    "    \"\"\" Policy Iteration Algorithm for gymnasium environment \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, env:gym.Env, gamma:float=0.99, theta:float=1e-8, bins_space:dict=None\n",
    "    ):\n",
    "\n",
    "        \"\"\" Initializes the Policy Iteration.\n",
    "        \n",
    "        Parameters:\n",
    "        - env (gym.Env): The environment in which the agent will interact.\n",
    "        - gamma (float): The discount factor for future rewards. Default is 0.99.\n",
    "        - theta (float): The convergence threshold for value iteration. Default is 1e-8.\n",
    "        - bins_space (dict): A dictionary specifying the number of bins for each state variable. Default is None.\n",
    "        \n",
    "        Returns: None \"\"\"\n",
    "        \n",
    "        self.env   = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "\n",
    "        self.action_space    = env.action_space\n",
    "        self.bins_space      = bins_space\n",
    "\n",
    "        policy = {state:{0:0.5, 1:0.5} for state in [0,1]}\n",
    "        self.states_space    = list(set(product(*bins_space.values())))  # avoid repited states\n",
    "        self.policy = {state:{0:0.5, 1:0.5} for state in self.states_space}\n",
    "        self.value_function  = {state:0 for state in self.states_space}  # initialize value function\n",
    " \n",
    "    def get_state(self, np_state: np.ndarray) -> tuple:\n",
    "        \n",
    "        \"\"\"Discretizes the given state values based on the provided bins dictionary.\n",
    "\n",
    "        Parameters:\n",
    "        state (tuple): The state values to be discretized.\n",
    "        bins_dict (dict): A dictionary containing the bins for each state value.\n",
    "\n",
    "        Returns:\n",
    "        tuple: The discretized state values.\"\"\"\n",
    "        state = tuple(np_state)\n",
    "        discretized_state = []\n",
    "        for value, (_, bins) in zip(state, self.bins_space.items()):\n",
    "            # Digitize the value and adjust the index to be 0-based\n",
    "            up_index = min(np.digitize(value, bins), len(bins)-1)\n",
    "            discretized_value = bins[up_index]\n",
    "            discretized_state.append(discretized_value)\n",
    "\n",
    "        return tuple(discretized_state)\n",
    "\n",
    "    def get_transition_reward_function(self) -> dict:\n",
    "        \n",
    "        \"\"\"Generate a transition reward function table.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary representing the transition reward function table.\n",
    "                The keys are tuples of (state, action), and the values are dictionaries\n",
    "                with 'reward' and 'next_state' as keys. \"\"\"\n",
    "        \n",
    "        table = {}\n",
    "        for state in tqdm(self.states_space):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                self.env.reset()\n",
    "                self.env.state = np.array(state, dtype=np.float32) # set the state\n",
    "                obs, _, terminated, done, info = self.env.step(action)\n",
    "                obs = self.get_state(obs)\n",
    "                reward = 0 if (-0.2 < obs[2] < 0.2) and (-2.4 < obs[0] < 2.4)  else -1\n",
    "                table[(state, action)] = {'reward':reward, 'next_state':obs}\n",
    "\n",
    "        return table\n",
    "\n",
    "    def get_value(self, state, value_function):\n",
    "\n",
    "        \"\"\" Get the value of a state from the value function.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            next_state_value = value_function[state]\n",
    "        except KeyError:                              # if next_state is not in value_function, assume it's a 'dead' state.\n",
    "            next_state_value = -500\n",
    "        return next_state_value\n",
    "\n",
    "    def evaluate_policy(self, transition_and_reward_function:dict)->dict:\n",
    "\n",
    "        new_value_function = {}\n",
    "        for state in self.states_space:\n",
    "            new_val = 0\n",
    "            for action in [0,1]:\n",
    "                reward, next_state = transition_and_reward_function[(state, action)].values()\n",
    "                next_state_value = self.get_value(next_state, self.value_function)\n",
    "                new_val += self.policy[state][action] * (reward + self.gamma*next_state_value)\n",
    "            new_value_function[state] = new_val\n",
    "        self.value_function=new_value_function\n",
    "        return new_value_function\n",
    "\n",
    "    def improve_policy(self, transition_and_reward_function:dict)->dict:\n",
    "\n",
    "        new_policy = {}\n",
    "        \n",
    "        for state in self.states_space:\n",
    "            action_values = {}\n",
    "            for action in [0,1]:\n",
    "                reward, next_state = transition_and_reward_function[(state, action)].values()\n",
    "                action_values[action] = reward + self.gamma*self.get_value(next_state, self.value_function)\n",
    "            greedy_action, value = max(action_values.items(), key= lambda pair: pair[1])\n",
    "            new_policy[state] = {action:1 if action is greedy_action else 0 for action in [0,1]}\n",
    "            \n",
    "        self.policy = new_policy \n",
    "        return new_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f01c26-9542-4660-9d57-590f75ee3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_space = {\"x_space\": np.linspace(-5, 5, 40),\n",
    "              \"x_dot_space\": np.linspace(-5, 5, 40),\n",
    "              \"theta_space\": np.linspace(-0.418, 0.418, 20),\n",
    "              \"theta_dot_space\": np.linspace(-5, 5, 40)}\n",
    "\n",
    "pi = PolicyIteration(env=CartPoleEnv(sutton_barto_reward=False), bins_space=bins_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51eb5abc-755e-46f9-9f03-85190171d115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1280000/1280000 [02:07<00:00, 10046.07it/s]\n"
     ]
    }
   ],
   "source": [
    "transition_and_reward_function = pi.get_transition_reward_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62342ae9-653c-4d3b-8be0-25145ff98562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:21<00:00,  8.15s/it]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(10)):\n",
    "    pi.evaluate_policy(transition_and_reward_function)\n",
    "    pi.improve_policy(transition_and_reward_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66df3e63-417f-4702-a13a-ccfd558d182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "Z=[]\n",
    "# Sample data\n",
    "side = np.linspace(-2,2,15)\n",
    "for k in pi.value_function.keys():\n",
    "    X.append(k[0])\n",
    "    Y.append(k[2])\n",
    "    Z.append(pi.value_function[k])\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Z = np.array(Z)\n",
    "\n",
    "# Define grid\n",
    "x_unique = np.sort(np.unique(X))\n",
    "y_unique = np.sort(np.unique(Y))\n",
    "\n",
    "# Create meshgrid\n",
    "X_grid, Y_grid = np.meshgrid(x_unique, y_unique)\n",
    "\n",
    "# Interpolate Z onto grid\n",
    "Z_grid = np.zeros_like(X_grid, dtype=float)\n",
    "for x, y, z in zip(X, Y, Z):\n",
    "    x_index = np.where(x_unique == x)[0][0]\n",
    "    y_index = np.where(y_unique == y)[0][0]\n",
    "    Z_grid[y_index, x_index] = z\n",
    "# Plot the grid\n",
    "plt.pcolor(X_grid, Y_grid, Z_grid, cmap='gray')\n",
    "plt.colorbar(label='Z')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Grid Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89c3c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(state, optimal_policy):\n",
    "        \n",
    "        greedy_action, _ = max(optimal_policy[state].items(), key= lambda pair: pair[1])\n",
    "        #print(greedy_action)\n",
    "        return greedy_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863b8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178943\n",
      "101057\n"
     ]
    }
   ],
   "source": [
    "zero=0\n",
    "ones=0\n",
    "\n",
    "for k in pi.policy:\n",
    "    zero += pi.policy[k][0]\n",
    "    ones += pi.policy[k][1]\n",
    "\n",
    "print(zero)\n",
    "print(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6548f38-61e5-4405-bfec-8f7542ef784b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m      7\u001b[0m     action \u001b[38;5;241m=\u001b[39m get_optimal_action(pi\u001b[38;5;241m.\u001b[39mget_state(observation), pi\u001b[38;5;241m.\u001b[39mpolicy)\n\u001b[0;32m----> 8\u001b[0m     observation, reward, done, terminated, info \u001b[38;5;241m=\u001b[39m \u001b[43mcartpole\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 214\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    211\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "Cell \u001b[0;32mIn[3], line 327\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    326\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_steps = 0\n",
    "num_episodes = 10000\n",
    "cartpole = CartPoleEnv(render_mode=\"human\")\n",
    "for episode in range(0,num_episodes):\n",
    "    observation, _ = cartpole.reset()\n",
    "    for timestep in range(1,1000):\n",
    "        action = get_optimal_action(pi.get_state(observation), pi.policy)\n",
    "        observation, reward, done, terminated, info = cartpole.step(action)\n",
    "        if done:\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e0d697-0a5b-4b49-be94-00d322638fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eff8697-d88e-416e-87a7-273f3ef206c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3237b6d-7620-4bdc-9ff3-6612c2388934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2102 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1442        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007686507 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.0149     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.11        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00971    |\n",
      "|    value_loss           | 52.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1271         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090292655 |\n",
      "|    clip_fraction        | 0.074        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.669       |\n",
      "|    explained_variance   | 0.0635       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.8         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0184      |\n",
      "|    value_loss           | 33.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1262        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010222774 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 46.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1266        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006943587 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.619      |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 62.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f741dc6aa30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26757d51-a194-4203-af1e-e009bccde455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 500.0 +/- 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#print(\"PPO action:\", action)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#print(\"PI action:\", action_)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dones:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/carOnTheHill/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/carOnTheHill/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gymnasium/envs/classic_control/cartpole.py:190\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    187\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gymnasium/envs/classic_control/cartpole.py:302\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    301\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "# Test the trained agent\n",
    "\n",
    "for episode in range(0,10000):\n",
    "    obs = env.reset()\n",
    "    while(True):\n",
    "        #action_ = get_optimal_action(tuple(obs), pi.policy)\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        #print(\"PPO action:\", action)\n",
    "        #print(\"PI action:\", action_)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        if dones:\n",
    "            break\n",
    "            \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da5b7142-7b7a-4149-8c05-d432c1c43940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 27\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated value of the initial state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mget_value\u001b[0;34m(model, observation)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_value\u001b[39m(model, observation):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Convert the observation to a numpy array if it's not already one\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m----> 7\u001b[0m         observation \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Check if the observation is already in the shape expected by the model (with a batch dimension)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# If not, add a batch dimension\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observation\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_value(model, observation):\n",
    "    # Convert the observation to a numpy array if it's not already one\n",
    "    if not isinstance(observation, np.ndarray):\n",
    "        observation = np.array(observation)\n",
    "    \n",
    "    # Check if the observation is already in the shape expected by the model (with a batch dimension)\n",
    "    # If not, add a batch dimension\n",
    "    if observation.ndim == 1:\n",
    "        observation = np.expand_dims(observation, axis=0)\n",
    "    \n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    obs_tensor = torch.tensor(observation, dtype=torch.float).to(model.device)\n",
    "    \n",
    "    # Get the value from the model's value network\n",
    "    with torch.no_grad():  # Inference only, no gradients required\n",
    "        value = model.policy.value_net(obs_tensor)\n",
    "    \n",
    "    # Convert the tensor to a numpy array and return the first item\n",
    "    return value.cpu().numpy()[0, 0]\n",
    "\n",
    "# Example usage\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "value = get_value(model, observation)\n",
    "print(f\"Estimated value of the initial state: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e79d4ea-2aa7-49a3-ad90-4c6d488c4a72",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4 and 64x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(k[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     11\u001b[0m     Y\u001b[38;5;241m.\u001b[39mappend(k[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m     Z\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X)\n\u001b[1;32m     16\u001b[0m Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Y)\n",
      "Cell \u001b[0;32mIn[53], line 7\u001b[0m, in \u001b[0;36mget_value\u001b[0;34m(model, observation)\u001b[0m\n\u001b[1;32m      5\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(observation, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Retrieve the value from the model's value network\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/carOnTheHill/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/carOnTheHill/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/carOnTheHill/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4 and 64x1)"
     ]
    }
   ],
   "source": [
    "import pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "Z=[]\n",
    "# Sample data\n",
    "side = np.linspace(-2,2,15)\n",
    "for k in pi.value_function.keys():\n",
    "    X.append(k[0])\n",
    "    Y.append(k[2])\n",
    "    Z.append(get_value(model, k))\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Z = np.array(Z)\n",
    "\n",
    "# Define grid\n",
    "x_unique = np.sort(np.unique(X))\n",
    "y_unique = np.sort(np.unique(Y))\n",
    "\n",
    "# Create meshgrid\n",
    "X_grid, Y_grid = np.meshgrid(x_unique, y_unique)\n",
    "\n",
    "# Interpolate Z onto grid\n",
    "Z_grid = np.zeros_like(X_grid, dtype=float)\n",
    "for x, y, z in zip(X, Y, Z):\n",
    "    x_index = np.where(x_unique == x)[0][0]\n",
    "    y_index = np.where(y_unique == y)[0][0]\n",
    "    Z_grid[y_index, x_index] = z\n",
    "# Plot the grid\n",
    "plt.pcolor(X_grid, Y_grid, Z_grid, cmap='gray')\n",
    "plt.colorbar(label='Z')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Grid Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f5b02-7025-41bc-8f21-eee57429d1b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -200     |\n",
      "| time/              |          |\n",
      "|    fps             | 6210     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -200       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2948       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00940301 |\n",
      "|    clip_fraction        | 0.0118     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.00134    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.73       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.000809  |\n",
      "|    value_loss           | 42.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -200        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2431        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007583328 |\n",
      "|    clip_fraction        | 0.00588     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.00233     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.67        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -8.82e-05   |\n",
      "|    value_loss           | 35.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -200        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2263        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009503416 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.00312    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.95        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.000583   |\n",
      "|    value_loss           | 22.6        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "vec_env = make_vec_env(\"MountainCar-v0\", n_envs=4)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be3c600-7ca5-4b52-8784-a4c467fd6221",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[1;32m     13\u001b[0m obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mvec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/carOnTheHill/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:103\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Gym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    they are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    :param mode: The rendering type.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/carOnTheHill/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:263\u001b[0m, in \u001b[0;36mVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Display it using OpenCV\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvecenv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbigimg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "\n",
    "\n",
    "\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97899f25-c3d7-4149-87c9-fe07accb7e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall opencv-python opencv-python-headless -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c44a67-bfce-44f2-ac54-1c57337ef9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in ./.local/lib/python3.8/site-packages (from opencv-python) (1.24.4)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1985b221-f051-4850-998a-b658a57b3206",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 17\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     observation, reward, done, terminate ,info \u001b[38;5;241m=\u001b[39m single_env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/carOnTheHill/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:553\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    535\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/carOnTheHill/lib/python3.8/site-packages/stable_baselines3/common/policies.py:355\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    363\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "\n",
    "\n",
    "\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "\n",
    "single_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation = single_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(observation)\n",
    "    observation, reward, done, terminate ,info = single_env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "088748d0-2695-4d42-bbd1-727ce77b92ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 40\n"
     ]
    }
   ],
   "source": [
    "def func4(n, base):\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return base\n",
    "    return base + func4(n - 1, base) + func4(n - 2, base)\n",
    "\n",
    "# Example usage:\n",
    "result = func4(6, 2)  # You can change the values of n and base as needed\n",
    "print(\"Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e01d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ackermann(m, n):\n",
    "    if m == 0:\n",
    "        return n + 1\n",
    "    elif n == 0:\n",
    "        return ackermann(m - 1, 1)\n",
    "    else:\n",
    "        return ackermann(m - 1, ackermann(m, n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ca4e024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ackermann(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d2c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
